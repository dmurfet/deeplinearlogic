{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Initial implementation of linear logic recurrent neural network\n",
    "#\n",
    "# The architecture is a modified RNN, see the paper \"Linear logic and recurrent neural networks\".\n",
    "# Our inputs are sequences of symbols taken from an alphabet of size num_classes. The length\n",
    "# of the sequences is N. Our outputs are also sequences of length N from the same alphabet.\n",
    "#\n",
    "# An input symbol is encoded as a one_hot vector, so our inputs are sequences of one-hot vectors\n",
    "# of size num_classes. Put another way, this is the dimension of the input space.\n",
    "#\n",
    "# The function to be learned is func_to_learn\n",
    "#\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "\n",
    "from random import shuffle\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our controller code\n",
    "import ntm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GLOBAL FLAGS\n",
    "num_classes = 2\n",
    "batch_size = 1000\n",
    "state_size = 35 # dimension of the state space H\n",
    "input_size = num_classes # dimension of the input space I\n",
    "N = 15 # length of sequences\n",
    "training_percent = 0.05 # precentage used for training\n",
    "epoch = 2\n",
    "\n",
    "controller_state_size = 10 # dimension of the internal state space of the controller\n",
    "memory_address_size = 5 # number of memory locations\n",
    "memory_content_size = 5 # size of vector stored at a memory location\n",
    "state_size = controller_state_size + 2*memory_address_size \\\n",
    "\t\t\t+ memory_address_size * memory_content_size\n",
    "    \n",
    "print(\"state size: \" + str(state_size))\n",
    "\n",
    "def f_ident(seq):\n",
    "    return seq\n",
    "\n",
    "func_to_learn = f_ident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 32768\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]\n",
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Create a shuffled list of all binary sequences of length N\n",
    "s = '{0:0' + str(N) + 'b}'\n",
    "seq_input = [s.format(i) for i in range(2**N)]\n",
    "shuffle(seq_input)\n",
    "seq_input = [map(int,i) for i in seq_input]\n",
    "ti = []\n",
    "for i in seq_input:\n",
    "    temp_list = []\n",
    "    for j in i:\n",
    "        temp_list.append(j)\n",
    "    ti.append(temp_list)\n",
    "seq_input = ti\n",
    "\n",
    "print(\"Number of sequences: \" + str(len(seq_input)))\n",
    "print(seq_input[0])\n",
    "# A typical element of seq_input at this point will be an array like\n",
    "# array([[1],[0],[1],[1],[0]])\n",
    "\n",
    "one_hots = []\n",
    "for i in range(num_classes):\n",
    "    a = [0.0]*num_classes\n",
    "    a[i] = 1.0\n",
    "    one_hots.append(np.array(a))\n",
    "\n",
    "seq_input_onehot = []\n",
    "for i in seq_input:\n",
    "    temp_list = []\n",
    "    for j in i:\n",
    "        temp_list.append(one_hots[j])\n",
    "    seq_input_onehot.append(np.array(temp_list))\n",
    "\n",
    "print(seq_input_onehot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]\n",
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Training output\n",
    "seq_output = []\n",
    "\n",
    "# Swaps 0s for 1s in each sequence\n",
    "for i in seq_input:\n",
    "    seq_output.append(func_to_learn(i))\n",
    "                    \n",
    "print(seq_output[0])\n",
    "\n",
    "seq_output_onehot = []\n",
    "for i in seq_output:\n",
    "    temp_list = []\n",
    "    for j in i:\n",
    "        temp_list.append(one_hots[j])\n",
    "    seq_output_onehot.append(np.array(temp_list))\n",
    "\n",
    "print(seq_output_onehot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1638/32768\n",
      "\n",
      "The first one-hot encoded digit of the first three output sequences\n",
      "[ 0.  1.]\n",
      "[ 1.  0.]\n",
      "[ 0.  1.]\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = int(training_percent * len(seq_input))\n",
    "print(\"Number of training examples: \" + str(NUM_EXAMPLES) + \"/\" + str(len(seq_input)))\n",
    "\n",
    "test_input = seq_input_onehot[NUM_EXAMPLES:3*NUM_EXAMPLES]\n",
    "test_output = seq_output_onehot[NUM_EXAMPLES:3*NUM_EXAMPLES]\n",
    "train_input = seq_input_onehot[:NUM_EXAMPLES]\n",
    "train_output = seq_output_onehot[:NUM_EXAMPLES]\n",
    "\n",
    "print(\"\")\n",
    "print(\"The first one-hot encoded digit of the first three output sequences\")\n",
    "print(test_output[0][0])\n",
    "print(test_output[1][0])\n",
    "print(test_output[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e90aee7cd3cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mntm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNTMRNNCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontroller_state_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory_address_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory_content_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mrnn_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.pyc\u001b[0m in \u001b[0;36mzero_state\u001b[0;34m(self, batch_size, dtype)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m       \u001b[0mzeros_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_state_size_with_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m       \u001b[0mzeros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzeros_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m       \u001b[0mzeros\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_state_size_with_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Input list contains non-constant tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m   \u001b[0mvalue_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvalue_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0mexpanded_num_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m    649\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    174\u001b[0m                                          as_ref=False):\n\u001b[1;32m    175\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m--> 165\u001b[0;31m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    166\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.pyc\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    358\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;31m# provided if possible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "# Definition of the model\n",
    "\n",
    "# inputs, we create N of them, each of shape [None,input_size], one for\n",
    "# each position in the sequence\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N)]\n",
    "\n",
    "# We use tf.nn.rnn rather than dynamic_rnn because there appears to\n",
    "# be a problem with tf.map_fn and the latter, at least in 0.10\n",
    "# state_size is the number of hidden neurons in each layer\n",
    "cell = ntm.NTMRNNCell(state_size,input_size,controller_state_size,memory_address_size,memory_content_size)\n",
    "\n",
    "state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "rnn_outputs = []\n",
    "reuse = False\n",
    "\n",
    "for i in range(N):\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    output_h0 = tf.split(output,[controller_state_size,state_size-controller_state_size],1)[0]\n",
    "    rnn_outputs.append(output_h0)\n",
    "    reuse = True\n",
    "    \n",
    "last_state = state\n",
    "\n",
    "# Final fully connected layer\n",
    "E = tf.Variable(tf.truncated_normal([controller_state_size,input_size]))\n",
    "F = tf.Variable(tf.constant(0.1, shape=[input_size]))\n",
    "\n",
    "# prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.softmax(logit) for logit in logits] \n",
    "print(\"prediction shape: \" + str(prediction[0].get_shape()))\n",
    "ce = [tf.reduce_sum(targets[i] * tf.log(prediction[i])) for i in range(N)]\n",
    "\n",
    "cross_entropy = -tf.add_n(ce)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the errors before training\n",
    "feed_dict = {}\n",
    "test_input_batch = test_input[:batch_size]\n",
    "for d in range(N):\n",
    "    in_node = inputs[d]\n",
    "    out_node = targets[d]\n",
    "    \n",
    "    ti = []\n",
    "    to = []\n",
    "    for k in range(len(test_input_batch)):\n",
    "        ti.append(test_input_batch[k][d]) # A vector giving the one-hot encoding of the dth symbol in the kth sequence\n",
    "        to.append(test_output_batch[k][d])\n",
    "    feed_dict[in_node] = np.array(ti)\n",
    "    feed_dict[out_node] = np.array(to)\n",
    "\n",
    "# The first three digits of this should match the printout for the\n",
    "# first three test output sequences given earlier\n",
    "print(sess.run(tf.argmax(targets[0],1),feed_dict))\n",
    "print(sess.run(tf.argmax(prediction[0],1),feed_dict))\n",
    "print(sess.run(tf.not_equal(tf.argmax(targets[0], 1), tf.argmax(prediction[0], 1)),feed_dict))\n",
    "\n",
    "print(\"\")\n",
    "print(\"The mean of the errors in each digit for the test set:\")\n",
    "incorrects = sess.run(errors, feed_dict)\n",
    "print(incorrects)\n",
    "print(\"Mean: \" + str(np.mean(incorrects)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(len(train_input)/batch_size)\n",
    "print(\"Number of batches: \" + str(no_of_batches))\n",
    "\n",
    "# An annoying thing here is that we cannot use a list as a key in a \n",
    "# dictionary. The workaround we found on StackOverflow here:\n",
    "# http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "\n",
    "# epoch is a global var\n",
    "for i in range(epoch):\n",
    "    ptr = 0\n",
    "    for j in range(no_of_batches):\n",
    "        inp = train_input[ptr:ptr+batch_size]\n",
    "        out = train_output[ptr:ptr+batch_size]\n",
    "        ptr += batch_size\n",
    "        \n",
    "        feed_dict = {}\n",
    "        for d in range(N):\n",
    "            in_node = inputs[d]\n",
    "            out_node = targets[d]\n",
    "            \n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "                to.append(out[k][d])\n",
    "\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "        sess.run(minimize, feed_dict)\n",
    "    print(\"Epoch - \" + str(i+1))\n",
    "    \n",
    "print(\"It took\", time.time() - pre_train_time, \"seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the errors after training\n",
    "feed_dict = {}\n",
    "for d in range(N):\n",
    "    in_node = inputs[d]\n",
    "    out_node = targets[d]\n",
    "    \n",
    "    ti = []\n",
    "    to = []\n",
    "    for k in range(len(test_input_batch)):\n",
    "        ti.append(test_input_batch[k][d]) # A vector giving the one-hot encoding of the dth symbol in the kth sequence\n",
    "        to.append(test_output_batch[k][d])\n",
    "    feed_dict[in_node] = np.array(ti)\n",
    "    feed_dict[out_node] = np.array(to)\n",
    "\n",
    "# The first three digits of this should match the printout for the\n",
    "# first three test output sequences given earlier\n",
    "data = sess.run([tf.argmax(targets[0],1),\n",
    "                 tf.argmax(prediction[0],1),\n",
    "                 cross_entropy],feed_dict)\n",
    "\n",
    "print(\"First digits of test outputs (actual)\")\n",
    "print(data[0])\n",
    "print(\"First digits of test outputs (predicted)\")\n",
    "print(data[1])\n",
    "print(\"Cross-entropy: \" + str(data[2]))\n",
    "\n",
    "# print the mean of the errors in each digit for the test set.\n",
    "incorrects = sess.run(errors, feed_dict)\n",
    "print(incorrects)\n",
    "print(\"Mean: \" + str(np.mean(incorrects)))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
