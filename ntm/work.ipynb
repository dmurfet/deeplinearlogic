{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of linear logic recurrent neural network\n",
    "#\n",
    "# The architecture is a modified RNN, see the paper \"Linear logic and recurrent neural networks\".\n",
    "# Our inputs are sequences of symbols taken from an alphabet of size num_classes. The length\n",
    "# of the sequences is N. Our outputs are also sequences of length N from the same alphabet.\n",
    "#\n",
    "# Here \"symbol\" means a one hot vector.\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# GLOBAL FLAGS\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, pattern_ntm_alt\n",
    "task                  = 'copy' # copy, repeat copy, pattern\n",
    "epoch                 = 100 # number of training epochs, default to 200\n",
    "num_classes           = 258 # number of symbols, INCLUDING initial and terminal symbols\n",
    "N                     = 10 # length of input sequences for training, default to 20, INCLUDING initial and terminal symbols\n",
    "Ntest                 = 10 # length of sequences for testing, default to N, INCLUDING initial and terminal symbols\n",
    "batch_size            = 500 # default to 500 (too large does not fit on GPUs)\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "memory_address_size   = 128 # number of memory locations, default 20\n",
    "memory_content_size   = 20 # size of vector stored at a memory location, default 5\n",
    "powers_ring1          = [0,-1,1] # powers of R used on ring 1, default [0,-1,1]\n",
    "powers_ring2          = [0,-1,1] # powers of R used on ring 2, default [0,-1,1]\n",
    "model_optimizer       = 'rmsprop' # adam, rmsprop, default to rmsprop\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "num_training          = 10000 # default to int(training_percent * (num_classes-2)**N)\n",
    "num_test              = num_training\n",
    "init_symbol           = num_classes - 2\n",
    "term_symbol           = num_classes - 1\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there\n",
    "# 2. The initial and terminal symbols are always from the end of the list of symbols, so they\n",
    "# are respectively num_classes - 2 and num_classes - 1. So the number of symbols which are\n",
    "# not initial or terminal is num_classes - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[71, 228, 102, 27, 142, 103, 69, 3, 236, 106]\n",
      "is mapped to\n",
      "[71, 228, 102, 27, 142, 103, 69, 3, 236, 106]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "# Below N_out and Ntest_out are the lengths of the outputs in both the training\n",
    "# and testing regimes respectively. Since outputs do not include the initial and terminal\n",
    "# symbols, these default to N - 2 and Ntest - 2 respectively.\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N - 2\n",
    "    Ntest_out = Ntest - 2\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "if( task == 'repeat copy' ):\n",
    "    pattern = [0,1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 2 * (N - 2)\n",
    "    Ntest_out = 2 * (Ntest - 2)\n",
    "\n",
    "##############\n",
    "# PATTERN TASK\n",
    "if( task == 'pattern' ):\n",
    "    pattern = [1,0,0,2,0]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 2 * (N - 2)\n",
    "    Ntest_out = 2 * (Ntest - 2)\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = [random.randint(0,num_classes-3) for i in range(N)]\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# INITIALISE STATE #\n",
    "####################\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "input_size = num_classes # dimension of the input space I\n",
    "state_size = 0\n",
    "\n",
    "#####\n",
    "# NTM\n",
    "\n",
    "def init_state_ntm(batch_size, css, mas, mcs):\n",
    "    state_size = controller_state_size + 2*memory_address_size + memory_address_size * memory_content_size\n",
    "    \n",
    "    ra = [0.0]*mas\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,mas]) + ra\n",
    "    \n",
    "    # DEBUG at the moment the read and write addresses are not distributions, i.e. they do not\n",
    "    # sum to 1, but after one step the gamma sharpening will normalise them. We should probably start\n",
    "    # with things that sum to 1, though.\n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)\n",
    "    #init_controller_state = tf.get_variable(\"init_ccs\", shape=[batch_size, controller_state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    init_read_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) + \\\n",
    "                       tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    \n",
    "    init_write_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) + \\\n",
    "                       tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    \n",
    "    init_memory = tf.truncated_normal([batch_size, mas*mcs], 0.0, 1e-6, dtype=tf.float32)\n",
    "    #init_memory = tf.get_variable(\"init_mem\", shape=[batch_size, memory_address_size*memory_content_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "    return state_size, state\n",
    "\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size,\n",
    "                                  memory_address_size, memory_content_size)\n",
    "    \n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,memory_address_size,memory_content_size, powers_ring1)\n",
    "        \n",
    "#############\n",
    "# PATTERN NTM\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size = controller_state_size + 4*memory_address_size + \\\n",
    "                memory_address_size * memory_content_size + \\\n",
    "                memory_address_size * len(powers_ring1)\n",
    "\n",
    "    cell = ntm.PatternNTM(state_size,input_size,controller_state_size,\n",
    "                          memory_address_size,memory_content_size, powers_ring1, powers_ring2)\n",
    "    \n",
    "    state = tf.truncated_normal([batch_size, state_size], 0.0, 0.01, dtype=tf.float32)\n",
    "    \n",
    "#################\n",
    "# PATTERN NTM ALT\n",
    "if( use_model == 'pattern_ntm_alt' ):\n",
    "    state_size = controller_state_size + 4*memory_address_size + \\\n",
    "                memory_address_size * memory_content_size + \\\n",
    "                memory_address_size * len(powers_ring1)\n",
    "\n",
    "    cell = ntm.PatternNTM_alt(state_size,input_size,controller_state_size,\n",
    "                          memory_address_size,memory_content_size, powers_ring1, powers_ring2)\n",
    "    \n",
    "    state = tf.truncated_normal([batch_size, state_size], 0.0, 0.01, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_27/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_26/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_25/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_24/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_23/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_22/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_21/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_20/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_18/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "# During training we feed in sequences of length N and capture outputs of length Nout\n",
    "# During testing we feed in sequences of length Ntest and capture outputs of length Ntest_out\n",
    "# Assuming Ntest >= N, creating Ntest input nodes and Ntest_out output nodes covers both\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest_out)]\n",
    "\n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "write_addresses = []\n",
    "gamma_writes = []\n",
    "gamma_reads = []\n",
    "ss = []\n",
    "\n",
    "for i in range(N):\n",
    "    # Logging\n",
    "    h0, curr_read, curr_write, _ = tf.split(state, [controller_state_size,\n",
    "                                                    memory_address_size,\n",
    "                                                    memory_address_size,-1], 1)\n",
    "    \n",
    "    #### RUN MODEL ####\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    ###################\n",
    "    \n",
    "    # More logging\n",
    "    read_addresses.append(curr_read[0,:])\n",
    "    write_addresses.append(curr_write[0,:])\n",
    "    \n",
    "    with tf.variable_scope(\"NTM\",reuse=True):\n",
    "        W_gamma_write = tf.get_variable(\"W_gamma_write\", [controller_state_size,1])\n",
    "        B_gamma_write = tf.get_variable(\"B_gamma_write\", [])\n",
    "        gamma_write = 1.0 + tf.nn.relu(tf.matmul(h0,W_gamma_write) + B_gamma_write) # shape [batch_size,1]\n",
    "        \n",
    "        W_gamma_read = tf.get_variable(\"W_gamma_read\", [controller_state_size,1])\n",
    "        B_gamma_read = tf.get_variable(\"B_gamma_read\", [])\n",
    "        gamma_read = 1.0 + tf.nn.relu(tf.matmul(h0,W_gamma_read) + B_gamma_read) # shape [batch_size,1]\n",
    "        \n",
    "        W_s = tf.get_variable(\"W_s\", [controller_state_size,len(powers_ring1)])\n",
    "        B_s = tf.get_variable(\"B_s\", [len(powers_ring1)])\n",
    "        s = tf.nn.softmax(tf.matmul(h0,W_s) + B_s) # shape [batch_size,len(powers)]\n",
    "\n",
    "    gamma_writes.append(gamma_write[0,:])\n",
    "    gamma_reads.append(gamma_read[0,:])\n",
    "    ss.append(s[0,:])\n",
    "    reuse = True\n",
    "\n",
    "# We only start recording the outputs of the controller once we have\n",
    "# finished feeding in the input. We feed terminal symbols as input in the second phase.\n",
    "\n",
    "term_symbol_tensor = tf.constant(np.zeros([batch_size,input_size]) + one_hots[term_symbol],\n",
    "                                 dtype=tf.float32,\n",
    "                                 shape=[batch_size,input_size])\n",
    "\n",
    "rnn_outputs = []\n",
    "for i in range(N_out):\n",
    "    output, state = cell(term_symbol_tensor,state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * tf.log(prediction[i])) for i in range(N_out)]\n",
    "\n",
    "if( model_optimizer == 'adam' ):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "elif( model_optimizer == 'rmsprop' ):\n",
    "    optimizer = tf.train.RMSPropOptimizer(1e-4,decay=0.9,momentum=0.9)\n",
    "\n",
    "cross_entropy = -tf.add_n(ce)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "mean_error = tf.scalar_mul(np.true_divide(1,N_out), tf.add_n(errors))\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "w rotations - [ 0.33333308  0.33333352  0.33333343]\n",
      "Write address -\n",
      "[  1.00000048e+00   6.59718523e-07   9.13902738e-07   7.69176722e-07\n",
      "   4.25503487e-07   3.17313663e-07   4.30234081e-07   5.04336583e-07\n",
      "   8.16435431e-07   5.41285488e-07   4.66977241e-07   5.24655945e-07\n",
      "   1.49126762e-07   4.73192927e-07   1.92991621e-07   9.97562324e-07\n",
      "   2.89596311e-08   3.96412617e-07   7.46760378e-08   2.05476276e-08\n",
      "   4.89373349e-07   5.02337230e-08   8.00892110e-07   7.98115821e-07\n",
      "   3.95464298e-07   8.00752503e-07   9.06989214e-07   8.55662961e-07\n",
      "   8.45294608e-07   7.72703629e-07   5.82860366e-07   8.36373943e-07\n",
      "   8.06321282e-07   3.53003742e-07   5.45748946e-07   7.17483260e-07\n",
      "   5.64628841e-08   7.64181607e-08   7.61563513e-07   9.92204150e-07\n",
      "   7.25849247e-07   2.63040306e-07   9.49234732e-07   6.62282559e-07\n",
      "   4.32011376e-07   4.24086693e-07   3.11746845e-07   4.15592325e-07\n",
      "   9.06233808e-08   7.41460326e-07   7.84701683e-07   9.89625448e-08\n",
      "   6.90401407e-07   7.58000596e-08   7.58973840e-07   5.60966953e-07\n",
      "   5.47231934e-07   5.51456822e-07   9.92759851e-07   7.96424843e-07\n",
      "   2.67342813e-07   8.85992506e-07   7.51544803e-07   7.78448566e-07\n",
      "   2.47593874e-07   9.99589020e-07   1.72178751e-08   9.83146492e-07\n",
      "   1.73123723e-07   8.08704044e-07   8.21541050e-07   9.31936256e-07\n",
      "   3.00571685e-07   7.30217948e-07   8.90566128e-08   3.36072929e-07\n",
      "   3.34933645e-07   5.01972295e-07   5.01637203e-07   1.22987743e-07\n",
      "   3.03337799e-07   4.30093536e-07   6.42806299e-07   2.23328584e-07\n",
      "   6.89626461e-07   7.35883702e-07   4.14107689e-07   9.39914571e-07\n",
      "   8.09284188e-07   2.46164795e-08   7.64929439e-07   8.03501109e-07\n",
      "   3.81005179e-07   6.73194165e-07   1.54430268e-07   5.63209426e-07\n",
      "   9.94744255e-07   4.24674738e-07   4.86615903e-08   1.17617368e-07\n",
      "   6.07740049e-07   1.45101424e-07   9.89729415e-07   4.95327811e-07\n",
      "   7.76705861e-07   9.25551319e-07   1.69084430e-07   9.20951404e-09\n",
      "   5.82886798e-07   8.74805551e-07   9.92533387e-07   5.46712727e-07\n",
      "   4.48172443e-07   7.27499696e-08   1.05826970e-07   3.78328792e-07\n",
      "   6.68832911e-07   7.23354958e-07   5.68078065e-07   4.21570292e-07\n",
      "   5.24570680e-07   2.41151582e-07   6.29716851e-07   8.15436465e-07\n",
      "   4.15544264e-07   1.15516183e-08   7.91355149e-07   5.48222054e-08]\n",
      "Write address argmax - 0\n",
      "\n",
      "\n",
      "Step 1 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 1.]\n",
      "Write gamma - [ 1.07622719]\n",
      "w rotations - [ 0.30471927  0.38177231  0.31350845]\n",
      "Write address -\n",
      "[  3.33269358e-01   3.33269984e-01   1.78058485e-06   1.70252849e-06\n",
      "   1.50370408e-06   1.39074541e-06   1.41701798e-06   1.58335945e-06\n",
      "   1.62036929e-06   1.60791865e-06   1.51067775e-06   1.37998359e-06\n",
      "   1.38205530e-06   1.27152180e-06   1.55427892e-06   1.40622933e-06\n",
      "   1.47402375e-06   1.16645469e-06   1.16365140e-06   1.19463232e-06\n",
      "   1.18648620e-06   1.44655053e-06   1.54944439e-06   1.66449888e-06\n",
      "   1.66445238e-06   1.70073645e-06   1.85410602e-06   1.86895045e-06\n",
      "   1.82419717e-06   1.73328078e-06   1.73030787e-06   1.74151160e-06\n",
      "   1.66490770e-06   1.56805186e-06   1.53844474e-06   1.43961699e-06\n",
      "   1.28320414e-06   1.29789464e-06   1.60974719e-06   1.82618203e-06\n",
      "   1.66004008e-06   1.64572009e-06   1.62453489e-06   1.68084762e-06\n",
      "   1.50583276e-06   1.38901021e-06   1.38353835e-06   1.27240548e-06\n",
      "   1.41561554e-06   1.53862788e-06   1.54140696e-06   1.52439088e-06\n",
      "   1.28813610e-06   1.50809728e-06   1.46496063e-06   1.62207391e-06\n",
      "   1.55291514e-06   1.69681812e-06   1.77986601e-06   1.68517988e-06\n",
      "   1.64959795e-06   1.63464051e-06   1.80497580e-06   1.59221815e-06\n",
      "   1.67488338e-06   1.42118904e-06   1.66632572e-06   1.39089082e-06\n",
      "   1.65466815e-06   1.60081015e-06   1.85369811e-06   1.68435372e-06\n",
      "   1.65391907e-06   1.37301367e-06   1.38484529e-06   1.25310953e-06\n",
      "   1.39072131e-06   1.44589853e-06   1.37526354e-06   1.30906517e-06\n",
      "   1.28522174e-06   1.45846093e-06   1.43179625e-06   1.51829056e-06\n",
      "   1.54931013e-06   1.61289074e-06   1.69630403e-06   1.72076591e-06\n",
      "   1.59096089e-06   1.53264421e-06   1.53071653e-06   1.64948949e-06\n",
      "   1.61891739e-06   1.40260249e-06   1.46332548e-06   1.57048771e-06\n",
      "   1.66055156e-06   1.48906918e-06   1.19675076e-06   1.25776069e-06\n",
      "   1.28990075e-06   1.58054820e-06   1.54308464e-06   1.75357843e-06\n",
      "   1.73218984e-06   1.62346316e-06   1.36768131e-06   1.25348208e-06\n",
      "   1.48867639e-06   1.81638700e-06   1.80433119e-06   1.66214807e-06\n",
      "   1.35561345e-06   1.20868037e-06   1.18540368e-06   1.38405915e-06\n",
      "   1.58986154e-06   1.65309893e-06   1.57069428e-06   1.50444566e-06\n",
      "   1.39549149e-06   1.46486025e-06   1.56179658e-06   1.61991591e-06\n",
      "   1.41390115e-06   1.40587588e-06   1.28565819e-06   3.33269894e-01]\n",
      "Write address argmax - 1\n",
      "\n",
      "\n",
      "Step 2 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 1.]\n",
      "Write gamma - [ 1.37145805]\n",
      "w rotations - [ 0.37691432  0.29439607  0.32868969]\n",
      "Write address -\n",
      "[  3.44211608e-01   2.05142707e-01   9.87834036e-02   1.11877489e-06\n",
      "   1.06066716e-06   1.02148010e-06   1.03777802e-06   1.07081769e-06\n",
      "   1.09740631e-06   1.08411609e-06   1.04635615e-06   1.01496505e-06\n",
      "   9.77783657e-07   1.01152636e-06   1.00926616e-06   1.04059757e-06\n",
      "   9.76047659e-07   9.43010036e-07   9.04722810e-07   9.07162132e-07\n",
      "   9.54913048e-07   1.00709724e-06   1.07796018e-06   1.10847589e-06\n",
      "   1.13114436e-06   1.16280796e-06   1.19187871e-06   1.20809136e-06\n",
      "   1.18815069e-06   1.16860906e-06   1.15716352e-06   1.14498380e-06\n",
      "   1.11916211e-06   1.08974348e-06   1.05482445e-06   1.01006663e-06\n",
      "   9.77172817e-07   1.01072465e-06   1.09314101e-06   1.13868248e-06\n",
      "   1.14403122e-06   1.11470024e-06   1.11949282e-06   1.09394352e-06\n",
      "   1.05758568e-06   1.01577587e-06   9.79411880e-07   9.88005013e-07\n",
      "   1.01313037e-06   1.05081165e-06   1.06570246e-06   1.02298759e-06\n",
      "   1.02598881e-06   1.01538035e-06   1.06770585e-06   1.07130461e-06\n",
      "   1.10898623e-06   1.13345789e-06   1.14904549e-06   1.14158445e-06\n",
      "   1.12056819e-06   1.14298462e-06   1.12752230e-06   1.13667227e-06\n",
      "   1.07388519e-06   1.09300663e-06   1.04321555e-06   1.08558754e-06\n",
      "   1.07356664e-06   1.14748002e-06   1.14544378e-06   1.15275202e-06\n",
      "   1.07574419e-06   1.03475656e-06   9.74021418e-07   9.81265430e-07\n",
      "   9.91032380e-07   1.00589511e-06   9.92604669e-07   9.69467692e-07\n",
      "   9.86616215e-07   1.00244074e-06   1.03818070e-06   1.05166737e-06\n",
      "   1.07920494e-06   1.10683823e-06   1.13167766e-06   1.12442876e-06\n",
      "   1.09984626e-06   1.07301457e-06   1.08500751e-06   1.09585005e-06\n",
      "   1.07108019e-06   1.04741071e-06   1.04361334e-06   1.08256529e-06\n",
      "   1.08055940e-06   1.01924286e-06   9.65408844e-07   9.38117182e-07\n",
      "   1.00100613e-06   1.03906075e-06   1.11155327e-06   1.13164538e-06\n",
      "   1.13981355e-06   1.07741153e-06   1.00708496e-06   9.95730261e-07\n",
      "   1.06842379e-06   1.14511602e-06   1.16550234e-06   1.09091468e-06\n",
      "   1.00318459e-06   9.35854587e-07   9.46031093e-07   1.00531588e-06\n",
      "   1.07269386e-06   1.09640882e-06   1.08254790e-06   1.04289904e-06\n",
      "   1.03048251e-06   1.04131766e-06   1.07455594e-06   1.06072662e-06\n",
      "   1.03919604e-06   9.88264333e-07   1.22111797e-01   2.29619682e-01]\n",
      "Write address argmax - 0\n",
      "\n",
      "\n",
      "Step 3 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 1.05146277]\n",
      "Write gamma - [ 1.01081669]\n",
      "w rotations - [ 0.31009689  0.29569295  0.39421016]\n",
      "Write address -\n",
      "[  3.03710878e-01   2.33891279e-01   8.46794918e-02   1.70086194e-02\n",
      "   2.99347676e-08   2.93592120e-08   2.94182563e-08   2.99259462e-08\n",
      "   3.02632657e-08   3.01163077e-08   2.95641183e-08   2.88702005e-08\n",
      "   2.85875572e-08   2.85842532e-08   2.89664772e-08   2.88104420e-08\n",
      "   2.83447434e-08   2.74694738e-08   2.69940656e-08   2.70402527e-08\n",
      "   2.77021286e-08   2.88008302e-08   2.98439353e-08   3.06694332e-08\n",
      "   3.12281827e-08   3.17906199e-08   3.23191536e-08   3.25178391e-08\n",
      "   3.23567626e-08   3.20067777e-08   3.17176259e-08   3.13963682e-08\n",
      "   3.09469002e-08   3.03504741e-08   2.96353555e-08   2.88850828e-08\n",
      "   2.85468644e-08   2.90595121e-08   3.01578851e-08   3.10731352e-08\n",
      "   3.12406918e-08   3.10824397e-08   3.07749168e-08   3.04003862e-08\n",
      "   2.97165812e-08   2.89582189e-08   2.84691311e-08   2.84433490e-08\n",
      "   2.89022069e-08   2.94313391e-08   2.95385423e-08   2.93345828e-08\n",
      "   2.90185600e-08   2.92627611e-08   2.96044949e-08   3.01928225e-08\n",
      "   3.06431787e-08   3.11677262e-08   3.14032107e-08   3.13252997e-08\n",
      "   3.12494741e-08   3.11890282e-08   3.12740376e-08   3.08706483e-08\n",
      "   3.05702237e-08   3.00108844e-08   3.00107281e-08   2.99326111e-08\n",
      "   3.05527763e-08   3.10189670e-08   3.15347748e-08   3.11210933e-08\n",
      "   3.03476035e-08   2.91853901e-08   2.85126678e-08   2.82317831e-08\n",
      "   2.84359132e-08   2.85318720e-08   2.83960642e-08   2.82388406e-08\n",
      "   2.83075874e-08   2.87410238e-08   2.91864772e-08   2.96792280e-08\n",
      "   3.01346148e-08   3.06684171e-08   3.09947801e-08   3.09586312e-08\n",
      "   3.05677368e-08   3.02754124e-08   3.02533714e-08   3.02680334e-08\n",
      "   3.00146645e-08   2.96541085e-08   2.96987857e-08   2.99534619e-08\n",
      "   2.98343181e-08   2.90514155e-08   2.81011481e-08   2.79165793e-08\n",
      "   2.84230062e-08   2.95318419e-08   3.04403223e-08   3.11164179e-08\n",
      "   3.09422745e-08   3.01133340e-08   2.91166007e-08   2.89985547e-08\n",
      "   2.99124991e-08   3.10856052e-08   3.13050741e-08   3.03598462e-08\n",
      "   2.88281772e-08   2.78266175e-08   2.78102306e-08   2.86998851e-08\n",
      "   2.97225569e-08   3.02588603e-08   3.00778069e-08   2.96185334e-08\n",
      "   2.93314724e-08   2.95248679e-08   2.97627931e-08   2.97503888e-08\n",
      "   2.92037310e-08   1.95573922e-02   9.47810933e-02   2.46364892e-01]\n",
      "Write address argmax - 0\n",
      "\n",
      "\n",
      "Step 4 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 1.]\n",
      "Write gamma - [ 1.07418394]\n",
      "w rotations - [ 0.40672779  0.3057754   0.28749683]\n",
      "Write address -\n",
      "[  2.61741698e-01   2.17936382e-01   1.23101272e-01   3.80536020e-02\n",
      "   6.47743838e-03   9.04394426e-07   9.04342471e-07   9.04591445e-07\n",
      "   9.04823594e-07   9.04756178e-07   9.04370552e-07   9.03911996e-07\n",
      "   9.03590376e-07   9.03590831e-07   9.03654040e-07   9.03622549e-07\n",
      "   9.03209980e-07   9.02681165e-07   9.02256033e-07   9.02276099e-07\n",
      "   9.02762963e-07   9.03570992e-07   9.04459398e-07   9.05198590e-07\n",
      "   9.05788966e-07   9.06278331e-07   9.06672824e-07   9.06870184e-07\n",
      "   9.06803450e-07   9.06574769e-07   9.06288392e-07   9.05980698e-07\n",
      "   9.05587910e-07   9.05078593e-07   9.04475996e-07   9.03930356e-07\n",
      "   9.03709122e-07   9.04020283e-07   9.04742365e-07   9.05422837e-07\n",
      "   9.05747811e-07   9.05682157e-07   9.05443699e-07   9.05053582e-07\n",
      "   9.04535000e-07   9.03958494e-07   9.03551552e-07   9.03493799e-07\n",
      "   9.03750106e-07   9.04084459e-07   9.04245667e-07   9.04143974e-07\n",
      "   9.04049728e-07   9.04096055e-07   9.04430067e-07   9.04829903e-07\n",
      "   9.05297554e-07   9.05661523e-07   9.05889408e-07   9.05930449e-07\n",
      "   9.05866443e-07   9.05845582e-07   9.05741899e-07   9.05581771e-07\n",
      "   9.05210982e-07   9.04951889e-07   9.04735600e-07   9.04876799e-07\n",
      "   9.05142542e-07   9.05623438e-07   9.05820002e-07   9.05683578e-07\n",
      "   9.05020670e-07   9.04253341e-07   9.03587534e-07   9.03328328e-07\n",
      "   9.03311445e-07   9.03373689e-07   9.03328555e-07   9.03255739e-07\n",
      "   9.03333500e-07   9.03593786e-07   9.03997602e-07   9.04408694e-07\n",
      "   9.04846729e-07   9.05238721e-07   9.05505885e-07   9.05507648e-07\n",
      "   9.05310628e-07   9.05087518e-07   9.04982926e-07   9.04912781e-07\n",
      "   9.04753506e-07   9.04577291e-07   9.04530225e-07   9.04584624e-07\n",
      "   9.04435410e-07   9.03928651e-07   9.03344528e-07   9.03094133e-07\n",
      "   9.03460034e-07   9.04181150e-07   9.04996853e-07   9.05455352e-07\n",
      "   9.05426418e-07   9.04875549e-07   9.04279830e-07   9.04138403e-07\n",
      "   9.04656758e-07   9.05357354e-07   9.05580237e-07   9.04994693e-07\n",
      "   9.03979071e-07   9.03162970e-07   9.03041496e-07   9.03549164e-07\n",
      "   9.04282899e-07   9.04741057e-07   9.04758281e-07   9.04493106e-07\n",
      "   9.04304159e-07   9.04319393e-07   9.04449280e-07   9.04385558e-07\n",
      "   5.57794515e-03   3.35142091e-02   1.09465361e-01   2.04023629e-01]\n",
      "Write address argmax - 0\n",
      "\n",
      "\n",
      "Step 5 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "w rotations - [ 0.31080329  0.2746971   0.41449958]\n",
      "Write address -\n",
      "[  2.39640176e-01   2.06241652e-01   1.22789986e-01   4.89727780e-02\n",
      "   1.13734994e-02   1.34761259e-03   8.26138375e-07   8.26211647e-07\n",
      "   8.26279404e-07   8.26242740e-07   8.26095345e-07   8.25910831e-07\n",
      "   8.25788504e-07   8.25754682e-07   8.25762186e-07   8.25705854e-07\n",
      "   8.25548170e-07   8.25332052e-07   8.25183463e-07   8.25199720e-07\n",
      "   8.25409757e-07   8.25754796e-07   8.26136727e-07   8.26479891e-07\n",
      "   8.26760527e-07   8.26988696e-07   8.27157123e-07   8.27237898e-07\n",
      "   8.27219026e-07   8.27126030e-07   8.26997223e-07   8.26844655e-07\n",
      "   8.26656276e-07   8.26421285e-07   8.26161113e-07   8.25945506e-07\n",
      "   8.25874736e-07   8.26006954e-07   8.26282530e-07   8.26554412e-07\n",
      "   8.26697885e-07   8.26694986e-07   8.26585392e-07   8.26405653e-07\n",
      "   8.26172936e-07   8.25936240e-07   8.25773611e-07   8.25744792e-07\n",
      "   8.25833183e-07   8.25953805e-07   8.26014741e-07   8.26003657e-07\n",
      "   8.25978702e-07   8.26022472e-07   8.26148892e-07   8.26336134e-07\n",
      "   8.26530197e-07   8.26694304e-07   8.26792018e-07   8.26821235e-07\n",
      "   8.26811629e-07   8.26784344e-07   8.26739040e-07   8.26642008e-07\n",
      "   8.26513372e-07   8.26383769e-07   8.26328233e-07   8.26363873e-07\n",
      "   8.26501548e-07   8.26656446e-07   8.26738585e-07   8.26644680e-07\n",
      "   8.26391442e-07   8.26062319e-07   8.25796349e-07   8.25655604e-07\n",
      "   8.25626557e-07   8.25629797e-07   8.25619168e-07   8.25610357e-07\n",
      "   8.25652421e-07   8.25769746e-07   8.25939708e-07   8.26134112e-07\n",
      "   8.26328062e-07   8.26499047e-07   8.26602445e-07   8.26610574e-07\n",
      "   8.26541623e-07   8.26458006e-07   8.26398320e-07   8.26348298e-07\n",
      "   8.26283724e-07   8.26222220e-07   8.26197379e-07   8.26180099e-07\n",
      "   8.26086989e-07   8.25887696e-07   8.25673510e-07   8.25599841e-07\n",
      "   8.25738425e-07   8.26040377e-07   8.26356882e-07   8.26549012e-07\n",
      "   8.26526389e-07   8.26333235e-07   8.26126325e-07   8.26093640e-07\n",
      "   8.26272810e-07   8.26506721e-07   8.26559415e-07   8.26333576e-07\n",
      "   8.25946358e-07   8.25638381e-07   8.25578297e-07   8.25762868e-07\n",
      "   8.26035034e-07   8.26222731e-07   8.26249561e-07   8.26174642e-07\n",
      "   8.26105520e-07   8.26101655e-07   8.26119162e-07   1.22633181e-03\n",
      "   1.04239210e-02   4.48615290e-02   1.14518560e-01   1.98507532e-01]\n",
      "Write address argmax - 0\n",
      "\n",
      "\n",
      "Step 6 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 1.26780343]\n",
      "Write gamma - [ 1.23987854]\n",
      "w rotations - [ 0.32414985  0.31260297  0.36324725]\n",
      "Write address -\n",
      "[  2.13389352e-01   1.97136655e-01   1.37086362e-01   6.92335218e-02\n",
      "   2.42021400e-02   5.13370894e-03   5.59995417e-04   1.82596148e-06\n",
      "   1.82600252e-06   1.82597876e-06   1.82586712e-06   1.82571512e-06\n",
      "   1.82559131e-06   1.82553219e-06   1.82550502e-06   1.82544727e-06\n",
      "   1.82531562e-06   1.82514236e-06   1.82501105e-06   1.82501208e-06\n",
      "   1.82517897e-06   1.82547819e-06   1.82583426e-06   1.82617600e-06\n",
      "   1.82646806e-06   1.82670169e-06   1.82687063e-06   1.82696044e-06\n",
      "   1.82696249e-06   1.82689030e-06   1.82677002e-06   1.82661745e-06\n",
      "   1.82643100e-06   1.82620863e-06   1.82597103e-06   1.82577674e-06\n",
      "   1.82570193e-06   1.82578924e-06   1.82600434e-06   1.82624251e-06\n",
      "   1.82639894e-06   1.82642736e-06   1.82634267e-06   1.82617748e-06\n",
      "   1.82596591e-06   1.82575104e-06   1.82559461e-06   1.82554243e-06\n",
      "   1.82559108e-06   1.82568192e-06   1.82574786e-06   1.82576287e-06\n",
      "   1.82576252e-06   1.82580038e-06   1.82590929e-06   1.82607300e-06\n",
      "   1.82625604e-06   1.82641429e-06   1.82652070e-06   1.82656788e-06\n",
      "   1.82656947e-06   1.82654458e-06   1.82649262e-06   1.82640815e-06\n",
      "   1.82629253e-06   1.82618351e-06   1.82612234e-06   1.82614815e-06\n",
      "   1.82624842e-06   1.82637621e-06   1.82644010e-06   1.82637530e-06\n",
      "   1.82616736e-06   1.82588701e-06   1.82562928e-06   1.82546751e-06\n",
      "   1.82540089e-06   1.82538702e-06   1.82538258e-06   1.82538702e-06\n",
      "   1.82542863e-06   1.82552913e-06   1.82568408e-06   1.82586814e-06\n",
      "   1.82605595e-06   1.82621795e-06   1.82632323e-06   1.82634949e-06\n",
      "   1.82630856e-06   1.82623762e-06   1.82617066e-06   1.82611268e-06\n",
      "   1.82605493e-06   1.82600206e-06   1.82596420e-06   1.82592316e-06\n",
      "   1.82583233e-06   1.82567294e-06   1.82550355e-06   1.82542988e-06\n",
      "   1.82552549e-06   1.82576355e-06   1.82603981e-06   1.82622443e-06\n",
      "   1.82624410e-06   1.82611768e-06   1.82596455e-06   1.82591793e-06\n",
      "   1.82602412e-06   1.82618555e-06   1.82623694e-06   1.82608221e-06\n",
      "   1.82578367e-06   1.82551082e-06   1.82541544e-06   1.82552253e-06\n",
      "   1.82573524e-06   1.82591384e-06   1.82597921e-06   1.82594806e-06\n",
      "   1.82589440e-06   1.82586950e-06   3.38424754e-04   3.24548734e-03\n",
      "   1.60703361e-02   4.97162379e-02   1.08704068e-01   1.74971282e-01]\n",
      "Write address argmax - 0\n",
      "\n",
      "\n",
      "Step 7 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "w rotations - [ 0.49342054  0.20374779  0.30283168]\n",
      "Write address -\n",
      "[  2.15014070e-01   2.01268032e-01   1.40237689e-01   7.13137984e-02\n",
      "   2.53018476e-02   5.85788209e-03   7.60102936e-04   4.39608448e-05\n",
      "   2.16096097e-07   2.16093468e-07   2.16084743e-07   2.16072564e-07\n",
      "   2.16061778e-07   2.16054886e-07   2.16050296e-07   2.16043674e-07\n",
      "   2.16032504e-07   2.16018762e-07   2.16008800e-07   2.16009255e-07\n",
      "   2.16023295e-07   2.16048761e-07   2.16080153e-07   2.16111573e-07\n",
      "   2.16139227e-07   2.16161467e-07   2.16177384e-07   2.16185995e-07\n",
      "   2.16187047e-07   2.16181334e-07   2.16170619e-07   2.16156252e-07\n",
      "   2.16138702e-07   2.16118394e-07   2.16097646e-07   2.16081318e-07\n",
      "   2.16074881e-07   2.16081361e-07   2.16098030e-07   2.16117385e-07\n",
      "   2.16131298e-07   2.16135021e-07   2.16128470e-07   2.16114245e-07\n",
      "   2.16095657e-07   2.16077197e-07   2.16063384e-07   2.16057856e-07\n",
      "   2.16060258e-07   2.16066638e-07   2.16072252e-07   2.16074980e-07\n",
      "   2.16076614e-07   2.16080963e-07   2.16090470e-07   2.16104723e-07\n",
      "   2.16120696e-07   2.16134993e-07   2.16145096e-07   2.16150255e-07\n",
      "   2.16151179e-07   2.16148962e-07   2.16144002e-07   2.16136158e-07\n",
      "   2.16126494e-07   2.16117343e-07   2.16112454e-07   2.16114103e-07\n",
      "   2.16121876e-07   2.16131156e-07   2.16135604e-07   2.16129635e-07\n",
      "   2.16112724e-07   2.16089276e-07   2.16066894e-07   2.16051106e-07\n",
      "   2.16043077e-07   2.16040235e-07   2.16039737e-07   2.16040959e-07\n",
      "   2.16045379e-07   2.16054488e-07   2.16068159e-07   2.16084715e-07\n",
      "   2.16101625e-07   2.16116206e-07   2.16125798e-07   2.16129010e-07\n",
      "   2.16126537e-07   2.16120981e-07   2.16114785e-07   2.16108930e-07\n",
      "   2.16103629e-07   2.16098897e-07   2.16094676e-07   2.16089390e-07\n",
      "   2.16080494e-07   2.16067434e-07   2.16054573e-07   2.16049287e-07\n",
      "   2.16056762e-07   2.16075563e-07   2.16097732e-07   2.16113506e-07\n",
      "   2.16116689e-07   2.16108958e-07   2.16098499e-07   2.16094961e-07\n",
      "   2.16101412e-07   2.16111545e-07   2.16114074e-07   2.16102279e-07\n",
      "   2.16079656e-07   2.16058169e-07   2.16049031e-07   2.16055327e-07\n",
      "   2.16070859e-07   2.16085624e-07   2.16092872e-07   2.16092559e-07\n",
      "   2.16089063e-07   1.98010512e-05   3.62032151e-04   3.00138374e-03\n",
      "   1.43756103e-02   4.58992198e-02   1.03765361e-01   1.72757074e-01]\n",
      "Write address argmax - 0\n",
      "\n",
      "\n",
      "Step 8 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 1.]\n",
      "Write gamma - [ 1.48601139]\n",
      "w rotations - [ 0.4987697   0.35276693  0.14846341]\n",
      "Write address -\n",
      "[  1.99392110e-01   1.92972347e-01   1.44659057e-01   8.28017294e-02\n",
      "   3.52705531e-02   1.07071027e-02   2.15868372e-03   2.52886181e-04\n",
      "   1.44615469e-05   1.21593723e-06   1.21592961e-06   1.21591859e-06\n",
      "   1.21590836e-06   1.21590097e-06   1.21589517e-06   1.21588823e-06\n",
      "   1.21587789e-06   1.21586572e-06   1.21585674e-06   1.21585674e-06\n",
      "   1.21586902e-06   1.21589221e-06   1.21592177e-06   1.21595235e-06\n",
      "   1.21598009e-06   1.21600272e-06   1.21601897e-06   1.21602818e-06\n",
      "   1.21603023e-06   1.21602557e-06   1.21601568e-06   1.21600169e-06\n",
      "   1.21598464e-06   1.21596509e-06   1.21594530e-06   1.21592961e-06\n",
      "   1.21592279e-06   1.21592745e-06   1.21594178e-06   1.21595917e-06\n",
      "   1.21597259e-06   1.21597714e-06   1.21597225e-06   1.21595940e-06\n",
      "   1.21594212e-06   1.21592461e-06   1.21591142e-06   1.21590483e-06\n",
      "   1.21590563e-06   1.21591074e-06   1.21591586e-06   1.21591904e-06\n",
      "   1.21592166e-06   1.21592632e-06   1.21593519e-06   1.21594837e-06\n",
      "   1.21596349e-06   1.21597725e-06   1.21598771e-06   1.21599351e-06\n",
      "   1.21599510e-06   1.21599317e-06   1.21598862e-06   1.21598123e-06\n",
      "   1.21597225e-06   1.21596383e-06   1.21595906e-06   1.21595974e-06\n",
      "   1.21596611e-06   1.21597395e-06   1.21597770e-06   1.21597270e-06\n",
      "   1.21595781e-06   1.21593644e-06   1.21591518e-06   1.21589903e-06\n",
      "   1.21588971e-06   1.21588585e-06   1.21588482e-06   1.21588630e-06\n",
      "   1.21589073e-06   1.21589937e-06   1.21591222e-06   1.21592780e-06\n",
      "   1.21594417e-06   1.21595860e-06   1.21596827e-06   1.21597225e-06\n",
      "   1.21597088e-06   1.21596611e-06   1.21596008e-06   1.21595428e-06\n",
      "   1.21594883e-06   1.21594417e-06   1.21593962e-06   1.21593393e-06\n",
      "   1.21592518e-06   1.21591347e-06   1.21590210e-06   1.21589721e-06\n",
      "   1.21590313e-06   1.21591893e-06   1.21593894e-06   1.21595406e-06\n",
      "   1.21595895e-06   1.21595383e-06   1.21594564e-06   1.21594189e-06\n",
      "   1.21594621e-06   1.21595372e-06   1.21595565e-06   1.21594599e-06\n",
      "   1.21592689e-06   1.21590767e-06   1.21589790e-06   1.21590131e-06\n",
      "   1.21591393e-06   1.21592734e-06   1.21593530e-06   1.21593655e-06\n",
      "   5.20581807e-06   8.45881295e-05   7.97054032e-04   4.52000089e-03\n",
      "   1.73527859e-02   4.81378250e-02   1.00286767e-01   1.60454482e-01]\n",
      "Write address argmax - 0\n",
      "\n",
      "\n",
      "Step 9 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 1.15783131]\n",
      "Write gamma - [ 1.]\n",
      "w rotations - [ 0.40552849  0.27419463  0.32027689]\n",
      "Write address -\n",
      "[  2.31918857e-01   2.06351966e-01   1.30593792e-01   5.79113774e-02\n",
      "   1.75342765e-02   3.48054245e-03   4.25500533e-04   2.90454900e-05\n",
      "   9.77174182e-07   2.75382224e-08   1.07148530e-08   1.07147766e-08\n",
      "   1.07147118e-08   1.07146603e-08   1.07146150e-08   1.07145555e-08\n",
      "   1.07144800e-08   1.07144018e-08   1.07143574e-08   1.07143787e-08\n",
      "   1.07144826e-08   1.07146532e-08   1.07148628e-08   1.07150742e-08\n",
      "   1.07152607e-08   1.07154152e-08   1.07155191e-08   1.07155751e-08\n",
      "   1.07155813e-08   1.07155431e-08   1.07154658e-08   1.07153628e-08\n",
      "   1.07152349e-08   1.07150981e-08   1.07149667e-08   1.07148734e-08\n",
      "   1.07148432e-08   1.07148885e-08   1.07149889e-08   1.07150990e-08\n",
      "   1.07151790e-08   1.07151967e-08   1.07151505e-08   1.07150564e-08\n",
      "   1.07149365e-08   1.07148219e-08   1.07147384e-08   1.07147029e-08\n",
      "   1.07147118e-08   1.07147455e-08   1.07147748e-08   1.07147988e-08\n",
      "   1.07148237e-08   1.07148663e-08   1.07149365e-08   1.07150298e-08\n",
      "   1.07151337e-08   1.07152252e-08   1.07152944e-08   1.07153300e-08\n",
      "   1.07153362e-08   1.07153202e-08   1.07152829e-08   1.07152287e-08\n",
      "   1.07151665e-08   1.07151150e-08   1.07150919e-08   1.07151044e-08\n",
      "   1.07151470e-08   1.07151932e-08   1.07152012e-08   1.07151505e-08\n",
      "   1.07150351e-08   1.07148885e-08   1.07147509e-08   1.07146469e-08\n",
      "   1.07145848e-08   1.07145590e-08   1.07145555e-08   1.07145706e-08\n",
      "   1.07146114e-08   1.07146780e-08   1.07147722e-08   1.07148841e-08\n",
      "   1.07149969e-08   1.07150884e-08   1.07151505e-08   1.07151701e-08\n",
      "   1.07151568e-08   1.07151248e-08   1.07150839e-08   1.07150431e-08\n",
      "   1.07150031e-08   1.07149702e-08   1.07149347e-08   1.07148876e-08\n",
      "   1.07148201e-08   1.07147393e-08   1.07146736e-08   1.07146603e-08\n",
      "   1.07147153e-08   1.07148299e-08   1.07149560e-08   1.07150440e-08\n",
      "   1.07150653e-08   1.07150298e-08   1.07149871e-08   1.07149756e-08\n",
      "   1.07150067e-08   1.07150431e-08   1.07150333e-08   1.07149516e-08\n",
      "   1.07148237e-08   1.07147100e-08   1.07146638e-08   1.07146967e-08\n",
      "   1.07147784e-08   1.07148628e-08   1.07149081e-08   2.22508287e-08\n",
      "   6.09493895e-07   1.77601451e-05   2.65254814e-04   2.26614811e-03\n",
      "   1.21562807e-02   4.34179120e-02   1.07104048e-01   1.86525598e-01]\n",
      "Write address argmax - 0\n",
      "\n",
      "Epoch - 1, Mean error of final batch in epoch - 0.97\n",
      "Epoch - 2, Mean error of final batch in epoch - 0.9295\n",
      "Epoch - 3, Mean error of final batch in epoch - 0.903\n",
      "Epoch - 4, Mean error of final batch in epoch - 0.88825\n",
      "Epoch - 5, Mean error of final batch in epoch - 0.8835\n",
      "Epoch - 6, Mean error of final batch in epoch - 0.88275\n",
      "Epoch - 7, Mean error of final batch in epoch - 0.86325\n",
      "Epoch - 8, Mean error of final batch in epoch - 0.86125\n",
      "Epoch - 9, Mean error of final batch in epoch - 0.84925\n",
      "Epoch - 10, Mean error of final batch in epoch - 0.83575\n",
      "Epoch - 11, Mean error of final batch in epoch - 0.83925\n",
      "Epoch - 12, Mean error of final batch in epoch - 0.823\n",
      "Epoch - 13, Mean error of final batch in epoch - 0.832\n",
      "Epoch - 14, Mean error of final batch in epoch - 0.81025\n",
      "Epoch - 15, Mean error of final batch in epoch - 0.80525\n",
      "Epoch - 16, Mean error of final batch in epoch - 0.79725\n",
      "Epoch - 17, Mean error of final batch in epoch - 0.7955\n",
      "Epoch - 18, Mean error of final batch in epoch - 0.78675\n",
      "Epoch - 19, Mean error of final batch in epoch - 0.77575\n",
      "Epoch - 20, Mean error of final batch in epoch - 0.76975\n",
      "Epoch - 21, Mean error of final batch in epoch - 0.7545\n",
      "Epoch - 22, Mean error of final batch in epoch - 0.7315\n",
      "Epoch - 23, Mean error of final batch in epoch - 0.7295\n",
      "Epoch - 24, Mean error of final batch in epoch - 0.70775\n",
      "Epoch - 25, Mean error of final batch in epoch - 0.70575\n",
      "\n",
      "Step 0 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 1.24557543]\n",
      "Write gamma - [ 1.23174655]\n",
      "w rotations - [ 0.36716637  0.32516429  0.30766931]\n",
      "Write address -\n",
      "[  1.00000036e+00   2.43290316e-07   1.74386386e-07   6.83084977e-07\n",
      "   2.25341328e-08   4.13006546e-07   5.86311955e-07   6.95028319e-08\n",
      "   3.57480275e-07   1.87145588e-07   5.11511928e-07   4.51672662e-07\n",
      "   6.89835929e-07   7.14393707e-07   1.23595001e-08   8.69250641e-07\n",
      "   9.58399937e-07   9.03848161e-07   3.33985810e-07   6.47845013e-07\n",
      "   5.74375633e-07   4.82504618e-07   4.06901961e-07   1.56278489e-07\n",
      "   6.65410141e-07   2.08029263e-07   3.31661710e-07   1.61827799e-07\n",
      "   8.88777379e-07   1.64107675e-07   5.48600894e-07   5.05597598e-07\n",
      "   9.64047103e-07   7.78143658e-07   3.92209301e-07   2.35534898e-08\n",
      "   9.99872100e-07   1.04757191e-07   7.49707212e-08   9.04364356e-07\n",
      "   8.68381619e-07   9.32006742e-07   3.17937975e-07   2.30357415e-07\n",
      "   2.50479815e-07   7.77741434e-07   3.25633408e-07   3.79324661e-07\n",
      "   4.25024751e-07   6.66320773e-07   4.90408070e-07   3.87704006e-07\n",
      "   5.93090066e-09   3.85738623e-08   6.66248809e-07   5.09771212e-07\n",
      "   1.98526386e-08   1.40302902e-07   8.18535455e-07   2.70499584e-07\n",
      "   1.59052604e-07   5.62864670e-07   7.43962175e-07   9.28923953e-07\n",
      "   5.50641801e-07   5.40090696e-07   6.56771192e-07   3.65139613e-07\n",
      "   7.13334430e-07   9.24493804e-07   7.55845690e-07   2.94275054e-07\n",
      "   7.48277046e-07   9.70597171e-07   3.30400951e-07   3.44107036e-07\n",
      "   7.74604530e-07   7.39047664e-07   5.46114443e-08   5.39172902e-07\n",
      "   3.08844335e-07   3.39034791e-08   3.34772835e-07   2.66769774e-07\n",
      "   5.57713406e-07   6.43213014e-07   6.63055630e-07   3.29365122e-07\n",
      "   8.73279077e-07   7.90837191e-07   4.68324771e-07   7.97146583e-07\n",
      "   2.16466546e-07   9.61293836e-07   2.75981535e-07   5.30613534e-07\n",
      "   1.68880817e-07   9.69357075e-07   1.01731779e-07   7.69729127e-07\n",
      "   6.17738579e-07   3.70471476e-08   2.97656413e-07   2.19494339e-07\n",
      "   5.47295429e-07   8.97875680e-07   9.69154712e-07   8.69323003e-07\n",
      "   4.64217663e-07   4.24423803e-07   3.21018803e-07   5.92009428e-07\n",
      "   2.74610512e-09   5.54572125e-07   2.13679201e-07   2.40612028e-08\n",
      "   9.93544973e-07   9.53576773e-07   4.91806134e-07   8.76019612e-07\n",
      "   6.39712198e-07   5.14493081e-07   9.61260525e-07   8.44791032e-07\n",
      "   5.58824183e-07   6.18079412e-07   6.28626708e-07   9.38083417e-07]\n",
      "Write address argmax - 0\n",
      "\n",
      "\n",
      "Step 1 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 1.]\n",
      "Write gamma - [ 1.30498791]\n",
      "w rotations - [ 0.22484657  0.56738883  0.20776457]\n",
      "Write address -\n",
      "[  3.75183135e-01   3.01766753e-01   7.66687478e-08   7.32686090e-08\n",
      "   7.60957661e-08   7.58526113e-08   7.69426975e-08   7.39821928e-08\n",
      "   6.65660025e-08   7.55619993e-08   7.88435486e-08   8.98102925e-08\n",
      "   9.53473815e-08   8.49062189e-08   8.69210979e-08   9.60747357e-08\n",
      "   1.16629124e-07   1.03418785e-07   9.43958156e-08   8.83731559e-08\n",
      "   9.12120797e-08   8.54459330e-08   7.58142420e-08   7.93099346e-08\n",
      "   7.66025110e-08   7.86131054e-08   6.82529517e-08   8.29228028e-08\n",
      "   8.14480501e-08   8.72839578e-08   8.05254743e-08   9.84912134e-08\n",
      "   1.05420099e-07   1.01551215e-07   7.87611825e-08   8.32033820e-08\n",
      "   7.99731694e-08   7.73266891e-08   7.61713750e-08   9.62836708e-08\n",
      "   1.15646863e-07   1.01764115e-07   8.48940758e-08   6.99845231e-08\n",
      "   8.04844618e-08   8.41957828e-08   8.51630659e-08   7.78269893e-08\n",
      "   8.56752891e-08   8.89072638e-08   8.72160939e-08   7.21266886e-08\n",
      "   6.12367614e-08   6.78613503e-08   8.09409997e-08   7.92869912e-08\n",
      "   6.63133832e-08   7.40946859e-08   8.16012005e-08   7.95848649e-08\n",
      "   7.41543715e-08   8.62485194e-08   1.04377854e-07   1.04423897e-07\n",
      "   9.81783259e-08   9.22290795e-08   8.82771900e-08   9.12823026e-08\n",
      "   9.89982212e-08   1.08533278e-07   9.77460104e-08   9.24183539e-08\n",
      "   9.94365692e-08   1.00395042e-07   8.86893829e-08   8.49989590e-08\n",
      "   9.57846495e-08   8.83761828e-08   8.09761858e-08   7.35030810e-08\n",
      "   7.17937709e-08   6.67575790e-08   6.70122731e-08   7.81493767e-08\n",
      "   8.61413980e-08   9.52616261e-08   8.98761172e-08   9.42170360e-08\n",
      "   9.93084228e-08   1.01659808e-07   9.89695366e-08   8.69337029e-08\n",
      "   9.62713571e-08   8.70852332e-08   9.13928986e-08   7.48446283e-08\n",
      "   8.93055798e-08   8.22540471e-08   9.25600219e-08   8.74756338e-08\n",
      "   8.47025419e-08   7.24893923e-08   6.51208580e-08   7.59155441e-08\n",
      "   9.07341686e-08   1.09163416e-07   1.16743976e-07   1.05844990e-07\n",
      "   9.18440861e-08   7.95966386e-08   8.22537132e-08   7.36292805e-08\n",
      "   7.68410047e-08   7.06740124e-08   6.95113371e-08   7.92193831e-08\n",
      "   9.95373597e-08   1.09351234e-07   1.05144935e-07   9.94427580e-08\n",
      "   9.87676927e-08   1.00706458e-07   1.07145567e-07   1.07271703e-07\n",
      "   9.83585053e-08   9.38122042e-08   1.02734518e-07   3.23040366e-01]\n",
      "Write address argmax - 0\n",
      "\n",
      "\n",
      "Step 2 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 1.]\n",
      "Write gamma - [ 2.08040047]\n",
      "w rotations - [ 0.31657356  0.54059702  0.14282949]\n",
      "Write address -\n",
      "[  3.56781095e-01   1.26515403e-01   4.20579389e-02   2.53972896e-08\n",
      "   2.53908556e-08   2.54263313e-08   2.53805688e-08   2.52374921e-08\n",
      "   2.53243400e-08   2.53965435e-08   2.56689443e-08   2.58634003e-08\n",
      "   2.57891326e-08   2.57874628e-08   2.58950532e-08   2.63335789e-08\n",
      "   2.63033808e-08   2.61847806e-08   2.59308788e-08   2.58807873e-08\n",
      "   2.57606203e-08   2.55699781e-08   2.55273882e-08   2.54425423e-08\n",
      "   2.54813415e-08   2.52968206e-08   2.54943888e-08   2.55039261e-08\n",
      "   2.56900012e-08   2.56026311e-08   2.59082693e-08   2.61117545e-08\n",
      "   2.62077968e-08   2.58247503e-08   2.57193857e-08   2.55474113e-08\n",
      "   2.55071537e-08   2.54478945e-08   2.57752237e-08   2.62483475e-08\n",
      "   2.62687028e-08   2.59997375e-08   2.55317367e-08   2.55039758e-08\n",
      "   2.55462087e-08   2.56562647e-08   2.55582737e-08   2.56510333e-08\n",
      "   2.57151189e-08   2.57583217e-08   2.55029331e-08   2.51972967e-08\n",
      "   2.51411851e-08   2.53457504e-08   2.54498662e-08   2.52953569e-08\n",
      "   2.53308947e-08   2.54329784e-08   2.54995882e-08   2.54386929e-08\n",
      "   2.55997907e-08   2.59670774e-08   2.61721631e-08   2.61794089e-08\n",
      "   2.60314668e-08   2.58804036e-08   2.58674184e-08   2.59984763e-08\n",
      "   2.62394568e-08   2.61657824e-08   2.60582453e-08   2.60750781e-08\n",
      "   2.61065516e-08   2.59525823e-08   2.58124757e-08   2.59009081e-08\n",
      "   2.58221515e-08   2.57101185e-08   2.54803432e-08   2.53512820e-08\n",
      "   2.52037111e-08   2.51624090e-08   2.53264201e-08   2.55450185e-08\n",
      "   2.58319073e-08   2.58521879e-08   2.59495980e-08   2.60346784e-08\n",
      "   2.61395083e-08   2.61413700e-08   2.59261110e-08   2.59890776e-08\n",
      "   2.58153161e-08   2.58870561e-08   2.55678074e-08   2.57339465e-08\n",
      "   2.56045887e-08   2.58291522e-08   2.57662887e-08   2.57485020e-08\n",
      "   2.54826755e-08   2.52515395e-08   2.53109622e-08   2.55978172e-08\n",
      "   2.60938293e-08   2.64517439e-08   2.64315076e-08   2.61575259e-08\n",
      "   2.57746358e-08   2.56462336e-08   2.54351722e-08   2.54485837e-08\n",
      "   2.53078269e-08   2.52653773e-08   2.53874894e-08   2.58032973e-08\n",
      "   2.61798281e-08   2.63053188e-08   2.62387836e-08   2.61599737e-08\n",
      "   2.61526356e-08   2.62752344e-08   2.63349484e-08   2.62203042e-08\n",
      "   2.60789097e-08   2.61467825e-08   1.70540914e-01   3.04098904e-01]\n",
      "Write address argmax - 0\n",
      "\n",
      "\n",
      "Step 3 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 1.]\n",
      "Write gamma - [ 2.53082418]\n",
      "w rotations - [ 0.71274966  0.24038963  0.04686074]\n",
      "Write address -\n",
      "[  2.29569167e-01   5.56561574e-02   3.82056297e-03   1.22580052e-04\n",
      "   1.77746099e-12   1.77740906e-12   1.77709605e-12   1.77707859e-12\n",
      "   1.77724469e-12   1.77790302e-12   1.77863052e-12   1.77884779e-12\n",
      "   1.77886005e-12   1.77902983e-12   1.78000702e-12   1.78050489e-12\n",
      "   1.78046499e-12   1.77981826e-12   1.77936962e-12   1.77894700e-12\n",
      "   1.77841238e-12   1.77804982e-12   1.77773735e-12   1.77769442e-12\n",
      "   1.77733501e-12   1.77752984e-12   1.77767859e-12   1.77815434e-12\n",
      "   1.77820118e-12   1.77879337e-12   1.77949430e-12   1.78007164e-12\n",
      "   1.77953908e-12   1.77894526e-12   1.77829247e-12   1.77796308e-12\n",
      "   1.77771318e-12   1.77826308e-12   1.77952921e-12   1.78027796e-12\n",
      "   1.78002025e-12   1.77881082e-12   1.77808365e-12   1.77789315e-12\n",
      "   1.77814165e-12   1.77809828e-12   1.77822362e-12   1.77840414e-12\n",
      "   1.77860949e-12   1.77819359e-12   1.77732872e-12   1.77673870e-12\n",
      "   1.77691585e-12   1.77732362e-12   1.77724664e-12   1.77719287e-12\n",
      "   1.77735311e-12   1.77761766e-12   1.77762796e-12   1.77790671e-12\n",
      "   1.77877515e-12   1.77967797e-12   1.78011588e-12   1.77994110e-12\n",
      "   1.77948118e-12   1.77920721e-12   1.77937005e-12   1.77998295e-12\n",
      "   1.78018245e-12   1.78001234e-12   1.77988429e-12   1.77990944e-12\n",
      "   1.77965390e-12   1.77922108e-12   1.77915376e-12   1.77902907e-12\n",
      "   1.77876626e-12   1.77814989e-12   1.77757776e-12   1.77702471e-12\n",
      "   1.77670921e-12   1.77690588e-12   1.77749775e-12   1.77839134e-12\n",
      "   1.77887132e-12   1.77923204e-12   1.77951957e-12   1.77987161e-12\n",
      "   1.78003912e-12   1.77967515e-12   1.77955296e-12   1.77917511e-12\n",
      "   1.77914877e-12   1.77851885e-12   1.77851527e-12   1.77828802e-12\n",
      "   1.77866391e-12   1.77873135e-12   1.77874035e-12   1.77816952e-12\n",
      "   1.77740602e-12   1.77712120e-12   1.77762904e-12   1.77895448e-12\n",
      "   1.78036741e-12   1.78099235e-12   1.78061916e-12   1.77954873e-12\n",
      "   1.77871997e-12   1.77796406e-12   1.77768314e-12   1.77731538e-12\n",
      "   1.77707903e-12   1.77719623e-12   1.77812431e-12   1.77939618e-12\n",
      "   1.78028566e-12   1.78049329e-12   1.78032816e-12   1.78018960e-12\n",
      "   1.78037955e-12   1.78063250e-12   1.78054034e-12   1.78016423e-12\n",
      "   1.78007609e-12   3.59506495e-02   2.16196388e-01   4.58684504e-01]\n",
      "Write address argmax - 127\n",
      "\n",
      "\n",
      "Step 4 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 1.68103242]\n",
      "Write gamma - [ 1.60895228]\n",
      "w rotations - [ 0.90315533  0.03624095  0.06060372]\n",
      "Write address -\n",
      "[  1.13252260e-01   3.69669776e-03   1.21492203e-05   6.15290174e-09\n",
      "   5.54911585e-13   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.42944784e-15   4.42944784e-15   4.42944784e-15   4.42944784e-15\n",
      "   4.06793588e-05   1.05117196e-02   2.37654030e-01   6.34832442e-01]\n",
      "Write address argmax - 127\n",
      "\n",
      "\n",
      "Step 5 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 2.43105984]\n",
      "Write gamma - [ 1.07153332]\n",
      "w rotations - [ 0.91502124  0.01641495  0.0685638 ]\n",
      "Write address -\n",
      "[  7.43612349e-02   1.08874764e-03   2.54069641e-06   9.43635392e-10\n",
      "   3.86627064e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   3.86394944e-10\n",
      "   3.86394944e-10   3.86394944e-10   3.86394944e-10   1.65983838e-09\n",
      "   6.39056043e-06   2.74042925e-03   1.73188493e-01   7.48612106e-01]\n",
      "Write address argmax - 127\n",
      "\n",
      "\n",
      "Step 6 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 2.50445628]\n",
      "Write gamma - [ 1.]\n",
      "w rotations - [ 0.91455865  0.01267765  0.07276379]\n",
      "Write address -\n",
      "[  1.08909734e-01   4.49484633e-03   4.20935576e-05   4.69901693e-07\n",
      "   3.95484250e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95467964e-07   3.95467964e-07\n",
      "   3.95467964e-07   3.95467964e-07   3.95476832e-07   4.40559177e-07\n",
      "   2.71754689e-05   3.90975364e-03   1.60000369e-01   7.22568035e-01]\n",
      "Write address argmax - 127\n",
      "\n",
      "\n",
      "Step 7 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 1.85462785]\n",
      "Write gamma - [ 1.]\n",
      "w rotations - [ 0.85569924  0.01350653  0.13079421]\n",
      "Write address -\n",
      "[  1.52219489e-01   1.20354695e-02   3.66517866e-04   4.49707386e-06\n",
      "   1.40071870e-06   1.39528959e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528834e-06   1.39528834e-06   1.39528834e-06\n",
      "   1.39528834e-06   1.39528856e-06   1.39586803e-06   1.77598611e-06\n",
      "   7.54423963e-05   5.60638402e-03   1.55755609e-01   6.73768103e-01]\n",
      "Write address argmax - 127\n",
      "\n",
      "\n",
      "Step 8 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 1.52845895]\n",
      "Write gamma - [ 1.21376538]\n",
      "w rotations - [ 0.81488138  0.03133743  0.15378124]\n",
      "Write address -\n",
      "[  2.18515083e-01   3.02103106e-02   1.88862125e-03   5.27988195e-05\n",
      "   2.80527706e-06   2.39569749e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39498672e-06   2.39498672e-06   2.39498672e-06\n",
      "   2.39498672e-06   2.39499468e-06   2.40062332e-06   3.72077693e-06\n",
      "   1.41493205e-04   6.91109104e-03   1.43096462e-01   5.98896265e-01]\n",
      "Write address argmax - 127\n",
      "\n",
      "\n",
      "Step 9 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma  - [ 2.50947833]\n",
      "Write gamma - [ 1.57011437]\n",
      "w rotations - [ 0.57510304  0.11648321  0.30841368]\n",
      "Write address -\n",
      "[  2.62655050e-01   4.06496674e-02   2.67192954e-03   7.74318542e-05\n",
      "   1.29230455e-06   3.01250992e-07   2.94544208e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532725e-07   2.94532725e-07   2.94532725e-07   2.94532725e-07\n",
      "   2.94532754e-07   2.94551995e-07   2.99398408e-07   9.40383870e-07\n",
      "   7.71232226e-05   4.86570783e-03   1.14137560e-01   5.74829459e-01]\n",
      "Write address argmax - 127\n",
      "\n",
      "Epoch - 26, Mean error of final batch in epoch - 0.683\n",
      "Epoch - 27, Mean error of final batch in epoch - 0.6785\n",
      "Epoch - 28, Mean error of final batch in epoch - 0.66325\n",
      "Epoch - 29, Mean error of final batch in epoch - 0.64575\n",
      "Epoch - 30, Mean error of final batch in epoch - 0.633\n",
      "Epoch - 31, Mean error of final batch in epoch - 0.6215\n",
      "Epoch - 32, Mean error of final batch in epoch - 0.60125\n",
      "Epoch - 33, Mean error of final batch in epoch - 0.58375\n",
      "Epoch - 34, Mean error of final batch in epoch - 0.57575\n",
      "Epoch - 35, Mean error of final batch in epoch - 0.5515\n",
      "Epoch - 36, Mean error of final batch in epoch - 0.53925\n",
      "Epoch - 37, Mean error of final batch in epoch - 0.5115\n",
      "Epoch - 38, Mean error of final batch in epoch - 0.508\n",
      "Epoch - 39, Mean error of final batch in epoch - 0.4975\n",
      "Epoch - 40, Mean error of final batch in epoch - 0.4665\n",
      "Epoch - 41, Mean error of final batch in epoch - 0.45525\n",
      "Epoch - 42, Mean error of final batch in epoch - 0.454\n",
      "Epoch - 43, Mean error of final batch in epoch - 0.4245\n",
      "Epoch - 44, Mean error of final batch in epoch - 0.42\n",
      "Epoch - 45, Mean error of final batch in epoch - 0.39175\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "# An annoying thing here is that we cannot use a list as a key in a \n",
    "# dictionary. The workaround we found on StackOverflow here:\n",
    "# http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "\n",
    "# epoch is a global var\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences\n",
    "        for z in range(batch_size):\n",
    "            # construct a sequence from 0,...,num_classes - 3 then append initial and terminal symbols\n",
    "            a = [random.randint(0,num_classes-3) for k in range(N-2)]\n",
    "            junk = [0 for k in range(Ntest-N+2)]\n",
    "            f_junk = [0 for k in range(Ntest_out-N_out)]\n",
    "            fa = func_to_learn(a) + f_junk\n",
    "            a = [init_symbol] + a + [term_symbol] + junk\n",
    "            a_onehot = [one_hots[e] for e in a]\n",
    "            fa_onehot = [one_hots[e] for e in fa]\n",
    "            inp.append(np.array(a_onehot))\n",
    "            out.append(np.array(fa_onehot))        \n",
    "        \n",
    "        # Note we have to feed all the input nodes, including those with d > N, \n",
    "        # although these inputs do not play any role in the running of the RNN\n",
    "        # during training (the additional nodes are there so we can use the\n",
    "        # same network for testing on longer sequences)\n",
    "        feed_dict = {}\n",
    "        for d in range(Ntest):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(Ntest_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "        \n",
    "        # for the first batch in an epoch, we have some logging\n",
    "        if( j == 0 and i % 25 == 0 ):\n",
    "            ss_val, gamma_reads_val, gamma_writes_val, read_addresses_val, write_addresses_val = sess.run([ss, gamma_reads,gamma_writes,read_addresses,write_addresses],feed_dict)\n",
    "    \n",
    "            s = 0\n",
    "            for r in range(len(write_addresses_val)):\n",
    "                print(\"\")\n",
    "                print(\"Step \" + str(s) + \" of the RNN run on the first input of first batch of this epoch\")\n",
    "                print(\"Read gamma  - \" + str(gamma_reads_val[r]))\n",
    "                print(\"Write gamma - \" + str(gamma_writes_val[r]))\n",
    "                print(\"w rotations - \" + str(ss_val[r]))\n",
    "                print(\"Write address -\")\n",
    "                print(write_addresses_val[r])\n",
    "                print(\"Write address argmax - \" + str(write_addresses_val[r].argmax()))\n",
    "                print(\"\")\n",
    "                s = s + 1\n",
    "        \n",
    "        ##### Do gradient descent #####\n",
    "        summary,_ = sess.run([merged_summaries,minimize], feed_dict)\n",
    "        ########\n",
    "        \n",
    "        # Write out TensorBoard logs\n",
    "        file_writer.add_summary(summary)\n",
    "    current_mean = np.mean(sess.run(errors, feed_dict))\n",
    "    print(\"Epoch - \" + str(i+1) + \", Mean error of final batch in epoch - \" + str(current_mean))\n",
    "    \n",
    "# Write out variables to disk\n",
    "#saver = tf.train.Saver()\n",
    "#save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "#sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took\", time.time() - pre_train_time, \"seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size,\n",
    "                                  memory_address_size, memory_content_size)\n",
    "    \n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,memory_address_size,memory_content_size, powers_ring1)\n",
    " \n",
    "# Restore the weights from training\n",
    "#sess = tf.Session()\n",
    "#saver.restore(sess,save_path)\n",
    "\n",
    "# Set up test graph\n",
    "reuse = True\n",
    "for i in range(Ntest):\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "\n",
    "rnn_outputs_test = []\n",
    "for i in range(Ntest_out):\n",
    "    output, state = cell(term_symbol_tensor,state,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "    \n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size])\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.softmax(logit) for logit in logits_test] \n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    # We sample each batch on the fly from the set of all sequences\n",
    "    for z in range(batch_size):\n",
    "        a = [random.randint(0,num_classes-3) for k in range(Ntest-2)]\n",
    "        fa = func_to_learn(a)\n",
    "        a = [init_symbol] + a + [term_symbol]\n",
    "        a_onehot = [one_hots[e] for e in a]\n",
    "        fa_onehot = [one_hots[e] for e in fa]\n",
    "        inp.append(np.array(a_onehot))\n",
    "        out.append(np.array(fa_onehot))        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(Ntest):\n",
    "        in_node = inputs[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest_out):\n",
    "        out_node = targets[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = np.mean(sess.run(errors_test, feed_dict))\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"N_out         - \" + str(N_out))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "print(\"Ntest_out     - \" + str(Ntest_out))\n",
    "print(\"ring 1 powers - \" + str(powers_ring1))\n",
    "print(\"ring 2 powers - \" + str(powers_ring2))\n",
    "print(\"# epochs      - \" + str(epoch))\n",
    "print(\"optimizer     - \" + str(model_optimizer))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"(css,mas,mcs) - (\" + str(controller_state_size) + \",\" + str(memory_address_size) + \",\" + str(memory_content_size) + \")\")\n",
    "#print(\"train percent - \" + str(training_percent))\n",
    "print(\"num_training  - \" + str(num_training) + \"/\" + str(num_classes**N))\n",
    "print(\"num_test      - \" + str(num_test) + \"/\" + str(num_classes**N))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error         - \" + str(final_error))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
