{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of the Linear Logic Recurrent Neural Network (LLRNN)\n",
    "#\n",
    "# Version 11.0\n",
    "\n",
    "###################\n",
    "# HYPERPARAMETERS #\n",
    "###################\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, mult_pattern_ntm\n",
    "task                  = 'copy' # copy, repeat copy, pattern i, mult pattern i, variable pattern i\n",
    "epoch                 = 200 # number of training epochs, default to 200\n",
    "num_classes           = 10 # number of symbols, INCLUDING initial and terminal symbols, default 10\n",
    "N                     = 30 # length of input sequences for training, default to 30\n",
    "Ntest                 = 35 # length of sequences for testing, default to 35\n",
    "batch_size            = 250 # default 250\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "num_training          = 10000 # default 10000\n",
    "num_test              = num_training\n",
    "term_symbol           = num_classes - 1\n",
    "init_symbol           = num_classes - 2\n",
    "div_symbol            = num_classes - 3\n",
    "learning_rate         = 1e-4 # default 1e-4\n",
    "memory_init_bias      = 1.0 # default 1.0\n",
    "use_curriculum        = True # default True\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "\n",
    "##################\n",
    "# MODEL SPECIFIC #\n",
    "##################\n",
    "\n",
    "ntm_memory_address_size   = 128 # number of memory locations, default 128\n",
    "ntm_memory_content_size   = 20 # size of vector stored at a memory location, default 20\n",
    "ntm_powers                = [0,-1,1] # powers of R used by controller, default [0,-1,1]\n",
    "\n",
    "pattern_ntm_powers               = [[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "pattern_ntm_powers_2_on_1        = [0,1,2] # allowed powers used by ring 2 to manipulate ring 1\n",
    "pattern_ntm_memory_address_sizes = [128, 20] # number of memory locations for the three rings\n",
    "pattern_ntm_memory_content_sizes = [20, 3] # size of content vector for each ring\n",
    "pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "mult_pattern_ntm_powers               = [[0,-1,1],[0,-1,1],[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "mult_pattern_ntm_powers_2_on_1        = [0,1,2] # allowed powers used by rings 2,3 to manipulate ring 1\n",
    "mult_pattern_ntm_memory_address_sizes = [128, 20, 20, 10] # number of memory locations for the rings\n",
    "mult_pattern_ntm_memory_content_sizes = [20, 3, 3, 2] # size of content vector for each ring\n",
    "mult_pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there\n",
    "# 2. The initial and terminal symbols are always from the end of the list of symbols, so they\n",
    "# are respectively num_classes - 2 and num_classes - 1. So the number of symbols which are\n",
    "# not initial or terminal is num_classes - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import pandas\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs\n",
    "\n",
    "assert use_model == 'ntm' or use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[-1, 0, 2, 0, 0, 8, 0, 2, 5, 7, 0, 5, 2, 2, 1, 0, 7, 4]\n",
      "is mapped to\n",
      "[0, 4, 4, 2, 2, 2, 0, 0, 5, 5, 5, 2, 2, 7, 7, 7, 5, 5, 0, 0, 0, 7, 7, 5, 5, 5, 0, 0, 2, 2, 2, 5, 5, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 0, 0, 0, 1, 1, 7, 7, 7, 0, 0, 4, 4, 4, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "# Below N_out and Ntest_out are the lengths of the outputs in both the training\n",
    "# and testing regimes respectively. Since outputs do not include the initial and terminal\n",
    "# symbols, these default to N - 2 and Ntest - 2 respectively.\n",
    "\n",
    "# Default sampling from space of inputs\n",
    "def generate_input_seq_default(max_symbol,input_length):\n",
    "    return [random.randint(0,max_symbol) for k in range(input_length)]\n",
    "\n",
    "generate_input_seq = generate_input_seq_default\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "#\n",
    "# In this task the input is simply copied to the output (although we\n",
    "# require the RNN to output the first output symbol after the last\n",
    "# input symbol has been read, so this effectively requires the system\n",
    "# to store the input and later retrieve it)\n",
    "\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N - 2\n",
    "    Ntest_out = Ntest - 2\n",
    "    seq_length_min = 7\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "#\n",
    "# In this task every digit of the input is repeated.\n",
    "#\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "\n",
    "if( task == 'repeat copy' ):\n",
    "    no_of_copies = 2\n",
    "    pattern = [0]*(no_of_copies - 1) + [1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = no_of_copies * (N - 2)\n",
    "    Ntest_out = no_of_copies * (Ntest - 2)\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 1\n",
    "if( task == 'pattern 1' ):\n",
    "    pattern = [0,1,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,c,c,d,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = (N - 2) + divmod(N - 2, 2)[0] # N - 2 plus the number of times 2 divides N - 2\n",
    "    Ntest_out = (Ntest - 2) + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 2\n",
    "if( task == 'pattern 2' ):\n",
    "    pattern = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = N - 2 + divmod(N - 2, 2)[0]\n",
    "    Ntest_out = Ntest - 2 + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 3\n",
    "if( task == 'pattern 3' ):\n",
    "    pattern = [0,2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,b,b,d,c,c,e,d,d,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 4 + (N - 2 - 2) * 3\n",
    "    Ntest_out = 4 + (Ntest - 2 - 2) * 3\n",
    "    seq_length_min = 7\n",
    "\n",
    "################\n",
    "# PATTERN TASK 4\n",
    "if( task == 'pattern 4' ):\n",
    "    pattern = [0,2,1,2,-2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,d,f,d,c,c,e,f,h,f,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 7\n",
    "\n",
    "################\n",
    "# PATTERN TASK 5\n",
    "if( task == 'pattern 5' ):\n",
    "    pattern = [4,1,1,-4] # so (a,b,c,d,e,f,...) goes to (a,e,f,g,k,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 7\n",
    "\n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 1\n",
    "if( task == 'mult pattern 1' or task == 'mult pattern 2'):\n",
    "    pattern1 = [1] # so (a,b,c,d,e,f,...) goes to (a,b,c,d,e,f,...)\n",
    "    pattern2 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 7\n",
    "    \n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 2\n",
    "if( task == 'mult pattern 2' ):\n",
    "    # Almost everything is the same as mult pattern 1, but in pattern 2 we \n",
    "    # make sure there is a div symbol somewhere in the sequence\n",
    "    def generate_input_seq_forcediv(max_symbol,input_length):\n",
    "        t = [random.randint(0,max_symbol) for k in range(input_length)]\n",
    "        div_pos = random.randint(0,len(t)-1)\n",
    "        t[div_pos] = div_symbol\n",
    "        return t\n",
    "    \n",
    "    generate_input_seq = generate_input_seq_forcediv\n",
    "\n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 3\n",
    "if( task == 'mult pattern 3'):\n",
    "    pattern1 = [1] # so (a,b,c,d,e,f,...) goes to (a,b,c,d,e,f,...)\n",
    "    pattern2 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    pattern3 = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2,pattern3],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 7\n",
    "    \n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 4\n",
    "if( task == 'mult pattern 4'):\n",
    "    pattern1 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    pattern2 = [2,-1] # so (a,b,c,d,e,f,...) goes to (a,c,b,d,c,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 7\n",
    "\n",
    "#########################\n",
    "# VARIABLE PATTERN TASK 1\n",
    "#\n",
    "# The input is a pattern together with a string to which we are supposed to apply the\n",
    "# pattern, separated by an initial symbol. There is no division symbol.\n",
    "\n",
    "def generate_input_seq_varpattern1(max_symbol,input_length):\n",
    "    varpatterns = [[1],[2],[0,1],[0,2],[1,2]]\n",
    "    vp = varpatterns[random.randint(0,len(varpatterns)-1)]\n",
    "    t = vp + [init_symbol] + [random.randint(0,max_symbol) for k in range(input_length-len(vp)-1)]\n",
    "    return t\n",
    "\n",
    "if( task == 'variable pattern 1'):\n",
    "    generate_input_seq = generate_input_seq_varpattern1\n",
    "    func_to_learn = lambda s: learnfuncs.f_varpattern(s,init_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 10\n",
    "    \n",
    "#########################\n",
    "# VARIABLE PATTERN TASK 2\n",
    "\n",
    "def generate_input_seq_varpattern2(max_symbol,input_length):\n",
    "    varpatterns = [[1],[2]]\n",
    "    varpatterns = varpatterns + [[0,1],[0,2],[1,2]]\n",
    "    varpatterns = varpatterns + [[0,1,0],[0,1,1],[0,1,2],[0,2,0],[0,2,1],[0,2,2],[1,1,2],[1,2,2]]\n",
    "    varpatterns = varpatterns + [[0,0,0,1],[0,0,0,2],[0,0,1,2],[0,1,1,2],[0,1,0,2],[0,2,0,2]]\n",
    "    vp = varpatterns[random.randint(0,len(varpatterns)-1)]\n",
    "    t = vp + [init_symbol] + [random.randint(0,max_symbol) for k in range(input_length-len(vp)-1)]\n",
    "    return t\n",
    "\n",
    "if( task == 'variable pattern 2'):\n",
    "    generate_input_seq = generate_input_seq_varpattern2\n",
    "    func_to_learn = lambda s: learnfuncs.f_varpattern(s,init_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 13\n",
    "\n",
    "#########################\n",
    "# VARIABLE PATTERN TASK 3\n",
    "#\n",
    "# In this task we randomly generate the pattern from the alphabet 0,1,2\n",
    "# We also generate longer sequences than in task 1 or 2. By default\n",
    "# we generate patterns between length 1 and 8\n",
    "\n",
    "def generate_input_seq_varpattern3(max_symbol,input_length):\n",
    "    while( True ):\n",
    "        vp_length = random.randint(1,8)\n",
    "        vp = [random.randint(0,2) for k in range(vp_length)]\n",
    "        \n",
    "        # We cannot allow patterns that are all zeros\n",
    "        if( reduce( lambda x,y : x + y, vp) > 0 ):\n",
    "            break\n",
    "    \n",
    "    t = vp + [init_symbol] + [random.randint(0,max_symbol) for k in range(input_length-len(vp)-1)]\n",
    "    return t\n",
    "\n",
    "if( task == 'variable pattern 3'):\n",
    "    generate_input_seq = generate_input_seq_varpattern3\n",
    "    func_to_learn = lambda s: learnfuncs.f_varpattern(s,init_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 20\n",
    "    \n",
    "#########################\n",
    "# VARIABLE PATTERN TASK 4\n",
    "#\n",
    "# In this task we randomly generate the pattern from the alphabet -2,-1,0,1,2\n",
    "\n",
    "def generate_input_seq_varpattern4(max_symbol,input_length):\n",
    "    \n",
    "    vp_length = random.randint(1,8)\n",
    "    \n",
    "    while( True ):    \n",
    "        vp = [random.randint(-2,2) for k in range(vp_length)]\n",
    "        \n",
    "        # We cannot allow patterns that add up to a non-positive integer\n",
    "        if( reduce( lambda x,y : x + y, vp) > 0 ):\n",
    "            break\n",
    "    \n",
    "    t = vp + [init_symbol] + [random.randint(0,max_symbol) for k in range(input_length-len(vp)-1)]\n",
    "    return t\n",
    "\n",
    "if( task == 'variable pattern 4'):\n",
    "    generate_input_seq = generate_input_seq_varpattern4\n",
    "    func_to_learn = lambda s: learnfuncs.f_varpattern(s,init_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 20\n",
    "\n",
    "# Make sure the given N is above the minimum sequence length\n",
    "assert N >= seq_length_min\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = generate_input_seq(num_classes-3,N-2)\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# INITIALISE STATE #\n",
    "####################\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "input_size = num_classes # dimension of the input space I\n",
    "\n",
    "#####\n",
    "# NTM\n",
    "\n",
    "def init_state_ntm(batch_size, css, mas, mcs):\n",
    "    state_size = css + 2*mas + mas * mcs\n",
    "    \n",
    "    ra = [0.0]*mas\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,mas]) + ra\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    init_read_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_write_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_memory = tf.truncated_normal([batch_size, mas*mcs], 0.0, 1e-6, dtype=tf.float32)\n",
    "    \n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "#############\n",
    "# PATTERN NTM\n",
    "\n",
    "def init_state_pattern_ntm(batch_size, css, mas, mcs):\n",
    "    # mas and mcs are arrays of address sizes and content sizes for rings\n",
    "    state_size = css\n",
    "    \n",
    "    init_address = []\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        state_size = state_size + mas[i] * mcs[i] # for memory vector\n",
    "        state_size = state_size + 2 * mas[i] # for addresses (read and write)\n",
    "    \n",
    "        ra = [0.0]*mas[i]\n",
    "        ra[0] = 1.0\n",
    "        init_address.append(np.zeros([batch_size,mas[i]]) + ra)\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    \n",
    "    tensor_list = [init_controller_state]\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        init_read_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        init_write_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        tensor_list = tensor_list + [init_read_address,init_write_address]\n",
    "        \n",
    "    for i in range(len(mas)):\n",
    "        # The first ring is initialised to zero, the rest differently\n",
    "        if( i == 0 ):\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "        else:\n",
    "            # This initialisation has the result of biasing the output of rings 2 and 3 to be\n",
    "            # \"no rotation\" and biasing ring 4 to say \"use ring 2\"\n",
    "            ra = [0.0]*mcs[i] \n",
    "            ra[0] = memory_init_bias\n",
    "            ra = np.zeros([batch_size,mas[i],mcs[i]]) + ra\n",
    "            ra = tf.constant(ra,dtype=tf.float32,shape=[batch_size,mas[i],mcs[i]])\n",
    "            ra = tf.reshape(ra,[batch_size,mas[i]*mcs[i]])\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32) + ra\n",
    "            #init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "            \n",
    "        tensor_list = tensor_list + [init_memory]\n",
    "    \n",
    "    state = tf.concat(tensor_list,1)\n",
    "\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "######################\n",
    "# MULTIPLE PATTERN NTM\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_55/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_54/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_53/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_52/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_51/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_50/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_49/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_48/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_47/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_46/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_45/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_44/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_43/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_42/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_41/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_40/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_39/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_38/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_37/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_36/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_35/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_34/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_33/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_32/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_31/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_30/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_29/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_28/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_27/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_26/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_25/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_24/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_23/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_22/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_21/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_20/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_19/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_18/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_17/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_15/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_13/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_11/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_9/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_7/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_5/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_3/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_1/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "\n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "read_addresses2 = []\n",
    "read_addresses3 = []\n",
    "read_addresses4 = []\n",
    "write_addresses = []\n",
    "write_addresses2 = []\n",
    "write_addresses3 = []\n",
    "write_addresses4 = []\n",
    "interps = []\n",
    "rnn_outputs = []\n",
    "m2 = []\n",
    "m3 = []\n",
    "m4 = []\n",
    "    \n",
    "for i in range(N + N_out):\n",
    "    \n",
    "    old_state = state\n",
    "\n",
    "    #### RUN MODEL ####\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "    ###################\n",
    "\n",
    "    reuse = True\n",
    "    \n",
    "    #### SET UP NODES FOR LOGGING #####\n",
    "    if( use_model == 'ntm' ):\n",
    "        h0, curr_read, curr_write, _ = tf.split(old_state, [controller_state_size,ntm_memory_address_size,\n",
    "                                                        ntm_memory_address_size,-1], 1)\n",
    "\n",
    "    if( use_model == 'pattern_ntm' ):\n",
    "        mas = pattern_ntm_memory_address_sizes\n",
    "        mcs = pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(old_state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],mas[0] * mcs[0],mas[1] * mcs[1]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        m1_state = ret[5]\n",
    "        m2_state = ret[6]\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm' ):\n",
    "        mas = mult_pattern_ntm_memory_address_sizes\n",
    "        mcs = mult_pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(old_state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],                        \n",
    "                            mas[2],mas[2],mas[3],mas[3],mas[0] * mcs[0],mas[1] * mcs[1],\n",
    "                            mas[2] * mcs[2],mas[3] * mcs[3]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        curr_read3 = ret[5]\n",
    "        curr_write3 = ret[6]\n",
    "        curr_read4 = ret[7]\n",
    "        curr_write4 = ret[8]\n",
    "        m1_state = ret[9]\n",
    "        m2_state = ret[10]\n",
    "        m3_state = ret[11]\n",
    "        m4_state = ret[12]\n",
    "        \n",
    "    read_addresses.append(curr_read[0,:])\n",
    "    write_addresses.append(curr_write[0,:])\n",
    "    \n",
    "    if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses2.append(curr_read2[0,:])\n",
    "        write_addresses2.append(curr_write2[0,:])\n",
    "        m2_state = tf.reshape(m2_state, [-1,mas[1],mcs[1]])\n",
    "        m2.append(tf.nn.softmax(m2_state[0,:]))\n",
    "        \n",
    "        with tf.variable_scope(\"NTM\",reuse=True):\n",
    "            W_interp = tf.get_variable(\"W_interp\", [controller_state_size,1])\n",
    "            B_interp = tf.get_variable(\"B_interp\", [1])\n",
    "            interp = tf.sigmoid(tf.matmul(h0,W_interp) + B_interp)\n",
    "            interp_matrix = tf.concat([interp,tf.ones_like(interp,dtype=tf.float32) - interp],axis=1) # shape [-1,2]\n",
    "            interps.append(interp_matrix[0,:])\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses3.append(curr_read3[0,:])\n",
    "        write_addresses3.append(curr_write3[0,:])\n",
    "        read_addresses4.append(curr_read4[0,:])\n",
    "        write_addresses4.append(curr_write4[0,:])\n",
    "        m3_state = tf.reshape(m3_state, [-1,mult_pattern_ntm_memory_address_sizes[2],mult_pattern_ntm_memory_content_sizes[2]])\n",
    "        m3.append(tf.nn.softmax(m3_state[0,:]))\n",
    "        m4_state = tf.reshape(m4_state, [-1,mult_pattern_ntm_memory_address_sizes[3],mult_pattern_ntm_memory_content_sizes[3]])\n",
    "        m4_state = m4_state[0,:]\n",
    "        m4_state = tf.concat([tf.nn.softmax(m4_state),tf.zeros([mult_pattern_ntm_memory_address_sizes[3],1])],1)\n",
    "        m4.append(m4_state)\n",
    "    ### END LOGGING ###\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# Note: prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "# Note: we use log_softmax to avoid precision issues with floats causing log(0) to create NaNs\n",
    "\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.log_softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * prediction[i]) for i in range(N + N_out)] # an array of numbers\n",
    "\n",
    "# Note: We allow the length of input sequences to vary between batches, which means\n",
    "# that the cross entropy needs to be masked to the relevant part of the output. The\n",
    "# relevant part consists of those positions that are not terminal symbols in the output\n",
    "# of _every_ input sequence in the batch. We detect such positions as follows. First,\n",
    "# we create a tensor term_detector which detects all the positions which are terminal symbols.\n",
    "# term_detector[i] is a boolean tensor which has False for those elements of the batch with\n",
    "# a terminal symbol in the output position i, and True otherwise.\n",
    "\n",
    "term_detector = [tf.not_equal(tf.argmax(targets[i],1),term_symbol) for i in range(N + N_out)]\n",
    "\n",
    "# We then convert False to 0.0 and True to 1.0, and compute the reduce_max, with the result\n",
    "# that mask is 1.0 in position i if and only if there was SOME element of the batch which\n",
    "# did NOT have a terminal symbol in position i\n",
    "\n",
    "mask = [tf.reduce_max(tf.cast(m, tf.float32)) for m in term_detector]\n",
    "ce_mask = [ce[i] * mask[i] for i in range(N + N_out)]\n",
    "cross_entropy = -tf.add_n(ce_mask)\n",
    "cross_entropy /= tf.add_n(mask)\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate,decay=0.9,momentum=0.9)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N + N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "errors_mask = [errors[i] * mask[i] for i in range(N + N_out)]\n",
    "mean_error = tf.add_n(errors_mask)\n",
    "mean_error /= tf.add_n(mask)\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1, mean error - 0.829351\n",
      "Epoch - 2, mean error - 0.577135\n",
      "\n",
      "It took 66 seconds to train.\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "###################\n",
    "# Note on sequences\n",
    "#\n",
    "# Our sequences are of varying length, in the alphabet {0,...,num_classes - 3}.\n",
    "# Each input sequence begins with an initial symbol and ends with a terminal symbol\n",
    "# (the value of which are num_classes - 2 and num_classes - 1 by default).\n",
    "#\n",
    "# Both input and output sequences are written on a \"tape\" of length N + N_out.\n",
    "# Input sequences are aligned at the BEGINNING of the tape, and all remaining space\n",
    "# is filled with terminal symbols. Output sequences are aligned at the END OF THE \n",
    "# MATCHING INPUT, with all remaining space filled with terminal symbols.\n",
    "#\n",
    "# Example: suppose N = N_out = 10, and num_classes = 10 so that init_symbol = 8\n",
    "# and term_symbol = 9. Then a sequence of length 8 (seq_length = 10 below) is\n",
    "#\n",
    "# a = [4, 4, 5, 6, 3, 3, 6, 7]\n",
    "#\n",
    "# which written on the tape is\n",
    "#\n",
    "# [8, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
    "#\n",
    "# If we are performing the copy task, so that the output sequence is also a, then\n",
    "# the output written on the tape will be (notice the alignment)\n",
    "#\n",
    "# [9, 9, 9, 9, 9, 9, 9, 9, 9, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9]\n",
    "#\n",
    "\n",
    "def io_generator(max_symbol, input_length, total_length):\n",
    "    \"\"\"\n",
    "    Returns a one-hot encoded pair of input and output sequence, with terminal and initial symbols.\n",
    "    \n",
    "    max_symbol - generate sequences in 0,...,max_symbol\n",
    "    input_length - length of input sequences, without initial and terminal symbols\n",
    "    total_length - length of the buffer, so that the sequences are padded to this length\n",
    "    \"\"\"\n",
    "    a = generate_input_seq(max_symbol,input_length)\n",
    "    fa = func_to_learn(a)\n",
    "    a = [init_symbol] + a + [term_symbol]\n",
    "    a = a + [term_symbol for k in range(total_length-len(a))]\n",
    "    a_onehot = [one_hots[e] for e in a]\n",
    "    \n",
    "    # If the output is too long to fit in the buffer, truncate it\n",
    "    if( len(fa) + input_length + 1 > total_length ):\n",
    "        fa = fa[:total_length-input_length-1]\n",
    "        \n",
    "    fa = [term_symbol for k in range(input_length+1)] + fa + \\\n",
    "                [term_symbol for k in range(total_length-(input_length+1)-len(fa))]\n",
    "    fa_onehot = [one_hots[e] for e in fa]\n",
    "    \n",
    "    return a, fa, np.array(a_onehot), np.array(fa_onehot)\n",
    "\n",
    "error_means = []\n",
    "epoch_error_means = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences. Each\n",
    "        # batch has a fixed length of the sequences. Recall that all input seqs\n",
    "        # have an initial and terminal symbol, so if seq_length = 10 then there\n",
    "        # are eight positions for the \"content\" symbols\n",
    "        \n",
    "        # Our version of curriculum training says: spend the first half\n",
    "        # of the epochs ramping up to the full training set. Assuming that\n",
    "        # epoch > N we divide allocate each integer in [seq_length_min,N]\n",
    "        # an equal portion of the first half of the epochs.\n",
    "        if( use_curriculum == True ):\n",
    "            if( 2 * i > epoch ):\n",
    "                seq_length_max = N\n",
    "            else:\n",
    "                curriculum_band = max(1,int(epoch/(2*(N - seq_length_min))))\n",
    "                seq_length_max = min(seq_length_min + int(i/curriculum_band),N)\n",
    "        else:\n",
    "            seq_length_max = N\n",
    "            \n",
    "        seq_length = random.randint(seq_length_min,seq_length_max)\n",
    "        \n",
    "        # Hack: if we are on the final batch of the final epoch, force\n",
    "        # it to use the full sequence length, so we get a good visualisation\n",
    "        if( i + 1 == epoch and j + 1 == no_of_batches ):\n",
    "            seq_length = N\n",
    "        \n",
    "        for z in range(batch_size):\n",
    "            a, fa, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=N+N_out)\n",
    "            \n",
    "            inp.append(a_onehot)\n",
    "            out.append(fa_onehot)\n",
    "            \n",
    "            # Record the first sequence in the last batch of the last epoch\n",
    "            if( i == epoch - 1 and j == no_of_batches - 1 and z == 0):\n",
    "                final_seq = a\n",
    "                final_seq_output = fa\n",
    "        \n",
    "        # An annoying thing here is that we cannot use a list as a key in a \n",
    "        # dictionary. The workaround we found on StackOverflow here:\n",
    "        # http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "        feed_dict = {}\n",
    "        \n",
    "        for d in range(N + N_out):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N + N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "\n",
    "        ##### Do gradient descent #####\n",
    "        mean_error_val,_ = sess.run([mean_error,minimize], feed_dict)\n",
    "        ###############################\n",
    "        \n",
    "        error_means.append(mean_error_val)\n",
    "    \n",
    "    epoch_error = np.mean(error_means[-no_of_batches:])\n",
    "    epoch_error_means.append(epoch_error)\n",
    "    \n",
    "    # Print the mean error of the final batch in the epoch\n",
    "    print_str = \"Epoch - \" + str(i+1) + \", mean error - \" + str(epoch_error)\n",
    "    \n",
    "    if( use_curriculum == True ):\n",
    "        print_str = print_str + \", training at max length - \" + str(seq_length_max)\n",
    "        \n",
    "    print(print_str)\n",
    "\n",
    "# For the final batch of the final epoch, we record the memory states as well\n",
    "seq_length_for_vis = seq_length - 2\n",
    "interps_val = sess.run(interps,feed_dict)\n",
    "m2_val, m3_val, m4_val = sess.run([m2,m3,m4],feed_dict)            \n",
    "r1_val, w1_val = sess.run([read_addresses,write_addresses],feed_dict)\n",
    "r2_val, w2_val = sess.run([read_addresses2,write_addresses2],feed_dict)\n",
    "r3_val, w3_val = sess.run([read_addresses3,write_addresses3],feed_dict)\n",
    "r4_val, w4_val = sess.run([read_addresses4,write_addresses4],feed_dict)\n",
    "errors_mask_val = sess.run(errors_mask,feed_dict)\n",
    "\n",
    "mask_val = sess.run(tf.cast(mask,tf.int64),feed_dict)\n",
    "predicted_seq = [tf.argmax(prediction[i], 1) for i in range(N + N_out)]\n",
    "predicted_seq_val = sess.run(predicted_seq,feed_dict)\n",
    "final_seq_pred_0 = [a[0] for a in predicted_seq_val]\n",
    "final_seq_pred = []\n",
    "\n",
    "for i in range(len(mask_val)):\n",
    "    if( mask_val[i] == 1.0 ):\n",
    "        final_seq_pred.append(final_seq_pred_0[i])\n",
    "    else:\n",
    "        final_seq_pred.append(9)\n",
    "\n",
    "# Write out variables to disk\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took \" + str(int(time.time() - pre_train_time)) + \" seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length used for visualisations - 18\n",
      "\n",
      "Sequence used for visualisations is (Note: initial symbol is 8, terminal symbol is 9)\n",
      "[8, -2, -2, 2, 0, 0, 2, 1, 1, 8, 1, 3, 3, 6, 7, 1, 4, 4, 4, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "\n",
      "Correct output for this sequence:\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 1, 4, 1, 4, 4, 4, 1, 3, 3, 1, 4, 1, 1, 1, 3, 6, 7, 3, 1, 3, 3, 3, 7, 1, 4, 7, 3, 7, 7, 7, 4, 4, 4, 4, 7, 4, 4]\n",
      "\n",
      "Predicted output for this sequence\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "\n",
      "Correct digits (1 means correct)\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Mask for output\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "Error probabilities for final batch\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98000002, 0.89200002, 0.87599999, 0.84799999, 0.83600003, 0.824, 0.78399998, 0.75199997, 0.68400002, 0.60399997, 0.56800002, 0.55599999, 0.54000002, 0.50400001, 0.484, 0.44400001, 0.428, 0.41600001, 0.396, 0.37200001, 0.34799999, 0.34, 0.28799999, 0.23999999, 0.236, 0.236, 0.22400001, 0.21600001, 0.208, 0.208]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAAQPCAYAAABGG5mEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnRH0AAAAOiPoAQAAdOb/AeH3hPU4OaNBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a57cf10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 1 #\n",
    "###########################\n",
    "\n",
    "print(\"Sequence length used for visualisations - \" + str(seq_length_for_vis))\n",
    "print(\"\")\n",
    "print(\"Sequence used for visualisations is (Note: initial symbol is \" + str(init_symbol) + \", terminal symbol is \" + str(term_symbol) + \")\")\n",
    "print(final_seq)\n",
    "print(\"\")\n",
    "print(\"Correct output for this sequence:\")\n",
    "print(final_seq_output)\n",
    "print(\"\")\n",
    "print(\"Predicted output for this sequence\")\n",
    "print(final_seq_pred)\n",
    "print(\"\")\n",
    "print(\"Correct digits (1 means correct)\")\n",
    "print([int(a==b) for a,b in zip(final_seq_output,final_seq_pred)])\n",
    "print(\"\")\n",
    "print(\"Mask for output\")\n",
    "print(mask_val)\n",
    "print(\"\")\n",
    "print(\"Error probabilities for final batch\")\n",
    "print(errors_mask_val)\n",
    "print(\"\")\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 9, 13\n",
    "fig_num = 0\n",
    "\n",
    "# RING 1\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "plt.figure(fig_num)\n",
    "ax1 = plt.subplot(2,1,1)\n",
    "ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "ax1.imshow(np.stack(w1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax1.set_title('Write address (ring 1)')\n",
    "ax1.set_xlabel('position')\n",
    "ax1.set_ylabel('time')\n",
    "\n",
    "ax2.imshow(np.stack(r1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax2.set_title('Read address (ring 1)')\n",
    "ax2.set_xlabel('position')\n",
    "ax2.set_ylabel('time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 2 #\n",
    "###########################\n",
    "\n",
    "if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 2)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 2)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Assume that powers2_on_1 has three entries we can use as colour channels\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 2)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    max_xticks = 2\n",
    "    xloc = plt.MaxNLocator(max_xticks)\n",
    "\n",
    "    ax.imshow(np.stack(interps_val), cmap='bone', interpolation='nearest', aspect='auto')\n",
    "    ax.set_title('Interpolation')\n",
    "    ax.set_xlabel('direct vs indirect')\n",
    "    ax.set_ylabel('time')\n",
    "    ax.xaxis.set_major_locator(xloc)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# VISUALISATIONS - OTHER RINGS #\n",
    "################################\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 3)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 3)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 3)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 4)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 4)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax6 = plt.subplot(1,1,1)    \n",
    "    ax6.imshow(np.stack(m4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax6.set_title('Memory contents (ring 4)')\n",
    "    ax6.set_xlabel('position')\n",
    "    ax6.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3kAAAUKCAYAAABblriAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFPlvf3zvdp4hcnsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x141129190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########################\n",
    "# VISUALISATIONS - ERROR #\n",
    "##########################\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "\n",
    "plt.figure(fig_num)\n",
    "ax = plt.subplot(1,1,1)\n",
    "sc = pandas.Series(error_means)\n",
    "ma = sc.rolling(window=500).mean()\n",
    "ax.plot(sc.index, sc, color='lightgray')\n",
    "ax.plot(ma.index, ma, color='red')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(sc.index.min(), sc.index.max())\n",
    "ax.set_title('Error')\n",
    "ax.set_xlabel('batch')\n",
    "ax.set_ylabel('error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on sequences of length 23\n",
      "\n",
      "Batch - 1, Mean error - 0.60417\n",
      "Batch - 2, Mean error - 0.628851\n",
      "Batch - 3, Mean error - 0.62817\n",
      "Batch - 4, Mean error - 0.622638\n",
      "\n",
      "###########\n",
      "# Summary #\n",
      "###########\n",
      "\n",
      "model         - ntm\n",
      "task name     - variable pattern 4\n",
      "epochs        - 2\n",
      "num_classes   - 10\n",
      "N             - 20\n",
      "Ntest         - 25\n",
      "# weights     - 18958\n",
      "\n",
      "\n",
      "error train(test) - 0.577135 (0.620957)\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "# Restore the weights from training\n",
    "sess = tf.Session()\n",
    "saver.restore(sess,save_path)\n",
    "\n",
    "inputs_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "targets_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "# Set up test graph\n",
    "rnn_outputs_test = []\n",
    "reuse = True\n",
    "for i in range(Ntest + Ntest_out):\n",
    "    output, state = cell(inputs_test[i],state,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "\n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size])\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.log_softmax(logit) for logit in logits_test] \n",
    "term_detector = [tf.not_equal(tf.argmax(targets_test[i],1),term_symbol) for i in range(Ntest + Ntest_out)]\n",
    "mask = [tf.reduce_max(tf.cast(m, tf.float32)) for m in term_detector]\n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets_test[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest + Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "errors_test_mask = [errors_test[i] * mask[i] for i in range(Ntest + Ntest_out)]\n",
    "mean_error_test = tf.add_n(errors_test_mask)\n",
    "mean_error_test /= tf.add_n(mask)\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "\n",
    "seq_length = Ntest\n",
    "print(\"Testing on sequences of length \" + str(seq_length-2))\n",
    "print(\"\")\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    for z in range(batch_size):\n",
    "        a, fa, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=Ntest+Ntest_out)\n",
    "            \n",
    "        inp.append(a_onehot)\n",
    "        out.append(fa_onehot)        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        in_node = inputs_test[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        out_node = targets_test[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = sess.run(mean_error_test, feed_dict)\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"epochs        - \" + str(epoch))\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error train(test) - \" + str(epoch_error_means[-1]) + \" (\" + str(final_error) + \")\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
