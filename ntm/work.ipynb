{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of the Linear Logic Recurrent Neural Network (LLRNN)\n",
    "#\n",
    "# Version 11.0\n",
    "\n",
    "###################\n",
    "# HYPERPARAMETERS #\n",
    "###################\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, mult_pattern_ntm, encoded_pattern_ntm\n",
    "task                  = 'copy' # copy, repeat copy, pattern i, mult pattern i, variable pattern i\n",
    "epoch                 = 2 # number of training epochs, default to 200\n",
    "num_classes           = 10 # number of symbols, INCLUDING initial and terminal symbols, default 10\n",
    "N                     = 30 # length of input sequences for training, default to 30\n",
    "Ntest                 = 35 # length of sequences for testing, default to 35\n",
    "batch_size            = 100 # default 250\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "num_training          = 100 # default 10000\n",
    "num_test              = num_training\n",
    "term_symbol           = num_classes - 1\n",
    "init_symbol           = num_classes - 2\n",
    "div_symbol            = num_classes - 3\n",
    "learning_rate         = 1e-4 # default 1e-4\n",
    "memory_init_bias      = 1.0 # default 1.0\n",
    "use_curriculum        = False # default True\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "max_pattern_length    = 8 # default 8\n",
    "\n",
    "##################\n",
    "# MODEL SPECIFIC #\n",
    "##################\n",
    "\n",
    "ntm_memory_address_size   = 128 # number of memory locations, default 128\n",
    "ntm_memory_content_size   = 20 # size of vector stored at a memory location, default 20\n",
    "ntm_powers                = [0,-1,1] # powers of R used by controller, default [0,-1,1]\n",
    "\n",
    "pattern_ntm_powers               = [[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "pattern_ntm_powers_2_on_1        = [0,1,2] # allowed powers used by ring 2 to manipulate ring 1\n",
    "pattern_ntm_memory_address_sizes = [128, 20] # number of memory locations for the three rings\n",
    "pattern_ntm_memory_content_sizes = [20, 3] # size of content vector for each ring\n",
    "pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "mult_pattern_ntm_powers               = [[0,-1,1],[0,-1,1],[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "mult_pattern_ntm_powers_2_on_1        = [0,1,2] # allowed powers used by rings 2,3 to manipulate ring 1\n",
    "mult_pattern_ntm_memory_address_sizes = [128, 20, 20, 10] # number of memory locations for the rings\n",
    "mult_pattern_ntm_memory_content_sizes = [20, 3, 3, 2] # size of content vector for each ring\n",
    "mult_pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "encoded_pattern_ntm_powers               = [[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "encoded_pattern_ntm_powers_2_on_1        = [0,1,2] # allowed powers used by ring 2 to manipulate ring 1\n",
    "encoded_pattern_ntm_memory_address_sizes = [128, 20] # number of memory locations for the three rings\n",
    "encoded_pattern_ntm_memory_content_sizes = [20, 3] # size of content vector for each ring\n",
    "encoded_pattern_ntm_direct_bias          = 1.0\n",
    "encoded_pattern_ntm_encoder_state_size   = 100\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there\n",
    "# 2. The initial and terminal symbols are always from the end of the list of symbols, so they\n",
    "# are respectively num_classes - 2 and num_classes - 1. So the number of symbols which are\n",
    "# not initial or terminal is num_classes - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import pandas\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs\n",
    "\n",
    "assert use_model == 'ntm' or use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm' or use_model == 'encoded_pattern_ntm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[4, 7, 2, 3, 5, 1, 7, 2, 4, 3, 5, 3, 0, 7, 2, 1, 5, 1, 6, 4, 5, 4, 3, 0, 6, 3, 7, 7]\n",
      "is mapped to\n",
      "[4, 7, 2, 3, 5, 1, 7, 2, 4, 3, 5, 3, 0, 7, 2, 1, 5, 1, 6, 4, 5, 4, 3, 0, 6, 3, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "# Below N_out and Ntest_out are the lengths of the outputs in both the training\n",
    "# and testing regimes respectively. Since outputs do not include the initial and terminal\n",
    "# symbols, these default to N - 2 and Ntest - 2 respectively.\n",
    "\n",
    "func_to_learn, N_out, Ntest_out, seq_length_min, generate_input_seq = learnfuncs.get_task(task, N, Ntest)\n",
    "\n",
    "# Make sure the given N is above the minimum sequence length\n",
    "assert N >= seq_length_min\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = generate_input_seq(num_classes-3,N-2)\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# INITIALISE STATE #\n",
    "####################\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "input_size = num_classes # dimension of the input space I\n",
    "\n",
    "#####\n",
    "# NTM\n",
    "\n",
    "def init_state_ntm(batch_size, css, mas, mcs):\n",
    "    state_size = css + 2*mas + mas * mcs\n",
    "    \n",
    "    ra = [0.0]*mas\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,mas]) + ra\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    init_read_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_write_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_memory = tf.truncated_normal([batch_size, mas*mcs], 0.0, 1e-6, dtype=tf.float32)\n",
    "    \n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "#############\n",
    "# PATTERN NTM\n",
    "\n",
    "def init_state_pattern_ntm(batch_size, css, mas, mcs):\n",
    "    # mas and mcs are arrays of address sizes and content sizes for rings\n",
    "    state_size = css\n",
    "    \n",
    "    init_address = []\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        state_size = state_size + mas[i] * mcs[i] # for memory vector\n",
    "        state_size = state_size + 2 * mas[i] # for addresses (read and write)\n",
    "    \n",
    "        ra = [0.0]*mas[i]\n",
    "        ra[0] = 1.0\n",
    "        init_address.append(np.zeros([batch_size,mas[i]]) + ra)\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    \n",
    "    tensor_list = [init_controller_state]\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        init_read_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        init_write_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        tensor_list = tensor_list + [init_read_address,init_write_address]\n",
    "        \n",
    "    for i in range(len(mas)):\n",
    "        # The first ring is initialised to zero, the rest differently\n",
    "        if( i == 0 ):\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "        else:\n",
    "            # This initialisation has the result of biasing the output of rings 2 and 3 to be\n",
    "            # \"no rotation\" and biasing ring 4 to say \"use ring 2\"\n",
    "            ra = [0.0]*mcs[i] \n",
    "            ra[0] = memory_init_bias\n",
    "            ra = np.zeros([batch_size,mas[i],mcs[i]]) + ra\n",
    "            ra = tf.constant(ra,dtype=tf.float32,shape=[batch_size,mas[i],mcs[i]])\n",
    "            ra = tf.reshape(ra,[batch_size,mas[i]*mcs[i]])\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32) + ra\n",
    "            #init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "            \n",
    "        tensor_list = tensor_list + [init_memory]\n",
    "    \n",
    "    state = tf.concat(tensor_list,1)\n",
    "\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "######################\n",
    "# MULTIPLE PATTERN NTM\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "    \n",
    "######################\n",
    "# ENCODED PATTERN NTM\n",
    "\n",
    "if( use_model == 'encoded_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, encoded_pattern_ntm_memory_address_sizes, \n",
    "                                                encoded_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.EncodedPatternNTM(state_size, input_size, controller_state_size, encoded_pattern_ntm_memory_address_sizes,\n",
    "                          encoded_pattern_ntm_memory_content_sizes, encoded_pattern_ntm_powers, encoded_pattern_ntm_powers_2_on_1, \n",
    "                              encoded_pattern_ntm_direct_bias)\n",
    "    \n",
    "    ecss = encoded_pattern_ntm_encoder_state_size\n",
    "    encoder_state = tf.truncated_normal([batch_size, ecss], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    \n",
    "    encoder_cell = ntm.StandardRNN(ecss, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_57/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_56/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_55/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_54/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_53/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_52/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_51/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_50/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_49/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_48/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_47/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_46/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_45/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_44/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_43/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_42/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_41/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_40/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_39/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_38/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_37/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_36/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_35/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_34/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_33/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_32/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_31/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_30/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_29/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_28/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_27/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_26/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_25/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_24/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_23/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_22/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_21/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_20/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_19/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_18/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_17/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_15/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_13/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_11/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_9/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_7/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_5/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_3/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_1/split_grad/concat:0' shape=(100, 2916) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "\n",
    "if( use_model == 'encoded_pattern_ntm' ):\n",
    "    pattern_inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(max_pattern_length)]\n",
    "    \n",
    "    reuse = False\n",
    "    for i in range(max_pattern_length):\n",
    "        #### RUN ENCODER RNN ####\n",
    "        encoder_output, encoder_state = encoder_cell(pattern_inputs[i],encoder_state,'ENC',reuse)\n",
    "        ###################\n",
    "\n",
    "        reuse = True\n",
    "    \n",
    "    mem_size = encoded_pattern_ntm_memory_address_sizes[1] * encoded_pattern_ntm_memory_content_sizes[1]\n",
    "        \n",
    "    with tf.variable_scope(\"decoder_layer\"):\n",
    "        E_dec = tf.get_variable(\"E_dec\",[ecss,mem_size])\n",
    "        F_dec = tf.get_variable(\"F_dec\",[mem_size],initializer=init_ops.constant_initializer(0.0))\n",
    "    \n",
    "    decoded_mem = tf.matmul(encoder_output, E_dec) + F_dec\n",
    "    \n",
    "    # Now set the state of M2 in the proper Encoded Pattern NTM to this decoded_mem\n",
    "    mas = pattern_ntm_memory_address_sizes\n",
    "    mcs = pattern_ntm_memory_content_sizes\n",
    "        \n",
    "    ret = tf.split(state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],mas[0] * mcs[0],mas[1] * mcs[1]], 1)\n",
    "    state = tf.concat([ret[0],ret[1],ret[2],ret[3],ret[4],ret[5],decoded_mem],1)\n",
    "        \n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "read_addresses2 = []\n",
    "read_addresses3 = []\n",
    "read_addresses4 = []\n",
    "write_addresses = []\n",
    "write_addresses2 = []\n",
    "write_addresses3 = []\n",
    "write_addresses4 = []\n",
    "interps = []\n",
    "rnn_outputs = []\n",
    "m2 = []\n",
    "m3 = []\n",
    "m4 = []\n",
    "    \n",
    "for i in range(N + N_out):\n",
    "    \n",
    "    old_state = state\n",
    "\n",
    "    #### RUN MODEL ####\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "    ###################\n",
    "\n",
    "    reuse = True\n",
    "    \n",
    "    #### SET UP NODES FOR LOGGING #####\n",
    "    if( use_model == 'ntm' ):\n",
    "        h0, curr_read, curr_write, _ = tf.split(old_state, [controller_state_size,ntm_memory_address_size,\n",
    "                                                        ntm_memory_address_size,-1], 1)\n",
    "\n",
    "    if( use_model == 'pattern_ntm' or use_model == 'encoded_pattern_ntm'):\n",
    "        mas = pattern_ntm_memory_address_sizes\n",
    "        mcs = pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(old_state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],mas[0] * mcs[0],mas[1] * mcs[1]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        m1_state = ret[5]\n",
    "        m2_state = ret[6]\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm' ):\n",
    "        mas = mult_pattern_ntm_memory_address_sizes\n",
    "        mcs = mult_pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(old_state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],                        \n",
    "                            mas[2],mas[2],mas[3],mas[3],mas[0] * mcs[0],mas[1] * mcs[1],\n",
    "                            mas[2] * mcs[2],mas[3] * mcs[3]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        curr_read3 = ret[5]\n",
    "        curr_write3 = ret[6]\n",
    "        curr_read4 = ret[7]\n",
    "        curr_write4 = ret[8]\n",
    "        m1_state = ret[9]\n",
    "        m2_state = ret[10]\n",
    "        m3_state = ret[11]\n",
    "        m4_state = ret[12]\n",
    "        \n",
    "    read_addresses.append(curr_read[0,:])\n",
    "    write_addresses.append(curr_write[0,:])\n",
    "    \n",
    "    if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm' or use_model == 'encoded_pattern_ntm'):\n",
    "        read_addresses2.append(curr_read2[0,:])\n",
    "        write_addresses2.append(curr_write2[0,:])\n",
    "        m2_state = tf.reshape(m2_state, [-1,mas[1],mcs[1]])\n",
    "        m2.append(tf.nn.softmax(m2_state[0,:]))\n",
    "        \n",
    "        with tf.variable_scope(\"NTM\",reuse=True):\n",
    "            W_interp = tf.get_variable(\"W_interp\", [controller_state_size,1])\n",
    "            B_interp = tf.get_variable(\"B_interp\", [1])\n",
    "            interp = tf.sigmoid(tf.matmul(h0,W_interp) + B_interp)\n",
    "            interp_matrix = tf.concat([interp,tf.ones_like(interp,dtype=tf.float32) - interp],axis=1) # shape [-1,2]\n",
    "            interps.append(interp_matrix[0,:])\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses3.append(curr_read3[0,:])\n",
    "        write_addresses3.append(curr_write3[0,:])\n",
    "        read_addresses4.append(curr_read4[0,:])\n",
    "        write_addresses4.append(curr_write4[0,:])\n",
    "        m3_state = tf.reshape(m3_state, [-1,mult_pattern_ntm_memory_address_sizes[2],mult_pattern_ntm_memory_content_sizes[2]])\n",
    "        m3.append(tf.nn.softmax(m3_state[0,:]))\n",
    "        m4_state = tf.reshape(m4_state, [-1,mult_pattern_ntm_memory_address_sizes[3],mult_pattern_ntm_memory_content_sizes[3]])\n",
    "        m4_state = m4_state[0,:]\n",
    "        m4_state = tf.concat([tf.nn.softmax(m4_state),tf.zeros([mult_pattern_ntm_memory_address_sizes[3],1])],1)\n",
    "        m4.append(m4_state)\n",
    "    ### END LOGGING ###\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# Note: prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "# Note: we use log_softmax to avoid precision issues with floats causing log(0) to create NaNs\n",
    "\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.log_softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * prediction[i]) for i in range(N + N_out)] # an array of numbers\n",
    "\n",
    "# Note: We allow the length of input sequences to vary between batches, which means\n",
    "# that the cross entropy needs to be masked to the relevant part of the output. The\n",
    "# relevant part consists of those positions that are not terminal symbols in the output\n",
    "# of _every_ input sequence in the batch. We detect such positions as follows. First,\n",
    "# we create a tensor term_detector which detects all the positions which are terminal symbols.\n",
    "# term_detector[i] is a boolean tensor which has False for those elements of the batch with\n",
    "# a terminal symbol in the output position i, and True otherwise.\n",
    "\n",
    "term_detector = [tf.not_equal(tf.argmax(targets[i],1),term_symbol) for i in range(N + N_out)]\n",
    "\n",
    "# We then convert False to 0.0 and True to 1.0, and compute the reduce_max, with the result\n",
    "# that mask is 1.0 in position i if and only if there was SOME element of the batch which\n",
    "# did NOT have a terminal symbol in position i\n",
    "\n",
    "mask = [tf.reduce_max(tf.cast(m, tf.float32)) for m in term_detector]\n",
    "ce_mask = [ce[i] * mask[i] for i in range(N + N_out)]\n",
    "cross_entropy = -tf.add_n(ce_mask)\n",
    "cross_entropy /= tf.add_n(mask)\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate,decay=0.9,momentum=0.9)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N + N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "errors_mask = [errors[i] * mask[i] for i in range(N + N_out)]\n",
    "mean_error = tf.add_n(errors_mask)\n",
    "mean_error /= tf.add_n(mask)\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1, mean error - 0.889\n",
      "Epoch - 2, mean error - 0.893571\n",
      "\n",
      "It took 68 seconds to train.\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "assert no_of_batches > 0\n",
    "\n",
    "###################\n",
    "# Note on sequences\n",
    "#\n",
    "# Our sequences are of varying length, in the alphabet {0,...,num_classes - 3}.\n",
    "# Each input sequence begins with an initial symbol and ends with a terminal symbol\n",
    "# (the value of which are num_classes - 2 and num_classes - 1 by default).\n",
    "#\n",
    "# Both input and output sequences are written on a \"tape\" of length N + N_out.\n",
    "# Input sequences are aligned at the BEGINNING of the tape, and all remaining space\n",
    "# is filled with terminal symbols. Output sequences are aligned at the END OF THE \n",
    "# MATCHING INPUT, with all remaining space filled with terminal symbols.\n",
    "#\n",
    "# Example: suppose N = N_out = 10, and num_classes = 10 so that init_symbol = 8\n",
    "# and term_symbol = 9. Then a sequence of length 8 (seq_length = 10 below) is\n",
    "#\n",
    "# a = [4, 4, 5, 6, 3, 3, 6, 7]\n",
    "#\n",
    "# which written on the tape is\n",
    "#\n",
    "# [8, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
    "#\n",
    "# If we are performing the copy task, so that the output sequence is also a, then\n",
    "# the output written on the tape will be (notice the alignment)\n",
    "#\n",
    "# [9, 9, 9, 9, 9, 9, 9, 9, 9, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9]\n",
    "#\n",
    "\n",
    "def io_generator(max_symbol, input_length, total_length):\n",
    "    \"\"\"\n",
    "    Returns a one-hot encoded pair of input and output sequence, with terminal and initial symbols.\n",
    "    \n",
    "    max_symbol - generate sequences in 0,...,max_symbol\n",
    "    input_length - length of input sequences, without initial and terminal symbols\n",
    "    total_length - length of the buffer, so that the sequences are padded to this length\n",
    "    \"\"\"\n",
    "    a = generate_input_seq(max_symbol,input_length)\n",
    "    fa = func_to_learn(a)\n",
    "    a = [init_symbol] + a + [term_symbol]\n",
    "    a = a + [term_symbol for k in range(total_length-len(a))]\n",
    "    a_onehot = [one_hots[e] for e in a]\n",
    "    \n",
    "    # If the output is too long to fit in the buffer, truncate it\n",
    "    if( len(fa) + input_length + 1 > total_length ):\n",
    "        fa = fa[:total_length-input_length-1]\n",
    "        \n",
    "    fa = [term_symbol for k in range(input_length+1)] + fa + \\\n",
    "                [term_symbol for k in range(total_length-(input_length+1)-len(fa))]\n",
    "    fa_onehot = [one_hots[e] for e in fa]\n",
    "    \n",
    "    return a, fa, np.array(a_onehot), np.array(fa_onehot)\n",
    "\n",
    "error_means = []\n",
    "epoch_error_means = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "        inp_unenc = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences. Each\n",
    "        # batch has a fixed length of the sequences. Recall that all input seqs\n",
    "        # have an initial and terminal symbol, so if seq_length = 10 then there\n",
    "        # are eight positions for the \"content\" symbols\n",
    "        \n",
    "        # Our version of curriculum training says: spend the first half\n",
    "        # of the epochs ramping up to the full training set. Assuming that\n",
    "        # epoch > N we divide allocate each integer in [seq_length_min,N]\n",
    "        # an equal portion of the first half of the epochs.\n",
    "        if( use_curriculum == True ):\n",
    "            if( 2 * i > epoch ):\n",
    "                seq_length_max = N\n",
    "            else:\n",
    "                curriculum_band = max(1,int(epoch/(2*(N - seq_length_min))))\n",
    "                seq_length_max = min(seq_length_min + int(i/curriculum_band),N)\n",
    "        else:\n",
    "            seq_length_max = N\n",
    "            \n",
    "        seq_length = random.randint(seq_length_min,seq_length_max)\n",
    "        \n",
    "        # Hack: if we are on the final batch of the final epoch, force\n",
    "        # it to use the full sequence length, so we get a good visualisation\n",
    "        if( i + 1 == epoch and j + 1 == no_of_batches ):\n",
    "            seq_length = N\n",
    "        \n",
    "        for z in range(batch_size):\n",
    "            a, fa, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=N+N_out)\n",
    "            \n",
    "            inp.append(a_onehot)\n",
    "            out.append(fa_onehot)\n",
    "            \n",
    "            inp_unenc.append(a)\n",
    "            \n",
    "            # Record the first sequence in the last batch of the last epoch\n",
    "            if( i == epoch - 1 and j == no_of_batches - 1 and z == 0):\n",
    "                final_seq = a\n",
    "                final_seq_output = fa\n",
    "        \n",
    "        # An annoying thing here is that we cannot use a list as a key in a \n",
    "        # dictionary. The workaround we found on StackOverflow here:\n",
    "        # http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "        feed_dict = {}\n",
    "        \n",
    "        # In the encoded pattern NTM we feed the initial part of the inputs, i.e.\n",
    "        # the pattern before the second initial symbol, also to the encoder RNN\n",
    "        # So we extract this part from the batch of input strings, and pad it out to\n",
    "        # the standard length max_pattern_length with initial symbols\n",
    "        if( use_model == 'encoded_pattern_ntm' ):\n",
    "            patt = []\n",
    "            \n",
    "            for z in range(batch_size):\n",
    "                a = inp_unenc[z]\n",
    "                \n",
    "                # Find the second init symbol\n",
    "                init_loc = 1\n",
    "                while( a[init_loc] != init_symbol ):\n",
    "                    init_loc = init_loc + 1\n",
    "                    \n",
    "                # Chop the sequence off at the location of the second init symbol\n",
    "                a = a[1:init_loc]\n",
    "                \n",
    "                # Now pad it out to length max_pattern_length\n",
    "                while( len(a) != max_pattern_length ):\n",
    "                    a.append(init_symbol)\n",
    "                \n",
    "                a_onehot = [one_hots[e] for e in a]\n",
    "                patt.append(np.array(a_onehot))\n",
    "            \n",
    "            for d in range(max_pattern_length):\n",
    "                pat_node = pattern_inputs[d]\n",
    "                ti = []\n",
    "                for k in range(batch_size):\n",
    "                    ti.append(patt[k][d])\n",
    "                feed_dict[pat_node] = np.array(ti)\n",
    "            \n",
    "        for d in range(N + N_out):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N + N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "\n",
    "        ##### Do gradient descent #####\n",
    "        mean_error_val,_ = sess.run([mean_error,minimize], feed_dict)\n",
    "        ###############################\n",
    "        \n",
    "        error_means.append(mean_error_val)\n",
    "    \n",
    "    epoch_error = np.mean(error_means[-no_of_batches:])\n",
    "    epoch_error_means.append(epoch_error)\n",
    "    \n",
    "    # Print the mean error of the final batch in the epoch\n",
    "    print_str = \"Epoch - \" + str(i+1) + \", mean error - \" + str(epoch_error)\n",
    "    \n",
    "    if( use_curriculum == True ):\n",
    "        print_str = print_str + \", training at max length - \" + str(seq_length_max)\n",
    "        \n",
    "    print(print_str)\n",
    "\n",
    "# For the final batch of the final epoch, we record the memory states as well\n",
    "seq_length_for_vis = seq_length - 2\n",
    "interps_val = sess.run(interps,feed_dict)\n",
    "m2_val, m3_val, m4_val = sess.run([m2,m3,m4],feed_dict)            \n",
    "r1_val, w1_val = sess.run([read_addresses,write_addresses],feed_dict)\n",
    "r2_val, w2_val = sess.run([read_addresses2,write_addresses2],feed_dict)\n",
    "r3_val, w3_val = sess.run([read_addresses3,write_addresses3],feed_dict)\n",
    "r4_val, w4_val = sess.run([read_addresses4,write_addresses4],feed_dict)\n",
    "errors_mask_val = sess.run(errors_mask,feed_dict)\n",
    "\n",
    "mask_val = sess.run(tf.cast(mask,tf.int64),feed_dict)\n",
    "predicted_seq = [tf.argmax(prediction[i], 1) for i in range(N + N_out)]\n",
    "predicted_seq_val = sess.run(predicted_seq,feed_dict)\n",
    "final_seq_pred_0 = [a[0] for a in predicted_seq_val]\n",
    "final_seq_pred = []\n",
    "\n",
    "for i in range(len(mask_val)):\n",
    "    if( mask_val[i] == 1.0 ):\n",
    "        final_seq_pred.append(final_seq_pred_0[i])\n",
    "    else:\n",
    "        final_seq_pred.append(9)\n",
    "\n",
    "# Write out variables to disk\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took \" + str(int(time.time() - pre_train_time)) + \" seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length used for visualisations - 28\n",
      "\n",
      "Sequence used for visualisations is (Note: initial symbol is 8, terminal symbol is 9)\n",
      "[8, 1, 1, 5, 0, 1, 6, 4, 2, 6, 7, 3, 4, 2, 1, 2, 7, 7, 3, 6, 2, 3, 3, 2, 1, 3, 6, 1, 0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "\n",
      "Correct output for this sequence:\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 1, 1, 5, 0, 1, 6, 4, 2, 6, 7, 3, 4, 2, 1, 2, 7, 7, 3, 6, 2, 3, 3, 2, 1, 3, 6, 1, 0, 9]\n",
      "\n",
      "Predicted output for this sequence\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 9]\n",
      "\n",
      "Correct digits (1 means correct)\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      "Mask for output\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
      "\n",
      "Error probabilities for final batch\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88999999, 0.88999999, 0.89999998, 0.89999998, 0.91000003, 0.88, 0.88999999, 0.87, 0.82999998, 0.94999999, 0.85000002, 0.89999998, 0.88999999, 0.97000003, 0.93000001, 0.88, 0.92000002, 0.87, 0.94, 0.88, 0.83999997, 0.83999997, 0.88999999, 0.89999998, 0.88, 0.93000001, 0.88, 0.91000003, 0.0]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAL/CAYAAAD2uXY3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\np+a9klnsC/0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16097c690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 1 #\n",
    "###########################\n",
    "\n",
    "print(\"Sequence length used for visualisations - \" + str(seq_length_for_vis))\n",
    "print(\"\")\n",
    "print(\"Sequence used for visualisations is (Note: initial symbol is \" + str(init_symbol) + \", terminal symbol is \" + str(term_symbol) + \")\")\n",
    "print(final_seq)\n",
    "print(\"\")\n",
    "print(\"Correct output for this sequence:\")\n",
    "print(final_seq_output)\n",
    "print(\"\")\n",
    "print(\"Predicted output for this sequence\")\n",
    "print(final_seq_pred)\n",
    "print(\"\")\n",
    "print(\"Correct digits (1 means correct)\")\n",
    "print([int(a==b) for a,b in zip(final_seq_output,final_seq_pred)])\n",
    "print(\"\")\n",
    "print(\"Mask for output\")\n",
    "print(mask_val)\n",
    "print(\"\")\n",
    "print(\"Error probabilities for final batch\")\n",
    "print(errors_mask_val)\n",
    "print(\"\")\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 9, 13\n",
    "fig_num = 0\n",
    "\n",
    "# RING 1\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "plt.figure(fig_num)\n",
    "ax1 = plt.subplot(2,1,1)\n",
    "ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "ax1.imshow(np.stack(w1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax1.set_title('Write address (ring 1)')\n",
    "ax1.set_xlabel('position')\n",
    "ax1.set_ylabel('time')\n",
    "\n",
    "ax2.imshow(np.stack(r1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax2.set_title('Read address (ring 1)')\n",
    "ax2.set_xlabel('position')\n",
    "ax2.set_ylabel('time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 2 #\n",
    "###########################\n",
    "\n",
    "if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm' or use_model == 'encoded_pattern_ntm'):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 2)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 2)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Assume that powers2_on_1 has three entries we can use as colour channels\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 2)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    max_xticks = 2\n",
    "    xloc = plt.MaxNLocator(max_xticks)\n",
    "\n",
    "    ax.imshow(np.stack(interps_val), cmap='bone', interpolation='nearest', aspect='auto')\n",
    "    ax.set_title('Interpolation')\n",
    "    ax.set_xlabel('direct vs indirect')\n",
    "    ax.set_ylabel('time')\n",
    "    ax.xaxis.set_major_locator(xloc)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# VISUALISATIONS - OTHER RINGS #\n",
    "################################\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 3)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 3)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 3)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 4)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 4)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax6 = plt.subplot(1,1,1)    \n",
    "    ax6.imshow(np.stack(m4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax6.set_title('Memory contents (ring 4)')\n",
    "    ax6.set_xlabel('position')\n",
    "    ax6.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAOgCAYAAACtFsehAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEAAAIGL+H/ofVhNvijM1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17fecbb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########################\n",
    "# VISUALISATIONS - ERROR #\n",
    "##########################\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "\n",
    "plt.figure(fig_num)\n",
    "ax = plt.subplot(1,1,1)\n",
    "sc = pandas.Series(error_means)\n",
    "ma = sc.rolling(window=500).mean()\n",
    "ax.plot(sc.index, sc, color='lightgray')\n",
    "ax.plot(ma.index, ma, color='red')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(sc.index.min(), sc.index.max())\n",
    "ax.set_title('Error')\n",
    "ax.set_xlabel('batch')\n",
    "ax.set_ylabel('error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on sequences of length 33\n",
      "\n",
      "Batch - 1, Mean error - 0.883333\n",
      "\n",
      "###########\n",
      "# Summary #\n",
      "###########\n",
      "\n",
      "model         - ntm\n",
      "task name     - copy\n",
      "epochs        - 2\n",
      "num_classes   - 10\n",
      "N             - 30\n",
      "Ntest         - 35\n",
      "# weights     - 18958\n",
      "\n",
      "\n",
      "error train(test) - 0.893571 (0.883333)\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "# Restore the weights from training\n",
    "sess = tf.Session()\n",
    "saver.restore(sess,save_path)\n",
    "\n",
    "inputs_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "targets_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "\n",
    "if( use_model == 'encoded_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, encoded_pattern_ntm_memory_address_sizes, \n",
    "                                                encoded_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.EncodedPatternNTM(state_size, input_size, controller_state_size, encoded_pattern_ntm_memory_address_sizes,\n",
    "                          encoded_pattern_ntm_memory_content_sizes, encoded_pattern_ntm_powers, encoded_pattern_ntm_powers_2_on_1, \n",
    "                              encoded_pattern_ntm_direct_bias)\n",
    "    \n",
    "    ecss = encoded_pattern_ntm_encoder_state_size\n",
    "    encoder_state = tf.truncated_normal([batch_size, ecss], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    \n",
    "    encoder_cell = ntm.StandardRNN(ecss, input_size)\n",
    "\n",
    "    \n",
    "if( use_model == 'encoded_pattern_ntm' ):\n",
    "    pattern_inputs_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(max_pattern_length)]\n",
    "    \n",
    "    reuse = True\n",
    "    for i in range(max_pattern_length):\n",
    "        #### RUN ENCODER RNN ####\n",
    "        encoder_output, encoder_state = encoder_cell(pattern_inputs_test[i],encoder_state,'ENC',reuse)\n",
    "        ###################\n",
    "\n",
    "    mem_size = encoded_pattern_ntm_memory_address_sizes[1] * encoded_pattern_ntm_memory_content_sizes[1]\n",
    "        \n",
    "    with tf.variable_scope(\"decoder_layer\",reuse=True):\n",
    "        E_dec = tf.get_variable(\"E_dec\",[ecss,mem_size])\n",
    "        F_dec = tf.get_variable(\"F_dec\",[mem_size])\n",
    "    \n",
    "    decoded_mem = tf.matmul(encoder_output, E_dec) + F_dec\n",
    "    \n",
    "    # Now set the state of M2 in the proper Encoded Pattern NTM to this decoded_mem\n",
    "    mas = pattern_ntm_memory_address_sizes\n",
    "    mcs = pattern_ntm_memory_content_sizes\n",
    "        \n",
    "    ret = tf.split(state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],mas[0] * mcs[0],mas[1] * mcs[1]], 1)\n",
    "    state = tf.concat([ret[0],ret[1],ret[2],ret[3],ret[4],ret[5],decoded_mem],1)\n",
    "    \n",
    "# Set up test graph\n",
    "rnn_outputs_test = []\n",
    "reuse = True\n",
    "for i in range(Ntest + Ntest_out):\n",
    "    output, state = cell(inputs_test[i],state,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "\n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size])\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.log_softmax(logit) for logit in logits_test] \n",
    "term_detector = [tf.not_equal(tf.argmax(targets_test[i],1),term_symbol) for i in range(Ntest + Ntest_out)]\n",
    "mask = [tf.reduce_max(tf.cast(m, tf.float32)) for m in term_detector]\n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets_test[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest + Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "errors_test_mask = [errors_test[i] * mask[i] for i in range(Ntest + Ntest_out)]\n",
    "mean_error_test = tf.add_n(errors_test_mask)\n",
    "mean_error_test /= tf.add_n(mask)\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "\n",
    "seq_length = Ntest\n",
    "print(\"Testing on sequences of length \" + str(seq_length-2))\n",
    "print(\"\")\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "    inp_unenc = []\n",
    "\n",
    "    for z in range(batch_size):\n",
    "        a, fa, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=Ntest+Ntest_out)\n",
    "            \n",
    "        inp.append(a_onehot)\n",
    "        out.append(fa_onehot)  \n",
    "        inp_unenc.append(a)\n",
    "        \n",
    "    feed_dict = {}\n",
    "    \n",
    "    if( use_model == 'encoded_pattern_ntm' ):\n",
    "        patt = []\n",
    "            \n",
    "        for z in range(batch_size):\n",
    "            a = inp_unenc[z]\n",
    "                \n",
    "            # Find the second init symbol\n",
    "            init_loc = 1\n",
    "            while( a[init_loc] != init_symbol ):\n",
    "                init_loc = init_loc + 1\n",
    "                    \n",
    "            # Chop the sequence off at the location of the second init symbol\n",
    "            a = a[1:init_loc]\n",
    "                \n",
    "            # Now pad it out to length max_pattern_length\n",
    "            while( len(a) != max_pattern_length ):\n",
    "                a.append(init_symbol)\n",
    "                \n",
    "            a_onehot = [one_hots[e] for e in a]\n",
    "            patt.append(np.array(a_onehot))\n",
    "            \n",
    "        for d in range(max_pattern_length):\n",
    "            pat_node = pattern_inputs_test[d]\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(patt[k][d])\n",
    "            feed_dict[pat_node] = np.array(ti)\n",
    "                \n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        in_node = inputs_test[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        out_node = targets_test[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = sess.run(mean_error_test, feed_dict)\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"epochs        - \" + str(epoch))\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error train(test) - \" + str(epoch_error_means[-1]) + \" (\" + str(final_error) + \")\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
