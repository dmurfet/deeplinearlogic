{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of linear logic recurrent neural network\n",
    "#\n",
    "# The architecture is a modified RNN, see the paper \"Linear logic and recurrent neural networks\".\n",
    "# Our inputs are sequences of symbols taken from an alphabet of size num_classes. The length\n",
    "# of the sequences is N. Our outputs are also sequences of length N from the same alphabet.\n",
    "#\n",
    "# Here \"symbol\" means a one hot vector.\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# GLOBAL FLAGS\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, pattern_ntm_alt\n",
    "task                  = 'copy' # copy, repeat copy, pattern\n",
    "epoch                 = 200 # number of training epochs, default to 200\n",
    "num_classes           = 2 # number of symbols in the alphabet, default to 2\n",
    "N                     = 20 # length of input sequences for training, default to 20\n",
    "Ntest                 = 20 # length of sequences for testing, default to N\n",
    "batch_size            = 500 # default to 500 (too large does not fit on GPUs)\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "memory_address_size   = 20 # number of memory locations, default 20\n",
    "memory_content_size   = 5 # size of vector stored at a memory location, default 5\n",
    "powers_ring1          = [0,-1,1] # powers of R used on ring 1, default [0,-1,1]\n",
    "powers_ring2          = [0,-1,1] # powers of R used on ring 2, default [0,-1,1]\n",
    "model_optimizer       = 'rmsprop' # adam, rmsprop, default to rmsprop\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "\n",
    "training_percent      = 0.01 # percentage used for training, default 0.01\n",
    "num_training          = int(training_percent * num_classes**N)\n",
    "num_test              = 2 * num_training\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0]\n",
      "is mapped to\n",
      "[1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N\n",
    "    Ntest_out = Ntest\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "if( task == 'repeat copy' ):\n",
    "    pattern = [0,1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 2 * N\n",
    "    Ntest_out = 2 * Ntest\n",
    "\n",
    "##############\n",
    "# PATTERN TASK\n",
    "if( task == 'pattern' ):\n",
    "    pattern = [1,0,0,2,0]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 2 * N\n",
    "    Ntest_out = 2 * Ntest\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = [random.randint(0,num_classes-1) for i in range(N)]\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_39/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_38/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_37/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_36/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_35/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_34/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_33/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_32/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_31/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_30/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_29/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_28/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_27/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_26/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_25/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_24/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_23/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_22/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_21/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_20/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_19/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_18/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_17/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_15/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_13/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_11/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_9/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_7/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_5/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_3/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_1/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "input_size = num_classes # dimension of the input space I\n",
    "\n",
    "# inputs, we create N of them, each of shape [None,input_size], one for each position in the sequence\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N_out)]\n",
    "\n",
    "# state_size is the number of hidden neurons in each layer\n",
    "state_size = 0\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size = controller_state_size + 2*memory_address_size + memory_address_size * memory_content_size\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,memory_address_size,memory_content_size, powers_ring1)\n",
    "elif( use_model == 'pattern_ntm' ):\n",
    "    state_size = controller_state_size + 4*memory_address_size + \\\n",
    "                memory_address_size * memory_content_size + \\\n",
    "                memory_address_size * len(powers_ring1)\n",
    "\n",
    "    cell = ntm.PatternNTM(state_size,input_size,controller_state_size,\n",
    "                          memory_address_size,memory_content_size, powers_ring1, powers_ring2)\n",
    "elif( use_model == 'pattern_ntm_alt' ):\n",
    "    state_size = controller_state_size + 4*memory_address_size + \\\n",
    "                memory_address_size * memory_content_size + \\\n",
    "                memory_address_size * len(powers_ring1)\n",
    "\n",
    "    cell = ntm.PatternNTM_alt(state_size,input_size,controller_state_size,\n",
    "                          memory_address_size,memory_content_size, powers_ring1, powers_ring2)\n",
    "\n",
    "# Initialise the state\n",
    "if( use_model == 'ntm' ):\n",
    "    ra = [0.0]*memory_address_size\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,memory_address_size]) + ra\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, controller_state_size], 0.0, 1e-6, dtype=tf.float32)\n",
    "    init_read_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,memory_address_size])\n",
    "    init_write_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,memory_address_size])\n",
    "    init_memory = tf.constant(1e-6,dtype=tf.float32,shape=[batch_size,memory_address_size*memory_content_size])\n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "else:\n",
    "    state = tf.truncated_normal([batch_size, state_size], 0.0, 0.01, dtype=tf.float32)\n",
    "\n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "write_addresses = []\n",
    "for i in range(N):\n",
    "    # Store read and write addresses for later logging\n",
    "    _, curr_read, curr_write, _ = tf.split(state, [controller_state_size,memory_address_size,memory_address_size,-1], 1)\n",
    "    read_addresses.append(curr_read)\n",
    "    write_addresses.append(curr_write)\n",
    "    \n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "\n",
    "    reuse = True\n",
    "\n",
    "# We only start recording the outputs of the controller once we have\n",
    "# finished feeding in the input. We feed zeros as input in the second phase.\n",
    "rnn_outputs = []\n",
    "for i in range(N_out):\n",
    "    output, state = cell(tf.zeros([batch_size,input_size]),state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * tf.log(prediction[i])) for i in range(N_out)]\n",
    "\n",
    "if( model_optimizer == 'adam' ):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "elif( model_optimizer == 'rmsprop' ):\n",
    "    optimizer = tf.train.RMSPropOptimizer(1e-4,decay=0.9,momentum=0.9)\n",
    "\n",
    "cross_entropy = -tf.add_n(ce)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "mean_error = tf.scalar_mul(np.true_divide(1,N_out), tf.add_n(errors))\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- write address in step 0\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "--- write address in step 1\n",
      "[[ 0.33333358  0.33333322  0.         ...,  0.          0.          0.33333319]\n",
      " [ 0.33333302  0.33333346  0.         ...,  0.          0.          0.33333352]\n",
      " [ 0.33333379  0.33333334  0.         ...,  0.          0.          0.33333287]\n",
      " ..., \n",
      " [ 0.33333355  0.33333316  0.         ...,  0.          0.          0.33333325]\n",
      " [ 0.33333325  0.33333352  0.         ...,  0.          0.          0.33333325]\n",
      " [ 0.33333343  0.33333334  0.         ...,  0.          0.          0.33333328]]\n",
      "--- write address in step 2\n",
      "[[ 0.33333331  0.2304818   0.12194162 ...,  0.          0.10285154\n",
      "   0.21139172]\n",
      " [ 0.33333334  0.22051208  0.1057817  ...,  0.          0.11282129\n",
      "   0.22755164]\n",
      " [ 0.33333331  0.23048198  0.12194172 ...,  0.          0.10285135\n",
      "   0.2113916 ]\n",
      " ..., \n",
      " [ 0.33333334  0.23048191  0.12194178 ...,  0.          0.10285142\n",
      "   0.21139154]\n",
      " [ 0.33333334  0.22051224  0.10578168 ...,  0.          0.11282109\n",
      "   0.22755165]\n",
      " [ 0.33333334  0.2205123   0.10578184 ...,  0.          0.11282104\n",
      "   0.22755152]]\n",
      "--- write address in step 3\n",
      "[[ 0.25269431  0.23997161  0.1297392  ...,  0.03082284  0.09336174\n",
      "   0.20359413]\n",
      " [ 0.26034439  0.22201416  0.11084616 ...,  0.03664277  0.11131915\n",
      "   0.22248715]\n",
      " [ 0.25269431  0.23997167  0.12973931 ...,  0.03082277  0.09336164\n",
      "   0.20359401]\n",
      " ..., \n",
      " [ 0.25818583  0.22503698  0.11358823 ...,  0.03624891  0.10829636\n",
      "   0.2197451 ]\n",
      " [ 0.26034451  0.22201422  0.1108462  ...,  0.03664272  0.11131915\n",
      "   0.22248717]\n",
      " [ 0.26034445  0.22201428  0.11084629 ...,  0.0366427   0.11131906\n",
      "   0.22248706]]\n",
      "--- write address in step 4\n",
      "[[ 0.23387179  0.20286769  0.13188036 ...,  0.04508612  0.1161419\n",
      "   0.19005519]\n",
      " [ 0.23451151  0.20341066  0.13087936 ...,  0.04499732  0.11610358\n",
      "   0.19151832]\n",
      " [ 0.2338718   0.20286778  0.13188046 ...,  0.04508607  0.11614181\n",
      "   0.19005516]\n",
      " ..., \n",
      " [ 0.23408248  0.20367207  0.1320602  ...,  0.04461493  0.11536743\n",
      "   0.19036227]\n",
      " [ 0.23511487  0.19739629  0.12236819 ...,  0.04960848  0.12402308\n",
      "   0.19870931]\n",
      " [ 0.23451149  0.20341076  0.13087945 ...,  0.04499726  0.11610347\n",
      "   0.19151823]]\n",
      "--- write address in step 5\n",
      "[[ 0.20841891  0.19310537  0.13494076 ...,  0.05387729  0.11198281\n",
      "   0.17584553]\n",
      " [ 0.20994395  0.1879573   0.12704039 ...,  0.0590094   0.11982299\n",
      "   0.18252984]\n",
      " [ 0.20841892  0.19310543  0.13494082 ...,  0.05387724  0.11198275\n",
      "   0.17584547]\n",
      " ..., \n",
      " [ 0.20933589  0.18880661  0.12859546 ...,  0.05811115  0.11827177\n",
      "   0.18107083]\n",
      " [ 0.21091707  0.18436083  0.12164659 ...,  0.06255569  0.12525012\n",
      "   0.18706596]\n",
      " [ 0.20994394  0.18795736  0.12704048 ...,  0.05900933  0.1198229\n",
      "   0.18252978]]\n",
      "--- write address in step 6\n",
      "[[ 0.19244069  0.17814727  0.13210426 ...,  0.06180643  0.11482467\n",
      "   0.16603756]\n",
      " [ 0.19387762  0.17668481  0.12844472 ...,  0.06425588  0.11846802\n",
      "   0.16967812]\n",
      " [ 0.19244067  0.17814733  0.13210432 ...,  0.06180637  0.11482459\n",
      "   0.16603751]\n",
      " ..., \n",
      " [ 0.19317806  0.17788155  0.13090709 ...,  0.06247487  0.11600507\n",
      "   0.16745666]\n",
      " [ 0.1945166   0.17174271  0.12073091 ...,  0.07038605  0.12618202\n",
      "   0.17557126]\n",
      " [ 0.19334024  0.17856395  0.13169476 ...,  0.0617713   0.11522068\n",
      "   0.16701633]]\n",
      "--- write address in step 7\n",
      "[[ 0.17882425  0.16911843  0.13119298 ...,  0.06517012  0.11185192\n",
      "   0.15613222]\n",
      " [ 0.1797176   0.1685565   0.12953801 ...,  0.06646291  0.11374137\n",
      "   0.15793999]\n",
      " [ 0.17882423  0.16911846  0.13119303 ...,  0.06517006  0.11185185\n",
      "   0.15613216]\n",
      " ..., \n",
      " [ 0.17910124  0.16978954  0.1319688  ...,  0.0643886   0.11116324\n",
      "   0.15583465]\n",
      " [ 0.1805906   0.1645776   0.12279535 ...,  0.07234243  0.12068458\n",
      "   0.16315028]\n",
      " [ 0.17827891  0.17059587  0.13394089 ...,  0.06279593  0.10899106\n",
      "   0.1538735 ]]\n",
      "--- write address in step 8\n",
      "[[ 0.16743819  0.16172527  0.13136622 ...,  0.06552339  0.10715331\n",
      "   0.14604737]\n",
      " [ 0.1688692   0.1586706   0.12523206 ...,  0.07128498  0.11381457\n",
      "   0.15126173]\n",
      " [ 0.16743819  0.1617253   0.13136628 ...,  0.06552333  0.10715324\n",
      "   0.14604734]\n",
      " ..., \n",
      " [ 0.16847223  0.15967681  0.12718199 ...,  0.06943846  0.111715\n",
      "   0.14965461]\n",
      " [ 0.16957541  0.15534261  0.11940821 ...,  0.07680769  0.11990274\n",
      "   0.15565938]\n",
      " [ 0.16845243  0.15951897  0.12694716 ...,  0.06968886  0.11194222\n",
      "   0.14977717]]\n",
      "--- write address in step 9\n",
      "[[ 0.15874587  0.1528808   0.12599115 ...,  0.06990429  0.10775082\n",
      "   0.14137118]\n",
      " [ 0.15938143  0.15280068  0.12524512 ...,  0.07069124  0.10883031\n",
      "   0.14243488]\n",
      " [ 0.15874587  0.15288083  0.12599121 ...,  0.06990425  0.10775077\n",
      "   0.14137115]\n",
      " ..., \n",
      " [ 0.15940592  0.15264098  0.12497032 ...,  0.07096383  0.10911592\n",
      "   0.1426318 ]\n",
      " [ 0.16031021  0.15053642  0.12074935 ...,  0.07525527  0.11379234\n",
      "   0.14613381]\n",
      " [ 0.15893416  0.15278234  0.12564836 ...,  0.07026701  0.10819311\n",
      "   0.14176165]]\n",
      "--- write address in step 10\n",
      "[[ 0.15063331  0.14716685  0.1248688  ...,  0.06930546  0.10366346\n",
      "   0.13408297]\n",
      " [ 0.15059528  0.14720936  0.12497759 ...,  0.06918263  0.10353267\n",
      "   0.13398097]\n",
      " [ 0.15063331  0.14716689  0.12486885 ...,  0.06930543  0.10366343\n",
      "   0.13408296]\n",
      " ..., \n",
      " [ 0.15161486  0.14556441  0.12130202 ...,  0.07320835  0.10783413\n",
      "   0.13724707]\n",
      " [ 0.15179709  0.14518349  0.12050042 ...,  0.07409029  0.10875018\n",
      "   0.1379187 ]\n",
      " [ 0.15015939  0.1473483   0.12563555 ...,  0.06841126  0.10260044\n",
      "   0.1331455 ]]\n",
      "--- write address in step 11\n",
      "[[ 0.14392385  0.14066392  0.12087739 ...,  0.07125524  0.10265492\n",
      "   0.12962401]\n",
      " [ 0.14455484  0.14014579  0.11939301 ...,  0.07305914  0.10459541\n",
      "   0.13116848]\n",
      " [ 0.14282191  0.14190415  0.12402532 ...,  0.06751063  0.0987182\n",
      "   0.12659374]\n",
      " ..., \n",
      " [ 0.14431764  0.14058898  0.12036417 ...,  0.0719308   0.10345508\n",
      "   0.13034007]\n",
      " [ 0.14440642  0.13956219  0.11858043 ...,  0.07388903  0.10529959\n",
      "   0.13151029]\n",
      " [ 0.14419262  0.1405693   0.12044808 ...,  0.07181913  0.10328276\n",
      "   0.13015738]]\n",
      "--- write address in step 12\n",
      "[[ 0.13795701  0.13564791  0.11846285 ...,  0.07125184  0.10015239\n",
      "   0.1247216 ]\n",
      " [ 0.13836497  0.13538358  0.11761163 ...,  0.07237107  0.10133319\n",
      "   0.12566172]\n",
      " [ 0.13789555  0.13556111  0.11837558 ...,  0.07133022  0.10019046\n",
      "   0.12470666]\n",
      " ..., \n",
      " [ 0.13778791  0.13585274  0.11897691 ...,  0.07060795  0.09950332\n",
      "   0.12423725]\n",
      " [ 0.13885984  0.13346773  0.11391188 ...,  0.0766723   0.10542251\n",
      "   0.12839371]\n",
      " [ 0.13796443  0.13576658  0.11865062 ...,  0.07106028  0.09997537\n",
      "   0.12461682]]\n",
      "--- write address in step 13\n",
      "[[ 0.13231228  0.13143854  0.11698741 ...,  0.06983463  0.0965717\n",
      "   0.11941783]\n",
      " [ 0.13319482  0.1303518   0.1142608  ...,  0.07338233  0.10006928\n",
      "   0.12198552]\n",
      " [ 0.13278955  0.13062666  0.11512968 ...,  0.07219232  0.09884504\n",
      "   0.1210302 ]\n",
      " ..., \n",
      " [ 0.13325885  0.13042583  0.11432267 ...,  0.07334689  0.10006527\n",
      "   0.12201955]\n",
      " [ 0.13332571  0.12969421  0.11301511 ...,  0.07486121  0.1014277\n",
      "   0.12286222]\n",
      " [ 0.13284832  0.13069516  0.11518729 ...,  0.07215995  0.09884103\n",
      "   0.12106104]]\n",
      "--- write address in step 14\n",
      "[[ 0.12696566  0.1273943   0.11542863 ...,  0.06816708  0.09297345\n",
      "   0.11433072]\n",
      " [ 0.12806383  0.12691456  0.11350344 ...,  0.07107462  0.09593853\n",
      "   0.11667965]\n",
      " [ 0.12763944  0.12698083  0.11404103 ...,  0.07019514  0.09499964\n",
      "   0.11589122]\n",
      " ..., \n",
      " [ 0.12834844  0.12651813  0.11253928 ...,  0.07236047  0.09716984\n",
      "   0.11755975]\n",
      " [ 0.12804316  0.12629195  0.11245631 ...,  0.07226871  0.09696212\n",
      "   0.11726581]\n",
      " [ 0.12765847  0.12714678  0.11430658 ...,  0.06990384  0.09475489\n",
      "   0.11575858]]\n",
      "--- write address in step 15\n",
      "[[ 0.12243548  0.12333212  0.1129543  ...,  0.06767343  0.09077246\n",
      "   0.11055893]\n",
      " [ 0.12390508  0.12285373  0.11065021 ...,  0.07142279  0.09455711\n",
      "   0.11356712]\n",
      " [ 0.12281193  0.12333676  0.11258224 ...,  0.06839015  0.09152898\n",
      "   0.11120399]\n",
      " ..., \n",
      " [ 0.12421196  0.12238251  0.10952757 ...,  0.07295185  0.09598512\n",
      "   0.11456539]\n",
      " [ 0.12425856  0.12201719  0.10884705 ...,  0.07378608  0.09671123\n",
      "   0.11501051]\n",
      " [ 0.12264921  0.12358949  0.11318327 ...,  0.06757452  0.09076799\n",
      "   0.11067241]]\n",
      "--- write address in step 16\n",
      "[[ 0.11924607  0.11928516  0.10909431 ...,  0.06958739  0.09114368\n",
      "   0.10902862]\n",
      " [ 0.11998244  0.11949546  0.10872287 ...,  0.07061177  0.092299\n",
      "   0.11009918]\n",
      " [ 0.11983106  0.11927724  0.10849147 ...,  0.07077542  0.09237044\n",
      "   0.11005446]\n",
      " ..., \n",
      " [ 0.12026572  0.1191807   0.1078898  ...,  0.07182103  0.09342916\n",
      "   0.11090529]\n",
      " [ 0.12022004  0.11880726  0.10728247 ...,  0.07251064  0.09398665\n",
      "   0.11119887]\n",
      " [ 0.11958295  0.11950684  0.10914157 ...,  0.06980813  0.09145941\n",
      "   0.10939432]]\n",
      "--- write address in step 17\n",
      "[[ 0.11544552  0.1162377   0.10754661 ...,  0.06810969  0.08842362\n",
      "   0.10539646]\n",
      " [ 0.11655728  0.11611386  0.10621312 ...,  0.07067104  0.09098786\n",
      "   0.10746875]\n",
      " [ 0.11611129  0.11617689  0.10676965 ...,  0.06962826  0.08994144\n",
      "   0.10662529]\n",
      " ..., \n",
      " [ 0.1168428   0.11582074  0.10541165 ...,  0.07187715  0.09210233\n",
      "   0.10826131]\n",
      " [ 0.11678183  0.11538953  0.10471315 ...,  0.07266937  0.09273158\n",
      "   0.10858512]\n",
      " [ 0.11626466  0.11607604  0.10643835 ...,  0.07015639  0.09044068\n",
      "   0.10699438]]\n",
      "--- write address in step 18\n",
      "[[ 0.1115827   0.11337265  0.10635065 ...,  0.06589525  0.08511139\n",
      "   0.10142028]\n",
      " [ 0.11348707  0.11292673  0.10364924 ...,  0.0709488   0.09000053\n",
      "   0.10522824]\n",
      " [ 0.11305687  0.11291716  0.10406215 ...,  0.07005725  0.08909584\n",
      "   0.10447907]\n",
      " ..., \n",
      " [ 0.11326096  0.1131079   0.10419689 ...,  0.07007673  0.08919592\n",
      "   0.10464875]\n",
      " [ 0.11348788  0.11265369  0.10316441 ...,  0.07154264  0.09048566\n",
      "   0.1055025 ]\n",
      " [ 0.11313031  0.11298466  0.10410815 ...,  0.07006878  0.0891346\n",
      "   0.10454133]]\n",
      "--- write address in step 19\n",
      "[[ 0.10820094  0.11044838  0.10453601 ...,  0.06465245  0.08281861\n",
      "   0.09830675]\n",
      " [ 0.11032623  0.11032502  0.10218237 ...,  0.06965712  0.08770327\n",
      "   0.10220931]\n",
      " [ 0.11001843  0.11026838  0.10238907 ...,  0.06910633  0.08713274\n",
      "   0.10171944]\n",
      " ..., \n",
      " [ 0.10986532  0.11046207  0.10288896 ...,  0.06833836  0.08645216\n",
      "   0.101254  ]\n",
      " [ 0.11060505  0.10978524  0.10094067 ...,  0.07145797  0.0892722\n",
      "   0.10324343]\n",
      " [ 0.11033447  0.11004274  0.10166987 ...,  0.07029907  0.08822432\n",
      "   0.10250588]]\n",
      "Epoch - 1, Mean error of final batch in epoch - 0.4038\n",
      "Epoch - 2, Mean error of final batch in epoch - 0.3767\n",
      "Epoch - 3, Mean error of final batch in epoch - 0.3555\n",
      "Epoch - 4, Mean error of final batch in epoch - 0.3504\n",
      "Epoch - 5, Mean error of final batch in epoch - 0.3383\n",
      "Epoch - 6, Mean error of final batch in epoch - 0.3316\n",
      "Epoch - 7, Mean error of final batch in epoch - 0.3221\n",
      "Epoch - 8, Mean error of final batch in epoch - 0.3121\n",
      "Epoch - 9, Mean error of final batch in epoch - 0.3058\n",
      "Epoch - 10, Mean error of final batch in epoch - 0.3033\n",
      "Epoch - 11, Mean error of final batch in epoch - 0.2546\n",
      "Epoch - 12, Mean error of final batch in epoch - 0.2357\n",
      "Epoch - 13, Mean error of final batch in epoch - 0.2293\n",
      "Epoch - 14, Mean error of final batch in epoch - 0.2014\n",
      "Epoch - 15, Mean error of final batch in epoch - 0.1895\n",
      "Epoch - 16, Mean error of final batch in epoch - 0.1567\n",
      "Epoch - 17, Mean error of final batch in epoch - 0.1556\n",
      "Epoch - 18, Mean error of final batch in epoch - 0.1171\n",
      "Epoch - 19, Mean error of final batch in epoch - 0.0967\n",
      "Epoch - 20, Mean error of final batch in epoch - 0.0808\n",
      "Epoch - 21, Mean error of final batch in epoch - 0.5033\n",
      "Epoch - 22, Mean error of final batch in epoch - 0.4992\n",
      "Epoch - 23, Mean error of final batch in epoch - 0.5033\n",
      "Epoch - 24, Mean error of final batch in epoch - 0.4946\n",
      "Epoch - 25, Mean error of final batch in epoch - 0.5024\n",
      "Epoch - 26, Mean error of final batch in epoch - 0.4935\n",
      "Epoch - 27, Mean error of final batch in epoch - 0.4986\n",
      "Epoch - 28, Mean error of final batch in epoch - 0.5041\n",
      "Epoch - 29, Mean error of final batch in epoch - 0.4883\n",
      "Epoch - 30, Mean error of final batch in epoch - 0.496\n",
      "Epoch - 31, Mean error of final batch in epoch - 0.4944\n",
      "Epoch - 32, Mean error of final batch in epoch - 0.5002\n",
      "Epoch - 33, Mean error of final batch in epoch - 0.5042\n",
      "Epoch - 34, Mean error of final batch in epoch - 0.5006\n",
      "Epoch - 35, Mean error of final batch in epoch - 0.495\n",
      "Epoch - 36, Mean error of final batch in epoch - 0.4906\n",
      "Epoch - 37, Mean error of final batch in epoch - 0.5046\n",
      "Epoch - 38, Mean error of final batch in epoch - 0.4985\n",
      "Epoch - 39, Mean error of final batch in epoch - 0.4928\n",
      "Epoch - 40, Mean error of final batch in epoch - 0.4962\n",
      "Epoch - 41, Mean error of final batch in epoch - 0.5011\n",
      "Epoch - 42, Mean error of final batch in epoch - 0.4985\n",
      "Epoch - 43, Mean error of final batch in epoch - 0.5016\n",
      "Epoch - 44, Mean error of final batch in epoch - 0.5101\n",
      "Epoch - 45, Mean error of final batch in epoch - 0.5093\n",
      "Epoch - 46, Mean error of final batch in epoch - 0.5016\n",
      "Epoch - 47, Mean error of final batch in epoch - 0.5056\n",
      "Epoch - 48, Mean error of final batch in epoch - 0.5002\n",
      "Epoch - 49, Mean error of final batch in epoch - 0.5061\n",
      "Epoch - 50, Mean error of final batch in epoch - 0.503\n",
      "--- write address in step 0\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "--- write address in step 1\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 3\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 4\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 5\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 6\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 7\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 8\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 9\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 10\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 11\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 12\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 13\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 14\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 15\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 16\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 17\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 18\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "--- write address in step 19\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "Epoch - 51, Mean error of final batch in epoch - 0.5017\n",
      "Epoch - 52, Mean error of final batch in epoch - 0.5068\n",
      "Epoch - 53, Mean error of final batch in epoch - 0.4923\n",
      "Epoch - 54, Mean error of final batch in epoch - 0.4911\n",
      "Epoch - 55, Mean error of final batch in epoch - 0.4984\n",
      "Epoch - 56, Mean error of final batch in epoch - 0.4894\n",
      "Epoch - 57, Mean error of final batch in epoch - 0.501\n",
      "Epoch - 58, Mean error of final batch in epoch - 0.5074\n",
      "Epoch - 59, Mean error of final batch in epoch - 0.4921\n",
      "Epoch - 60, Mean error of final batch in epoch - 0.5047\n",
      "Epoch - 61, Mean error of final batch in epoch - 0.4991\n",
      "Epoch - 62, Mean error of final batch in epoch - 0.5003\n",
      "Epoch - 63, Mean error of final batch in epoch - 0.4961\n",
      "Epoch - 64, Mean error of final batch in epoch - 0.5005\n",
      "Epoch - 65, Mean error of final batch in epoch - 0.4949\n",
      "Epoch - 66, Mean error of final batch in epoch - 0.5023\n",
      "Epoch - 67, Mean error of final batch in epoch - 0.4984\n",
      "Epoch - 68, Mean error of final batch in epoch - 0.4937\n",
      "Epoch - 69, Mean error of final batch in epoch - 0.4973\n",
      "Epoch - 70, Mean error of final batch in epoch - 0.4964\n",
      "Epoch - 71, Mean error of final batch in epoch - 0.4955\n",
      "Epoch - 72, Mean error of final batch in epoch - 0.4969\n",
      "Epoch - 73, Mean error of final batch in epoch - 0.4993\n",
      "Epoch - 74, Mean error of final batch in epoch - 0.5009\n",
      "Epoch - 75, Mean error of final batch in epoch - 0.4977\n",
      "Epoch - 76, Mean error of final batch in epoch - 0.5047\n",
      "Epoch - 77, Mean error of final batch in epoch - 0.5079\n",
      "Epoch - 78, Mean error of final batch in epoch - 0.4997\n",
      "Epoch - 79, Mean error of final batch in epoch - 0.5007\n",
      "Epoch - 80, Mean error of final batch in epoch - 0.4976\n",
      "Epoch - 81, Mean error of final batch in epoch - 0.4951\n",
      "Epoch - 82, Mean error of final batch in epoch - 0.5018\n",
      "Epoch - 83, Mean error of final batch in epoch - 0.4995\n",
      "Epoch - 84, Mean error of final batch in epoch - 0.4981\n",
      "Epoch - 85, Mean error of final batch in epoch - 0.5045\n",
      "Epoch - 86, Mean error of final batch in epoch - 0.5055\n",
      "Epoch - 87, Mean error of final batch in epoch - 0.4937\n",
      "Epoch - 88, Mean error of final batch in epoch - 0.4954\n",
      "Epoch - 89, Mean error of final batch in epoch - 0.5027\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e18c785c6ee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# We sample each batch on the fly from the set of all sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mfa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_to_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0ma_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mone_hots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/random.pyc\u001b[0m in \u001b[0;36mrandint\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \"\"\"\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     def _randbelow(self, n, _log=_log, _int=int, _maxwidth=1L<<BPF,\n",
      "\u001b[0;32m//anaconda/lib/python2.7/random.pyc\u001b[0m in \u001b[0;36mrandrange\u001b[0;34m(self, start, stop, step, _int, _maxwidth)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"non-integer stop for randrange()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mistop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mistart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0;31m# Note that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m#     int(istart + self.random()*width)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "# An annoying thing here is that we cannot use a list as a key in a \n",
    "# dictionary. The workaround we found on StackOverflow here:\n",
    "# http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "\n",
    "# epoch is a global var\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences\n",
    "        for z in range(batch_size):\n",
    "            a = [random.randint(0,num_classes-1) for k in range(N)]\n",
    "            fa = func_to_learn(a)\n",
    "            a_onehot = [one_hots[e] for e in a]\n",
    "            fa_onehot = [one_hots[e] for e in fa]\n",
    "            inp.append(np.array(a_onehot))\n",
    "            out.append(np.array(fa_onehot))        \n",
    "        \n",
    "        feed_dict = {}\n",
    "        for d in range(N):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "        \n",
    "        # for the first batch in an epoch, we have some logging\n",
    "        if( j == 0 and i % 50 == 0 ):\n",
    "            read_addresses_val, write_addresses_val = sess.run([read_addresses,write_addresses],feed_dict)\n",
    "    \n",
    "            s = 0\n",
    "            for r in write_addresses_val:\n",
    "                print(\"--- write address in step \" + str(s))\n",
    "                s = s + 1\n",
    "                print(r)\n",
    "        \n",
    "        # Do gradient descent\n",
    "        summary,_ = sess.run([merged_summaries,minimize], feed_dict)\n",
    "        \n",
    "        # Write out TensorBoard logs\n",
    "        file_writer.add_summary(summary)\n",
    "    current_mean = np.mean(sess.run(errors, feed_dict))\n",
    "    print(\"Epoch - \" + str(i+1) + \", Mean error of final batch in epoch - \" + str(current_mean))\n",
    "    \n",
    "    # DEBUG\n",
    "    #with tf.variable_scope(\"NTM\",reuse=True):\n",
    "    #    H = tf.get_variable(\"H\", [controller_state_size,controller_state_size])\n",
    "    #    print(sess.run(H))\n",
    "\n",
    "# Write out variables to disk\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took\", time.time() - pre_train_time, \"seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0a31d5f0cde8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Restore the weights from training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# DEBUG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'saver' is not defined"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "# Note that all the weights will be loaded from the saved training session\n",
    "inputs_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest)]\n",
    "targets_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest_out)]\n",
    "state_test = tf.truncated_normal([batch_size, state_size], 0.0, 0.01, dtype=tf.float32)\n",
    "\n",
    "# Set up test graph\n",
    "reuse = True\n",
    "for i in range(Ntest):\n",
    "    output, state = cell(inputs_test[i],state_test,'NTM',reuse)\n",
    "\n",
    "rnn_outputs_test = []\n",
    "for i in range(Ntest_out):\n",
    "    output, state = cell(tf.zeros([batch_size,input_size]),state_test,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "    \n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.softmax(logit) for logit in logits_test] \n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets_test[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "\n",
    "# Restore the weights from training\n",
    "sess = tf.Session()\n",
    "saver.restore(sess,save_path)\n",
    "\n",
    "# DEBUG\n",
    "#with tf.variable_scope(\"NTM\",reuse=True):\n",
    "#    H = tf.get_variable(\"H\", [controller_state_size,controller_state_size])\n",
    "#    print(sess.run(H))\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "#print(\"Number of batches: \" + str(no_of_batches))\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    # We sample each batch on the fly from the set of all sequences\n",
    "    for z in range(batch_size):\n",
    "        a = [random.randint(0,num_classes-1) for k in range(Ntest)]\n",
    "        fa = func_to_learn(a)\n",
    "        a_onehot = [one_hots[e] for e in a]\n",
    "        fa_onehot = [one_hots[e] for e in fa]\n",
    "        inp.append(np.array(a_onehot))\n",
    "        out.append(np.array(fa_onehot))        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(Ntest):\n",
    "        in_node = inputs_test[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest_out):\n",
    "        out_node = targets_test[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = np.mean(sess.run(errors_test, feed_dict))\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "# The first three digits of this should match the printout for the\n",
    "# first three test output sequences given earlier\n",
    "#data = sess.run([tf.argmax(targets[0],1), tf.argmax(prediction[0],1)],feed_dict)\n",
    "\n",
    "#print(\"First digits of test outputs (actual)\")\n",
    "#print(data[0])\n",
    "#print(\"First digits of test outputs (predicted)\")\n",
    "#print(data[1])\n",
    "\n",
    "# print the mean of the errors in each digit for the test set.\n",
    "#incorrects = sess.run(errors, feed_dict)\n",
    "# print(incorrects)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"N_out         - \" + str(N_out))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "print(\"Ntest_out     - \" + str(Ntest_out))\n",
    "print(\"ring 1 powers - \" + str(powers_ring1))\n",
    "print(\"ring 2 powers - \" + str(powers_ring2))\n",
    "print(\"# epochs      - \" + str(epoch))\n",
    "print(\"optimizer     - \" + str(model_optimizer))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"(css,mas,mcs) - (\" + str(controller_state_size) + \",\" + str(memory_address_size) + \",\" + str(memory_content_size) + \")\")\n",
    "print(\"train percent - \" + str(training_percent))\n",
    "print(\"num_training  - \" + str(num_training) + \"/\" + str(num_classes**N))\n",
    "print(\"num_test      - \" + str(num_test) + \"/\" + str(num_classes**N))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error         - \" + str(final_error))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
