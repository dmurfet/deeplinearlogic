{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of the Linear Logic Recurrent Neural Network (LLRNN)\n",
    "#\n",
    "# Version 8.0\n",
    "\n",
    "###################\n",
    "# HYPERPARAMETERS #\n",
    "###################\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, mult_pattern_ntm\n",
    "task                  = 'copy' # copy, repeat copy, pattern i, mult pattern i\n",
    "epoch                 = 100 # number of training epochs, default to 100\n",
    "num_classes           = 10 # number of symbols, INCLUDING initial and terminal symbols, default 10\n",
    "N                     = 30 # length of input sequences for training, default to 30, INCLUDING initial and terminal symbols\n",
    "Ntest                 = 35 # length of sequences for testing, default to 35, INCLUDING initial and terminal symbols\n",
    "batch_size            = 250 # default 250\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "num_training          = 10000 # default 10000\n",
    "num_test              = num_training\n",
    "term_symbol           = num_classes - 1\n",
    "init_symbol           = num_classes - 2\n",
    "div_symbol            = num_classes - 3\n",
    "learning_rate         = 1e-4 # default 1e-4\n",
    "memory_init_bias      = 1.0 # default 1.0\n",
    "use_curriculum        = False # default False\n",
    "\n",
    "##################\n",
    "# MODEL SPECIFIC #\n",
    "##################\n",
    "\n",
    "ntm_memory_address_size   = 128 # number of memory locations, default 128\n",
    "ntm_memory_content_size   = 20 # size of vector stored at a memory location, default 20\n",
    "ntm_powers                = [0,-1,1] # powers of R used by controller, default [0,-1,1]\n",
    "\n",
    "pattern_ntm_powers               = [[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "pattern_ntm_powers_2_on_1        = [0,2,4] # allowed powers used by ring 2 to manipulate ring 1\n",
    "pattern_ntm_memory_address_sizes = [128, 128] # number of memory locations for the three rings\n",
    "pattern_ntm_memory_content_sizes = [20, 3] # size of content vector for each ring\n",
    "pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "mult_pattern_ntm_powers               = [[0,-1,1],[0,-1,1],[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "mult_pattern_ntm_powers_2_on_1        = [0,1,-1] # allowed powers used by rings 2,3 to manipulate ring 1\n",
    "mult_pattern_ntm_memory_address_sizes = [128, 20, 20, 10] # number of memory locations for the rings\n",
    "mult_pattern_ntm_memory_content_sizes = [20, 3, 3, 2] # size of content vector for each ring\n",
    "mult_pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there\n",
    "# 2. The initial and terminal symbols are always from the end of the list of symbols, so they\n",
    "# are respectively num_classes - 2 and num_classes - 1. So the number of symbols which are\n",
    "# not initial or terminal is num_classes - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import pandas\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs\n",
    "\n",
    "assert use_model == 'ntm' or use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[7, 1, 3, 5, 3, 4, 5, 6]\n",
      "is mapped to\n",
      "[1, 1, 3, 3, 5, 5, 3, 3, 4, 4, 5, 5, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "# Below N_out and Ntest_out are the lengths of the outputs in both the training\n",
    "# and testing regimes respectively. Since outputs do not include the initial and terminal\n",
    "# symbols, these default to N - 2 and Ntest - 2 respectively.\n",
    "\n",
    "# Default sampling from space of inputs\n",
    "def generate_input_seq_default(max_symbol,input_length):\n",
    "    return [random.randint(0,max_symbol) for k in range(input_length)]\n",
    "\n",
    "generate_input_seq = generate_input_seq_default\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N - 2\n",
    "    Ntest_out = Ntest - 2\n",
    "    seq_length_min = 4\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "if( task == 'repeat copy' ):\n",
    "    no_of_copies = 2\n",
    "    pattern = [0]*(no_of_copies - 1) + [1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = no_of_copies * (N - 2)\n",
    "    Ntest_out = no_of_copies * (Ntest - 2)\n",
    "    seq_length_min = 4\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 1\n",
    "if( task == 'pattern 1' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,1,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,c,c,d,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = (N - 2) + divmod(N - 2, 2)[0] # N - 2 plus the number of times 2 divides N - 2\n",
    "    Ntest_out = (Ntest - 2) + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 4\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 2\n",
    "if( task == 'pattern 2' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = N - 2 + divmod(N - 2, 2)[0]\n",
    "    Ntest_out = Ntest - 2 + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 4\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 3\n",
    "if( task == 'pattern 3' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,b,b,d,c,c,e,d,d,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 4 + (N - 2 - 2) * 3\n",
    "    Ntest_out = 4 + (Ntest - 2 - 2) * 3\n",
    "    seq_length_min = 4\n",
    "\n",
    "################\n",
    "# PATTERN TASK 4\n",
    "if( task == 'pattern 4' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2,1,2,-2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,d,f,d,c,c,e,f,h,f,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 4\n",
    "\n",
    "################\n",
    "# PATTERN TASK 5\n",
    "if( task == 'pattern 5' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [4,1,1,-4] # so (a,b,c,d,e,f,...) goes to (a,e,f,g,k,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 4\n",
    "\n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 1\n",
    "if( task == 'mult pattern 1' or task == 'mult pattern 2'):\n",
    "    pattern1 = [1] # so (a,b,c,d,e,f,...) goes to (a,b,c,d,e,f,...)\n",
    "    pattern2 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 4\n",
    "    \n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 2\n",
    "if( task == 'mult pattern 2' ):\n",
    "    # Almost everything is the same as mult pattern 1, but in pattern 2 we \n",
    "    # make sure there is a div symbol somewhere in the sequence\n",
    "    def generate_input_seq_forcediv(max_symbol,input_length):\n",
    "        t = [random.randint(0,max_symbol) for k in range(input_length)]\n",
    "        div_pos = random.randint(0,len(t)-1)\n",
    "        t[div_pos] = div_symbol\n",
    "        return t\n",
    "    \n",
    "    generate_input_seq = generate_input_seq_forcediv\n",
    "\n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 3\n",
    "if( task == 'mult pattern 3'):\n",
    "    pattern1 = [1] # so (a,b,c,d,e,f,...) goes to (a,b,c,d,e,f,...)\n",
    "    pattern2 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    pattern3 = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2,pattern3],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 6\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = [random.randint(0,num_classes-3) for i in range(N - 2)]\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# INITIALISE STATE #\n",
    "####################\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "input_size = num_classes # dimension of the input space I\n",
    "\n",
    "#####\n",
    "# NTM\n",
    "\n",
    "def init_state_ntm(batch_size, css, mas, mcs):\n",
    "    state_size = css + 2*mas + mas * mcs\n",
    "    \n",
    "    ra = [0.0]*mas\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,mas]) + ra\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    init_read_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_write_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_memory = tf.truncated_normal([batch_size, mas*mcs], 0.0, 1e-6, dtype=tf.float32)\n",
    "    \n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "#############\n",
    "# PATTERN NTM\n",
    "\n",
    "def init_state_pattern_ntm(batch_size, css, mas, mcs):\n",
    "    # mas and mcs are arrays of address sizes and content sizes for rings\n",
    "    state_size = css\n",
    "    \n",
    "    init_address = []\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        state_size = state_size + mas[i] * mcs[i] # for memory vector\n",
    "        state_size = state_size + 2 * mas[i] # for addresses (read and write)\n",
    "    \n",
    "        ra = [0.0]*mas[i]\n",
    "        ra[0] = 1.0\n",
    "        init_address.append(np.zeros([batch_size,mas[i]]) + ra)\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    \n",
    "    tensor_list = [init_controller_state]\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        init_read_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        init_write_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        tensor_list = tensor_list + [init_read_address,init_write_address]\n",
    "        \n",
    "    for i in range(len(mas)):\n",
    "        # The first ring is initialised to zero, the rest differently\n",
    "        if( i == 0 ):\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "        else:\n",
    "            # This initialisation has the result of biasing the output of rings 2 and 3 to be\n",
    "            # \"no rotation\" and biasing ring 4 to say \"use ring 2\"\n",
    "            ra = [0.0]*mcs[i] \n",
    "            ra[0] = memory_init_bias\n",
    "            ra = np.zeros([batch_size,mas[i],mcs[i]]) + ra\n",
    "            ra = tf.constant(ra,dtype=tf.float32,shape=[batch_size,mas[i],mcs[i]])\n",
    "            ra = tf.reshape(ra,[batch_size,mas[i]*mcs[i]])\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32) + ra\n",
    "            #init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "            \n",
    "        tensor_list = tensor_list + [init_memory]\n",
    "    \n",
    "    state = tf.concat(tensor_list,1)\n",
    "\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "######################\n",
    "# MULTIPLE PATTERN NTM\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_25/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_24/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_23/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_22/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_21/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_20/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_19/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_18/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_17/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_15/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_13/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_11/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_9/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_7/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_5/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_3/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_1/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "\n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "read_addresses2 = []\n",
    "read_addresses3 = []\n",
    "read_addresses4 = []\n",
    "write_addresses = []\n",
    "write_addresses2 = []\n",
    "write_addresses3 = []\n",
    "write_addresses4 = []\n",
    "interps = []\n",
    "rnn_outputs = []\n",
    "m2 = []\n",
    "m3 = []\n",
    "m4 = []\n",
    "    \n",
    "for i in range(N + N_out):\n",
    "    # Logging\n",
    "    if( use_model == 'ntm' ):\n",
    "        h0, curr_read, curr_write, _ = tf.split(state, [controller_state_size,ntm_memory_address_size,\n",
    "                                                        ntm_memory_address_size,-1], 1)\n",
    "\n",
    "    if( use_model == 'pattern_ntm' ):\n",
    "        mas = pattern_ntm_memory_address_sizes\n",
    "        mcs = pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],mas[0] * mcs[0],mas[1] * mcs[1]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        m1_state = ret[5]\n",
    "        m2_state = ret[6]\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm' ):\n",
    "        mas = mult_pattern_ntm_memory_address_sizes\n",
    "        mcs = mult_pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],                        \n",
    "                            mas[2],mas[2],mas[3],mas[3],mas[0] * mcs[0],mas[1] * mcs[1],\n",
    "                            mas[2] * mcs[2],mas[3] * mcs[3]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        curr_read3 = ret[5]\n",
    "        curr_write3 = ret[6]\n",
    "        curr_read4 = ret[7]\n",
    "        curr_write4 = ret[8]\n",
    "        m1_state = ret[9]\n",
    "        m2_state = ret[10]\n",
    "        m3_state = ret[11]\n",
    "        m4_state = ret[12]\n",
    "\n",
    "    #### RUN MODEL ####\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "    ###################\n",
    "    \n",
    "    # More logging\n",
    "    read_addresses.append(curr_read[0,:])\n",
    "    write_addresses.append(curr_write[0,:])\n",
    "    \n",
    "    if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses2.append(curr_read2[0,:])\n",
    "        write_addresses2.append(curr_write2[0,:])\n",
    "        m2_state = tf.reshape(m2_state, [-1,mas[1],mcs[1]])\n",
    "        m2.append(tf.nn.softmax(m2_state[0,:]))\n",
    "        \n",
    "        with tf.variable_scope(\"NTM\",reuse=True):\n",
    "            W_interp = tf.get_variable(\"W_interp\", [controller_state_size,1])\n",
    "            B_interp = tf.get_variable(\"B_interp\", [1])\n",
    "            interp = tf.sigmoid(tf.matmul(h0,W_interp) + B_interp)\n",
    "            interp_matrix = tf.concat([interp,tf.ones_like(interp,dtype=tf.float32) - interp],axis=1) # shape [-1,2]\n",
    "            interps.append(interp_matrix[0,:])\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses3.append(curr_read3[0,:])\n",
    "        write_addresses3.append(curr_write3[0,:])\n",
    "        read_addresses4.append(curr_read4[0,:])\n",
    "        write_addresses4.append(curr_write4[0,:])\n",
    "        m3_state = tf.reshape(m3_state, [-1,mult_pattern_ntm_memory_address_sizes[2],mult_pattern_ntm_memory_content_sizes[2]])\n",
    "        m3.append(tf.nn.softmax(m3_state[0,:]))\n",
    "        m4_state = tf.reshape(m4_state, [-1,mult_pattern_ntm_memory_address_sizes[3],mult_pattern_ntm_memory_content_sizes[3]])\n",
    "        m4_state = m4_state[0,:]\n",
    "        m4_state = tf.concat([tf.nn.softmax(m4_state),tf.zeros([mult_pattern_ntm_memory_address_sizes[3],1])],1)\n",
    "        m4.append(m4_state)\n",
    "\n",
    "    reuse = True\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# Note: prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "\n",
    "# Note: We allow the length of input sequences to vary between batches, which means\n",
    "# that the cross entropy needs to be masked to the relevant part of the output\n",
    "\n",
    "# Note: we use log_softmax to avoid precision issues with floats causing log(0) to create NaNs\n",
    "\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.log_softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * prediction[i]) for i in range(N + N_out)] # an array of numbers\n",
    "mask = [tf.sign(tf.reduce_max(tf.abs(targets[i]))) for i in range(N + N_out)]\n",
    "ce_mask = [ce[i] * mask[i] for i in range(N + N_out)]\n",
    "cross_entropy = -tf.add_n(ce_mask)\n",
    "cross_entropy /= tf.add_n(mask) # DEBUG do we really need this?\n",
    "# NOTE: here in creating the mask we are assuming that batches have the same sequence length\n",
    "                    \n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate,decay=0.9,momentum=0.9)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N + N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "errors_mask = [errors[i] * mask[i] for i in range(N + N_out)]\n",
    "mean_error = tf.add_n(errors_mask)\n",
    "mean_error /= tf.add_n(mask)\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1, mean error - 0.928417\n",
      "Epoch - 2, mean error - 0.900462\n",
      "\n",
      "It took 28 seconds to train.\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "###################\n",
    "# Note on sequences\n",
    "#\n",
    "# Our sequences are of varying length, in the alphabet {0,...,num_classes - 3}.\n",
    "# Each input sequence begins with an initial symbol and ends with a terminal symbol\n",
    "# (the value of which are num_classes - 2 and num_classes - 1 by default). Output\n",
    "# sequences do not have either an initial nor a terminal symbol.\n",
    "#\n",
    "# Both input and output sequences are written on a \"tape\" of length N + N_out.\n",
    "# Input sequences are aligned at the BEGINNING of the tape, and all remaining space\n",
    "# is filled with terminal symbols. Output sequences are aligned at the END OF THE \n",
    "# MATCHING INPUT, with all remaining space filled with zero vectors.\n",
    "#\n",
    "# Example: suppose N = N_out = 10, and num_classes = 10 so that init_symbol = 8\n",
    "# and term_symbol = 9. Then a sequence of length 8 (seq_length = 10 below) is\n",
    "#\n",
    "# a = [4, 4, 5, 6, 3, 3, 6, 7]\n",
    "#\n",
    "# which written on the tape is\n",
    "#\n",
    "# [8, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
    "#\n",
    "# If we are performing the copy task, so that the output sequence is also a, then\n",
    "# the output written on the tape will be (notice the alignment)\n",
    "#\n",
    "# [-, -, -, -, -, -, -, -, -, 4, 4, 5, 6, 3, 3, 6, 7, -, -, -]\n",
    "#\n",
    "# where - is a symbol whose encoding is the zero vector.\n",
    "\n",
    "def io_generator(max_symbol, input_length, total_length):\n",
    "    \"\"\"\n",
    "    Returns a one-hot encoded pair of input and output sequence, with terminal and initial symbols.\n",
    "    \n",
    "    max_symbol - generate sequences in 0,...,max_symbol\n",
    "    input_length - length of input sequences, without initial and terminal symbols\n",
    "    total_length - length of the buffer, so that the sequences are padded to this length\n",
    "    \"\"\"\n",
    "    a = generate_input_seq(max_symbol,input_length)\n",
    "    fa = func_to_learn(a)\n",
    "    a = [init_symbol] + a + [term_symbol]\n",
    "    a = a + [term_symbol for k in range(total_length-len(a))]\n",
    "    a_onehot = [one_hots[e] for e in a]\n",
    "    fa_onehot = [[0.0]*num_classes for k in range(input_length+1)] + \\\n",
    "                [one_hots[e] for e in fa] + \\\n",
    "                [[0.0]*num_classes for k in range(total_length-(input_length+1)-len(fa))]\n",
    "    return a, np.array(a_onehot), np.array(fa_onehot)\n",
    "\n",
    "error_means = []\n",
    "epoch_error_means = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences. Each\n",
    "        # batch has a fixed length of the sequences. Recall that all input seqs\n",
    "        # have an initial and terminal symbol, so if seq_length = 10 then there\n",
    "        # are eight positions for the \"content\" symbols\n",
    "        \n",
    "        # Our version of curriculum training says: spend the first half\n",
    "        # of the epochs ramping up to the full training set. Assuming that\n",
    "        # epoch > N we divide allocate each integer in [seq_length_min,N]\n",
    "        # an equal portion of the first half of the epochs.\n",
    "        if( use_curriculum == True ):\n",
    "            if( 2 * i > epoch ):\n",
    "                seq_length_max = N\n",
    "            else:\n",
    "                curriculum_band = max(1,int(epoch/(2*(N - seq_length_min))))\n",
    "                seq_length_max = min(seq_length_min + int(i/curriculum_band),N)\n",
    "        else:\n",
    "            seq_length_max = N\n",
    "            \n",
    "        seq_length = random.randint(seq_length_min,seq_length_max)\n",
    "        \n",
    "        for z in range(batch_size):\n",
    "            a, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=N+N_out)\n",
    "            \n",
    "            inp.append(a_onehot)\n",
    "            out.append(fa_onehot)\n",
    "            \n",
    "            # Record the first sequence in the last batch of the last epoch\n",
    "            if( i == epoch - 1 and j == no_of_batches - 1 and z == 0):\n",
    "                final_seq = a\n",
    "        \n",
    "        # An annoying thing here is that we cannot use a list as a key in a \n",
    "        # dictionary. The workaround we found on StackOverflow here:\n",
    "        # http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "        feed_dict = {}\n",
    "        \n",
    "        for d in range(N + N_out):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N + N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "\n",
    "        ##### Do gradient descent #####\n",
    "        mean_error_val,_ = sess.run([mean_error,minimize], feed_dict)\n",
    "        ###############################\n",
    "        \n",
    "        error_means.append(mean_error_val)\n",
    "    \n",
    "    epoch_error = np.mean(error_means[-no_of_batches:])\n",
    "    epoch_error_means.append(epoch_error)\n",
    "    \n",
    "    # Print the mean error of the final batch in the epoch\n",
    "    print_str = \"Epoch - \" + str(i+1) + \", mean error - \" + str(epoch_error)\n",
    "    \n",
    "    if( use_curriculum == True ):\n",
    "        print_str = print_str + \", training at max length - \" + str(seq_length_max)\n",
    "        \n",
    "    print(print_str)\n",
    "\n",
    "# For the final batch of the final epoch, we record the memory states as well\n",
    "seq_length_for_vis = seq_length - 2\n",
    "interps_val = sess.run(interps,feed_dict)\n",
    "m2_val, m3_val, m4_val = sess.run([m2,m3,m4],feed_dict)            \n",
    "r1_val, w1_val = sess.run([read_addresses,write_addresses],feed_dict)\n",
    "r2_val, w2_val = sess.run([read_addresses2,write_addresses2],feed_dict)\n",
    "r3_val, w3_val = sess.run([read_addresses3,write_addresses3],feed_dict)\n",
    "r4_val, w4_val = sess.run([read_addresses4,write_addresses4],feed_dict)\n",
    "errors_mask_val = sess.run(errors_mask,feed_dict)\n",
    "#errors_mask_0 = [a[0,:] for a in errors_mask_val]\n",
    "\n",
    "# Write out variables to disk\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took \" + str(int(time.time() - pre_train_time)) + \" seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length used for visualisations - 7\n",
      "\n",
      "Sequence used for visualisations is (Note: initialisation symbol is 8 and terminal symbol is 9)\n",
      "[8, 3, 2, 3, 2, 4, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "\n",
      "Error probabilities for final batch\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78399998, 0.81599998, 0.80000001, 0.81199998, 0.78799999, 0.77999997, 0.83600003, 0.92000002, 0.94, 0.94800001, 0.97600001, 0.99199998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAAL8CAYAAAC2zccJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12be03550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 1 #\n",
    "###########################\n",
    "\n",
    "print(\"Sequence length used for visualisations - \" + str(seq_length_for_vis))\n",
    "print(\"\")\n",
    "print(\"Sequence used for visualisations is (Note: initialisation symbol is \" + str(init_symbol) + \" and terminal symbol is \" + str(term_symbol) + \")\")\n",
    "print(final_seq)\n",
    "print(\"\")\n",
    "print(\"Error probabilities for final batch\")\n",
    "print(errors_mask_val)\n",
    "print(\"\")\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 9, 13\n",
    "fig_num = 0\n",
    "\n",
    "# RING 1\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "plt.figure(fig_num)\n",
    "ax1 = plt.subplot(2,1,1)\n",
    "ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "ax1.imshow(np.stack(w1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax1.set_title('Write address (ring 1)')\n",
    "ax1.set_xlabel('position')\n",
    "ax1.set_ylabel('time')\n",
    "\n",
    "ax2.imshow(np.stack(r1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax2.set_title('Read address (ring 1)')\n",
    "ax2.set_xlabel('position')\n",
    "ax2.set_ylabel('time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 2 #\n",
    "###########################\n",
    "\n",
    "if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 2)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 2)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Assume that powers2_on_1 has three entries we can use as colour channels\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 2)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    max_xticks = 2\n",
    "    xloc = plt.MaxNLocator(max_xticks)\n",
    "\n",
    "    ax.imshow(np.stack(interps_val), cmap='bone', interpolation='nearest', aspect='auto')\n",
    "    ax.set_title('Interpolation')\n",
    "    ax.set_xlabel('direct vs indirect')\n",
    "    ax.set_ylabel('time')\n",
    "    ax.xaxis.set_major_locator(xloc)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# VISUALISATIONS - OTHER RINGS #\n",
    "################################\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 3)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 3)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 3)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 4)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 4)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax6 = plt.subplot(1,1,1)    \n",
    "    ax6.imshow(np.stack(m4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax6.set_title('Memory contents (ring 4)')\n",
    "    ax6.set_xlabel('position')\n",
    "    ax6.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3kAAAUKCAYAAABblriAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x152e1c9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########################\n",
    "# VISUALISATIONS - ERROR #\n",
    "##########################\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "\n",
    "plt.figure(fig_num)\n",
    "ax = plt.subplot(1,1,1)\n",
    "sc = pandas.Series(error_means)\n",
    "ma = sc.rolling(window=500).mean()\n",
    "ax.plot(sc.index, sc, color='lightgray')\n",
    "ax.plot(ma.index, ma, color='red')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(sc.index.min(), sc.index.max())\n",
    "ax.set_title('Error')\n",
    "ax.set_xlabel('batch')\n",
    "ax.set_ylabel('error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on sequences of length 13\n",
      "\n",
      "Batch - 1, Mean error - 0.907\n",
      "Batch - 2, Mean error - 0.901833\n",
      "Batch - 3, Mean error - 0.917667\n",
      "Batch - 4, Mean error - 0.907833\n",
      "\n",
      "###########\n",
      "# Summary #\n",
      "###########\n",
      "\n",
      "model         - ntm\n",
      "task name     - mult pattern 3\n",
      "epochs        - 2\n",
      "num_classes   - 10\n",
      "N             - 10\n",
      "Ntest         - 15\n",
      "# weights     - 18958\n",
      "\n",
      "\n",
      "error train(test) - 0.900462 (0.908583)\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "# Restore the weights from training\n",
    "sess = tf.Session()\n",
    "saver.restore(sess,save_path)\n",
    "\n",
    "inputs_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "targets_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "# Set up test graph\n",
    "rnn_outputs_test = []\n",
    "reuse = True\n",
    "for i in range(Ntest + Ntest_out):\n",
    "    output, state = cell(inputs_test[i],state,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "\n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size])\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.log_softmax(logit) for logit in logits_test] \n",
    "mask = [tf.sign(tf.reduce_max(tf.abs(targets_test[i]))) for i in range(Ntest + Ntest_out)]\n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets_test[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest + Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "errors_test_mask = [errors_test[i] * mask[i] for i in range(Ntest + Ntest_out)]\n",
    "mean_error_test = tf.add_n(errors_test_mask)\n",
    "mean_error_test /= tf.add_n(mask)\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "\n",
    "seq_length = Ntest\n",
    "print(\"Testing on sequences of length \" + str(seq_length-2))\n",
    "print(\"\")\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    for z in range(batch_size):\n",
    "        a, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=Ntest+Ntest_out)\n",
    "            \n",
    "        inp.append(a_onehot)\n",
    "        out.append(fa_onehot)        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        in_node = inputs_test[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        out_node = targets_test[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = sess.run(mean_error_test, feed_dict)\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"epochs        - \" + str(epoch))\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error train(test) - \" + str(epoch_error_means[-1]) + \" (\" + str(final_error) + \")\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
