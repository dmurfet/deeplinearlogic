{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of the Linear Logic Recurrent Neural Network (LLRNN)\n",
    "#\n",
    "# Version 8.0\n",
    "\n",
    "###################\n",
    "# HYPERPARAMETERS #\n",
    "###################\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, mult_pattern_ntm\n",
    "task                  = 'mult pattern 3' # copy, repeat copy, pattern i, mult pattern i\n",
    "epoch                 = 2 # number of training epochs, default to 100\n",
    "num_classes           = 10 # number of symbols, INCLUDING initial and terminal symbols, default 10\n",
    "N                     = 10 # length of input sequences for training, default to 30, INCLUDING initial and terminal symbols\n",
    "Ntest                 = 15 # length of sequences for testing, default to 35, INCLUDING initial and terminal symbols\n",
    "batch_size            = 250 # default 250\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "num_training          = 1000 # default 10000\n",
    "num_test              = num_training\n",
    "term_symbol           = num_classes - 1\n",
    "init_symbol           = num_classes - 2\n",
    "div_symbol            = num_classes - 3\n",
    "learning_rate         = 1e-4\n",
    "memory_init_bias      = 1.0\n",
    "use_curriculum        = False\n",
    "\n",
    "##################\n",
    "# MODEL SPECIFIC #\n",
    "##################\n",
    "\n",
    "ntm_memory_address_size   = 128 # number of memory locations, default 128\n",
    "ntm_memory_content_size   = 20 # size of vector stored at a memory location, default 20\n",
    "ntm_powers                = [0,-1,1] # powers of R used by controller, default [0,-1,1]\n",
    "\n",
    "pattern_ntm_powers               = [[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "pattern_ntm_powers_2_on_1        = [0,2,4] # allowed powers used by ring 2 to manipulate ring 1\n",
    "pattern_ntm_memory_address_sizes = [128, 128] # number of memory locations for the three rings\n",
    "pattern_ntm_memory_content_sizes = [20, 3] # size of content vector for each ring\n",
    "pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "mult_pattern_ntm_powers               = [[0,-1,1],[0,-1,1],[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "mult_pattern_ntm_powers_2_on_1        = [0,1,-1] # allowed powers used by rings 2,3 to manipulate ring 1\n",
    "mult_pattern_ntm_memory_address_sizes = [128, 20, 20, 10] # number of memory locations for the rings\n",
    "mult_pattern_ntm_memory_content_sizes = [20, 3, 3, 2] # size of content vector for each ring\n",
    "mult_pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there\n",
    "# 2. The initial and terminal symbols are always from the end of the list of symbols, so they\n",
    "# are respectively num_classes - 2 and num_classes - 1. So the number of symbols which are\n",
    "# not initial or terminal is num_classes - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import pandas\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs\n",
    "\n",
    "assert use_model == 'ntm' or use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[4, 6, 4, 4, 2, 4, 3, 3]\n",
      "is mapped to\n",
      "[4, 6, 4, 4, 2, 4, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "# Below N_out and Ntest_out are the lengths of the outputs in both the training\n",
    "# and testing regimes respectively. Since outputs do not include the initial and terminal\n",
    "# symbols, these default to N - 2 and Ntest - 2 respectively.\n",
    "\n",
    "# Default sampling from space of inputs\n",
    "def generate_input_seq_default(max_symbol,input_length):\n",
    "    return [random.randint(0,max_symbol) for k in range(input_length)]\n",
    "\n",
    "generate_input_seq = generate_input_seq_default\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N - 2\n",
    "    Ntest_out = Ntest - 2\n",
    "    seq_length_min = 4\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "if( task == 'repeat copy' ):\n",
    "    no_of_copies = 2\n",
    "    pattern = [0]*(no_of_copies - 1) + [1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = no_of_copies * (N - 2)\n",
    "    Ntest_out = no_of_copies * (Ntest - 2)\n",
    "    seq_length_min = 4\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 1\n",
    "if( task == 'pattern 1' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,1,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,c,c,d,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = (N - 2) + divmod(N - 2, 2)[0] # N - 2 plus the number of times 2 divides N - 2\n",
    "    Ntest_out = (Ntest - 2) + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 4\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 2\n",
    "if( task == 'pattern 2' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = N - 2 + divmod(N - 2, 2)[0]\n",
    "    Ntest_out = Ntest - 2 + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 4\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 3\n",
    "if( task == 'pattern 3' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,b,b,d,c,c,e,d,d,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 4 + (N - 2 - 2) * 3\n",
    "    Ntest_out = 4 + (Ntest - 2 - 2) * 3\n",
    "    seq_length_min = 4\n",
    "\n",
    "################\n",
    "# PATTERN TASK 4\n",
    "if( task == 'pattern 4' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2,1,2,-2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,d,f,d,c,c,e,f,h,f,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 4\n",
    "\n",
    "################\n",
    "# PATTERN TASK 5\n",
    "if( task == 'pattern 5' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [4,1,1,-4] # so (a,b,c,d,e,f,...) goes to (a,e,f,g,k,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 4\n",
    "\n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 1\n",
    "if( task == 'mult pattern 1' or task == 'mult pattern 2'):\n",
    "    pattern1 = [1] # so (a,b,c,d,e,f,...) goes to (a,b,c,d,e,f,...)\n",
    "    pattern2 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,pattern1,pattern2,div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 4\n",
    "    \n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 2\n",
    "if( task == 'mult pattern 2' ):\n",
    "    # Almost everything is the same as mult pattern 1, but in pattern 2 we \n",
    "    # make sure there is a div symbol somewhere in the sequence\n",
    "    def generate_input_seq_forcediv(max_symbol,input_length):\n",
    "        t = [random.randint(0,max_symbol) for k in range(input_length)]\n",
    "        div_pos = random.randint(0,len(t)-1)\n",
    "        t[div_pos] = div_symbol\n",
    "        return t\n",
    "    \n",
    "    generate_input_seq = generate_input_seq_forcediv\n",
    "\n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 3\n",
    "if( task == 'mult pattern 3'):\n",
    "    pattern1 = [1] # so (a,b,c,d,e,f,...) goes to (a,b,c,d,e,f,...)\n",
    "    pattern2 = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,pattern1,pattern2,div_symbol)\n",
    "    N_out = N-2\n",
    "    Ntest_out = Ntest-2\n",
    "    seq_length_min = 6\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = [random.randint(0,num_classes-3) for i in range(N - 2)]\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# INITIALISE STATE #\n",
    "####################\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "input_size = num_classes # dimension of the input space I\n",
    "\n",
    "#####\n",
    "# NTM\n",
    "\n",
    "def init_state_ntm(batch_size, css, mas, mcs):\n",
    "    state_size = css + 2*mas + mas * mcs\n",
    "    \n",
    "    ra = [0.0]*mas\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,mas]) + ra\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    init_read_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_write_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_memory = tf.truncated_normal([batch_size, mas*mcs], 0.0, 1e-6, dtype=tf.float32)\n",
    "    \n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "#############\n",
    "# PATTERN NTM\n",
    "\n",
    "def init_state_pattern_ntm(batch_size, css, mas, mcs):\n",
    "    # mas and mcs are arrays of address sizes and content sizes for rings\n",
    "    state_size = css\n",
    "    \n",
    "    init_address = []\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        state_size = state_size + mas[i] * mcs[i] # for memory vector\n",
    "        state_size = state_size + 2 * mas[i] # for addresses (read and write)\n",
    "    \n",
    "        ra = [0.0]*mas[i]\n",
    "        ra[0] = 1.0\n",
    "        init_address.append(np.zeros([batch_size,mas[i]]) + ra)\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    \n",
    "    tensor_list = [init_controller_state]\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        init_read_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        init_write_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        tensor_list = tensor_list + [init_read_address,init_write_address]\n",
    "        \n",
    "    for i in range(len(mas)):\n",
    "        # The first ring is initialised to zero, the rest differently\n",
    "        if( i == 0 ):\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "        else:\n",
    "            # This initialisation has the result of biasing the output of rings 2 and 3 to be\n",
    "            # \"no rotation\" and biasing ring 4 to say \"use ring 2\"\n",
    "            ra = [0.0]*mcs[i] \n",
    "            ra[0] = memory_init_bias\n",
    "            ra = np.zeros([batch_size,mas[i],mcs[i]]) + ra\n",
    "            ra = tf.constant(ra,dtype=tf.float32,shape=[batch_size,mas[i],mcs[i]])\n",
    "            ra = tf.reshape(ra,[batch_size,mas[i]*mcs[i]])\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32) + ra\n",
    "            #init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "            \n",
    "        tensor_list = tensor_list + [init_memory]\n",
    "    \n",
    "    state = tf.concat(tensor_list,1)\n",
    "\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "######################\n",
    "# MULTIPLE PATTERN NTM\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_17/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_15/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_13/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_11/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_9/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_7/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_5/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_3/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_1/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "\n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "read_addresses2 = []\n",
    "read_addresses3 = []\n",
    "read_addresses4 = []\n",
    "write_addresses = []\n",
    "write_addresses2 = []\n",
    "write_addresses3 = []\n",
    "write_addresses4 = []\n",
    "interps = []\n",
    "rnn_outputs = []\n",
    "m2 = []\n",
    "m3 = []\n",
    "m4 = []\n",
    "    \n",
    "for i in range(N + N_out):\n",
    "    # Logging\n",
    "    if( use_model == 'ntm' ):\n",
    "        h0, curr_read, curr_write, _ = tf.split(state, [controller_state_size,ntm_memory_address_size,\n",
    "                                                        ntm_memory_address_size,-1], 1)\n",
    "\n",
    "    if( use_model == 'pattern_ntm' ):\n",
    "        mas = pattern_ntm_memory_address_sizes\n",
    "        mcs = pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],mas[0] * mcs[0],mas[1] * mcs[1]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        m1_state = ret[5]\n",
    "        m2_state = ret[6]\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm' ):\n",
    "        mas = mult_pattern_ntm_memory_address_sizes\n",
    "        mcs = mult_pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],                        \n",
    "                            mas[2],mas[2],mas[3],mas[3],mas[0] * mcs[0],mas[1] * mcs[1],\n",
    "                            mas[2] * mcs[2],mas[3] * mcs[3]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        curr_read3 = ret[5]\n",
    "        curr_write3 = ret[6]\n",
    "        curr_read4 = ret[7]\n",
    "        curr_write4 = ret[8]\n",
    "        m1_state = ret[9]\n",
    "        m2_state = ret[10]\n",
    "        m3_state = ret[11]\n",
    "        m4_state = ret[12]\n",
    "\n",
    "    #### RUN MODEL ####\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "    ###################\n",
    "    \n",
    "    # More logging\n",
    "    read_addresses.append(curr_read[0,:])\n",
    "    write_addresses.append(curr_write[0,:])\n",
    "    \n",
    "    if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses2.append(curr_read2[0,:])\n",
    "        write_addresses2.append(curr_write2[0,:])\n",
    "        m2_state = tf.reshape(m2_state, [-1,mas[1],mcs[1]])\n",
    "        m2.append(tf.nn.softmax(m2_state[0,:]))\n",
    "        \n",
    "        with tf.variable_scope(\"NTM\",reuse=True):\n",
    "            W_interp = tf.get_variable(\"W_interp\", [controller_state_size,1])\n",
    "            B_interp = tf.get_variable(\"B_interp\", [1])\n",
    "            interp = tf.sigmoid(tf.matmul(h0,W_interp) + B_interp)\n",
    "            interp_matrix = tf.concat([interp,tf.ones_like(interp,dtype=tf.float32) - interp],axis=1) # shape [-1,2]\n",
    "            interps.append(interp_matrix[0,:])\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses3.append(curr_read3[0,:])\n",
    "        write_addresses3.append(curr_write3[0,:])\n",
    "        read_addresses4.append(curr_read4[0,:])\n",
    "        write_addresses4.append(curr_write4[0,:])\n",
    "        m3_state = tf.reshape(m3_state, [-1,mult_pattern_ntm_memory_address_sizes[2],mult_pattern_ntm_memory_content_sizes[2]])\n",
    "        m3.append(tf.nn.softmax(m3_state[0,:]))\n",
    "        m4_state = tf.reshape(m4_state, [-1,mult_pattern_ntm_memory_address_sizes[3],mult_pattern_ntm_memory_content_sizes[3]])\n",
    "        m4_state = m4_state[0,:]\n",
    "        m4_state = tf.concat([tf.nn.softmax(m4_state),tf.zeros([mult_pattern_ntm_memory_address_sizes[3],1])],1)\n",
    "        m4.append(m4_state)\n",
    "\n",
    "    reuse = True\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# Note: prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "\n",
    "# Note: We allow the length of input sequences to vary between batches, which means\n",
    "# that the cross entropy needs to be masked to the relevant part of the output\n",
    "\n",
    "# Note: we use log_softmax to avoid precision issues with floats causing log(0) to create NaNs\n",
    "\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.log_softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * prediction[i]) for i in range(N + N_out)] # an array of numbers\n",
    "mask = [tf.sign(tf.reduce_max(tf.abs(targets[i]))) for i in range(N + N_out)]\n",
    "ce_mask = [ce[i] * mask[i] for i in range(N + N_out)]\n",
    "cross_entropy = -tf.add_n(ce_mask)\n",
    "cross_entropy /= tf.add_n(mask) # DEBUG do we really need this?\n",
    "# NOTE: here in creating the mask we are assuming that batches have the same sequence length\n",
    "                    \n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate,decay=0.9,momentum=0.9)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N + N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "errors_mask = [errors[i] * mask[i] for i in range(N + N_out)]\n",
    "mean_error = tf.add_n(errors_mask)\n",
    "mean_error /= tf.add_n(mask)\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1, mean error - 0.853483\n",
      "Epoch - 2, mean error - 0.820839\n",
      "\n",
      "It took 20 seconds to train.\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "###################\n",
    "# Note on sequences\n",
    "#\n",
    "# Our sequences are of varying length, in the alphabet {0,...,num_classes - 3}.\n",
    "# Each input sequence begins with an initial symbol and ends with a terminal symbol\n",
    "# (the value of which are num_classes - 2 and num_classes - 1 by default). Output\n",
    "# sequences do not have either an initial nor a terminal symbol.\n",
    "#\n",
    "# Both input and output sequences are written on a \"tape\" of length N + N_out.\n",
    "# Input sequences are aligned at the BEGINNING of the tape, and all remaining space\n",
    "# is filled with terminal symbols. Output sequences are aligned at the END OF THE \n",
    "# MATCHING INPUT, with all remaining space filled with zero vectors.\n",
    "#\n",
    "# Example: suppose N = N_out = 10, and num_classes = 10 so that init_symbol = 8\n",
    "# and term_symbol = 9. Then a sequence of length 8 (seq_length = 10 below) is\n",
    "#\n",
    "# a = [4, 4, 5, 6, 3, 3, 6, 7]\n",
    "#\n",
    "# which written on the tape is\n",
    "#\n",
    "# [8, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
    "#\n",
    "# If we are performing the copy task, so that the output sequence is also a, then\n",
    "# the output written on the tape will be (notice the alignment)\n",
    "#\n",
    "# [-, -, -, -, -, -, -, -, -, 4, 4, 5, 6, 3, 3, 6, 7, -, -, -]\n",
    "#\n",
    "# where - is a symbol whose encoding is the zero vector.\n",
    "\n",
    "def io_generator(max_symbol, input_length, total_length):\n",
    "    \"\"\"\n",
    "    Returns a one-hot encoded pair of input and output sequence, with terminal and initial symbols.\n",
    "    \n",
    "    max_symbol - generate sequences in 0,...,max_symbol\n",
    "    input_length - length of input sequences, without initial and terminal symbols\n",
    "    total_length - length of the buffer, so that the sequences are padded to this length\n",
    "    \"\"\"\n",
    "    a = generate_input_seq(max_symbol,input_length)\n",
    "    fa = func_to_learn(a)\n",
    "    a = [init_symbol] + a + [term_symbol]\n",
    "    a = a + [term_symbol for k in range(total_length-len(a))]\n",
    "    a_onehot = [one_hots[e] for e in a]\n",
    "    fa_onehot = [[0.0]*num_classes for k in range(input_length+1)] + \\\n",
    "                [one_hots[e] for e in fa] + \\\n",
    "                [[0.0]*num_classes for k in range(total_length-(input_length+1)-len(fa))]\n",
    "    return a, np.array(a_onehot), np.array(fa_onehot)\n",
    "\n",
    "error_means = []\n",
    "epoch_error_means = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences. Each\n",
    "        # batch has a fixed length of the sequences. Recall that all input seqs\n",
    "        # have an initial and terminal symbol, so if seq_length = 10 then there\n",
    "        # are eight positions for the \"content\" symbols\n",
    "        \n",
    "        # Our version of curriculum training says: spend the first half\n",
    "        # of the epochs ramping up to the full training set. Assuming that\n",
    "        # epoch > N we divide allocate each integer in [seq_length_min,N]\n",
    "        # an equal portion of the first half of the epochs.\n",
    "        if( use_curriculum == True ):\n",
    "            if( 2 * i > epoch ):\n",
    "                seq_length_max = N\n",
    "            else:\n",
    "                curriculum_band = int(epoch/(2*(N - seq_length_min)))\n",
    "                seq_length_max = min(seq_length_min + int(i/curriculum_band),N)\n",
    "        else:\n",
    "            seq_length_max = N\n",
    "            \n",
    "        seq_length = random.randint(seq_length_min,seq_length_max)\n",
    "        \n",
    "        for z in range(batch_size):\n",
    "            a, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=N+N_out)\n",
    "            \n",
    "            inp.append(a_onehot)\n",
    "            out.append(fa_onehot)\n",
    "            \n",
    "            # Record the first sequence in the last batch of the last epoch\n",
    "            if( i == epoch - 1 and j == no_of_batches - 1 and z == 0):\n",
    "                final_seq = a\n",
    "        \n",
    "        # An annoying thing here is that we cannot use a list as a key in a \n",
    "        # dictionary. The workaround we found on StackOverflow here:\n",
    "        # http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "        feed_dict = {}\n",
    "        \n",
    "        for d in range(N + N_out):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N + N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "\n",
    "        ##### Do gradient descent #####\n",
    "        mean_error_val,_ = sess.run([mean_error,minimize], feed_dict)\n",
    "        ###############################\n",
    "        \n",
    "        error_means.append(mean_error_val)\n",
    "    \n",
    "    epoch_error = np.mean(error_means[-no_of_batches:])\n",
    "    epoch_error_means.append(epoch_error)\n",
    "    \n",
    "    # Print the mean error of the final batch in the epoch\n",
    "    print(\"Epoch - \" + str(i+1) + \", mean error - \" + str(epoch_error))\n",
    "\n",
    "# For the final batch of the final epoch, we record the memory states as well\n",
    "seq_length_for_vis = seq_length - 2\n",
    "interps_val = sess.run(interps,feed_dict)\n",
    "m2_val, m3_val, m4_val = sess.run([m2,m3,m4],feed_dict)            \n",
    "r1_val, w1_val = sess.run([read_addresses,write_addresses],feed_dict)\n",
    "r2_val, w2_val = sess.run([read_addresses2,write_addresses2],feed_dict)\n",
    "r3_val, w3_val = sess.run([read_addresses3,write_addresses3],feed_dict)\n",
    "r4_val, w4_val = sess.run([read_addresses4,write_addresses4],feed_dict)\n",
    "errors_mask_val = sess.run(errors_mask,feed_dict)\n",
    "#errors_mask_0 = [a[0,:] for a in errors_mask_val]\n",
    "\n",
    "# Write out variables to disk\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took \" + str(int(time.time() - pre_train_time)) + \" seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length used for visualisations - 8\n",
      "\n",
      "Sequence used for visualisations is (Note: initialisation symbol is 8 and terminal symbol is 9)\n",
      "[8, 7, 4, 1, 1, 6, 3, 1, 3, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "\n",
      "Error probabilities for final batch\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.83999997, 0.75199997, 0.78799999, 0.75999999, 0.77600002, 0.77200001, 0.81199998, 0.84399998, 0.0]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAAKyCAYAAACDn6wVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a741fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 1 #\n",
    "###########################\n",
    "\n",
    "print(\"Sequence length used for visualisations - \" + str(seq_length_for_vis))\n",
    "print(\"\")\n",
    "print(\"Sequence used for visualisations is (Note: initialisation symbol is \" + str(init_symbol) + \" and terminal symbol is \" + str(term_symbol) + \")\")\n",
    "print(final_seq)\n",
    "print(\"\")\n",
    "print(\"Error probabilities for final batch\")\n",
    "print(errors_mask_val)\n",
    "print(\"\")\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 9, 13\n",
    "fig_num = 0\n",
    "\n",
    "# RING 1\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "plt.figure(fig_num)\n",
    "ax1 = plt.subplot(2,1,1)\n",
    "ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "ax1.imshow(np.stack(w1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax1.set_title('Write address (ring 1)')\n",
    "ax1.set_xlabel('position')\n",
    "ax1.set_ylabel('time')\n",
    "\n",
    "ax2.imshow(np.stack(r1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax2.set_title('Read address (ring 1)')\n",
    "ax2.set_xlabel('position')\n",
    "ax2.set_ylabel('time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 2 #\n",
    "###########################\n",
    "\n",
    "if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 2)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 2)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Assume that powers2_on_1 has three entries we can use as colour channels\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 2)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    max_xticks = 2\n",
    "    xloc = plt.MaxNLocator(max_xticks)\n",
    "\n",
    "    ax.imshow(np.stack(interps_val), cmap='bone', interpolation='nearest', aspect='auto')\n",
    "    ax.set_title('Interpolation')\n",
    "    ax.set_xlabel('direct vs indirect')\n",
    "    ax.set_ylabel('time')\n",
    "    ax.xaxis.set_major_locator(xloc)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# VISUALISATIONS - OTHER RINGS #\n",
    "################################\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 3)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 3)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 3)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 4)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 4)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax6 = plt.subplot(1,1,1)    \n",
    "    ax6.imshow(np.stack(m4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax6.set_title('Memory contents (ring 4)')\n",
    "    ax6.set_xlabel('position')\n",
    "    ax6.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3kAAAUKCAYAAABblriAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a802990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########################\n",
    "# VISUALISATIONS - ERROR #\n",
    "##########################\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "\n",
    "plt.figure(fig_num)\n",
    "ax = plt.subplot(1,1,1)\n",
    "sc = pandas.Series(error_means)\n",
    "ma = sc.rolling(window=500).mean()\n",
    "ax.plot(sc.index, sc, color='lightgray')\n",
    "ax.plot(ma.index, ma, color='red')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(sc.index.min(), sc.index.max())\n",
    "ax.set_title('Error')\n",
    "ax.set_xlabel('batch')\n",
    "ax.set_ylabel('error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on sequences of length 13\n",
      "\n",
      "Batch - 1, Mean error - 0.836\n",
      "Batch - 2, Mean error - 0.836308\n",
      "Batch - 3, Mean error - 0.841538\n",
      "Batch - 4, Mean error - 0.840615\n",
      "\n",
      "###########\n",
      "# Summary #\n",
      "###########\n",
      "\n",
      "model         - ntm\n",
      "task name     - mult pattern 3\n",
      "epochs        - 2\n",
      "num_classes   - 10\n",
      "N             - 10\n",
      "Ntest         - 15\n",
      "# weights     - 18958\n",
      "\n",
      "\n",
      "error train(test) - 0.820839 (0.838615)\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "# Restore the weights from training\n",
    "sess = tf.Session()\n",
    "saver.restore(sess,save_path)\n",
    "\n",
    "inputs_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "targets_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "# Set up test graph\n",
    "rnn_outputs_test = []\n",
    "reuse = True\n",
    "for i in range(Ntest + Ntest_out):\n",
    "    output, state = cell(inputs_test[i],state,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "\n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size])\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.log_softmax(logit) for logit in logits_test] \n",
    "mask = [tf.sign(tf.reduce_max(tf.abs(targets_test[i]))) for i in range(Ntest + Ntest_out)]\n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets_test[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest + Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "errors_test_mask = [errors_test[i] * mask[i] for i in range(Ntest + Ntest_out)]\n",
    "mean_error_test = tf.add_n(errors_test_mask)\n",
    "mean_error_test /= tf.add_n(mask)\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "\n",
    "seq_length = Ntest\n",
    "print(\"Testing on sequences of length \" + str(seq_length-2))\n",
    "print(\"\")\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    for z in range(batch_size):\n",
    "        a, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=Ntest+Ntest_out)\n",
    "            \n",
    "        inp.append(a_onehot)\n",
    "        out.append(fa_onehot)        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        in_node = inputs_test[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        out_node = targets_test[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = sess.run(mean_error_test, feed_dict)\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"epochs        - \" + str(epoch))\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error train(test) - \" + str(epoch_error_means[-1]) + \" (\" + str(final_error) + \")\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
