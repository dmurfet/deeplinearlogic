{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of linear logic recurrent neural network\n",
    "#\n",
    "# The architecture is a modified RNN, see the paper \"Linear logic and recurrent neural networks\".\n",
    "# Our inputs are sequences of symbols taken from an alphabet of size num_classes. The length\n",
    "# of the sequences is N. Our outputs are also sequences of length N from the same alphabet.\n",
    "#\n",
    "# Here \"symbol\" means a one hot vector.\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# GLOBAL FLAGS\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, pattern_ntm_alt\n",
    "task                  = 'copy' # copy, repeat copy, pattern\n",
    "epoch                 = 100 # number of training epochs, default to 200\n",
    "num_classes           = 258 # number of symbols, INCLUDING initial and terminal symbols\n",
    "N                     = 5 # length of input sequences for training, default to 20, INCLUDING initial and terminal symbols\n",
    "Ntest                 = 5 # length of sequences for testing, default to N, INCLUDING initial and terminal symbols\n",
    "batch_size            = 500 # default to 500 (too large does not fit on GPUs)\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "memory_address_size   = 128 # number of memory locations, default 20\n",
    "memory_content_size   = 20 # size of vector stored at a memory location, default 5\n",
    "powers_ring1          = [0,-1,1] # powers of R used on ring 1, default [0,-1,1]\n",
    "powers_ring2          = [0,-1,1] # powers of R used on ring 2, default [0,-1,1]\n",
    "model_optimizer       = 'rmsprop' # adam, rmsprop, default to rmsprop\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "\n",
    "training_percent      = 0.01 # percentage used for training, default 0.01\n",
    "num_training          = 10000 #int(training_percent * (num_classes-2)**N)\n",
    "num_test              = num_training\n",
    "\n",
    "init_symbol           = num_classes - 2\n",
    "term_symbol           = num_classes - 1\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there\n",
    "# 2. The initial and terminal symbols are always from the end of the list of symbols, so they\n",
    "# are respectively num_classes - 2 and num_classes - 1. So the number of symbols which are\n",
    "# not initial or terminal is num_classes - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[208, 60, 96, 155, 210]\n",
      "is mapped to\n",
      "[208, 60, 96, 155, 210]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "# Below N_out and Ntest_out are the lengths of the outputs in both the training\n",
    "# and testing regimes respectively. Since outputs do not include the initial and terminal\n",
    "# symbols, these default to N - 2 and Ntest - 2 respectively.\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N - 2\n",
    "    Ntest_out = Ntest - 2\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "if( task == 'repeat copy' ):\n",
    "    pattern = [0,1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 2 * (N - 2)\n",
    "    Ntest_out = 2 * (Ntest - 2)\n",
    "\n",
    "##############\n",
    "# PATTERN TASK\n",
    "if( task == 'pattern' ):\n",
    "    pattern = [1,0,0,2,0]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 2 * (N - 2)\n",
    "    Ntest_out = 2 * (Ntest - 2)\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = [random.randint(0,num_classes-3) for i in range(N)]\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# INITIALISE STATE #\n",
    "####################\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "input_size = num_classes # dimension of the input space I\n",
    "state_size = 0\n",
    "\n",
    "#####\n",
    "# NTM\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size = controller_state_size + 2*memory_address_size + memory_address_size * memory_content_size\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,memory_address_size,memory_content_size, powers_ring1)\n",
    "    \n",
    "    ra = [0.0]*memory_address_size\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,memory_address_size]) + ra\n",
    "    \n",
    "    # DEBUG at the moment the read and write addresses are not distributions, i.e. they do not\n",
    "    # sum to 1, but after one step the gamma sharpening will normalise them. We should probably start\n",
    "    # with things that sum to 1, though.\n",
    "    #init_controller_state = tf.truncated_normal([batch_size, controller_state_size], 0.0, 1e-6, dtype=tf.float32)\n",
    "    init_controller_state = tf.get_variable(\"init_ccs\", shape=[batch_size, controller_state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    init_read_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,memory_address_size]) + \\\n",
    "                       tf.random_uniform([batch_size, memory_address_size], 0.0, 1e-6)\n",
    "    \n",
    "    init_write_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,memory_address_size]) + \\\n",
    "                       tf.random_uniform([batch_size, memory_address_size], 0.0, 1e-6)\n",
    "    \n",
    "    #init_memory = tf.truncated_normal([batch_size, memory_address_size*memory_content_size], 0.0, 1e-6, dtype=tf.float32)\n",
    "    init_memory = tf.get_variable(\"init_mem\", shape=[batch_size, memory_address_size*memory_content_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "    \n",
    "#############\n",
    "# PATTERN NTM\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size = controller_state_size + 4*memory_address_size + \\\n",
    "                memory_address_size * memory_content_size + \\\n",
    "                memory_address_size * len(powers_ring1)\n",
    "\n",
    "    cell = ntm.PatternNTM(state_size,input_size,controller_state_size,\n",
    "                          memory_address_size,memory_content_size, powers_ring1, powers_ring2)\n",
    "    \n",
    "    state = tf.truncated_normal([batch_size, state_size], 0.0, 0.01, dtype=tf.float32)\n",
    "    \n",
    "#################\n",
    "# PATTERN NTM ALT\n",
    "if( use_model == 'pattern_ntm_alt' ):\n",
    "    state_size = controller_state_size + 4*memory_address_size + \\\n",
    "                memory_address_size * memory_content_size + \\\n",
    "                memory_address_size * len(powers_ring1)\n",
    "\n",
    "    cell = ntm.PatternNTM_alt(state_size,input_size,controller_state_size,\n",
    "                          memory_address_size,memory_content_size, powers_ring1, powers_ring2)\n",
    "    \n",
    "    state = tf.truncated_normal([batch_size, state_size], 0.0, 0.01, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_11/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "# inputs, we create N of them, each of shape [None,input_size], one for each position in the sequence\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N_out)]\n",
    "\n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "write_addresses = []\n",
    "gamma_writes = []\n",
    "gamma_reads = []\n",
    "ss = []\n",
    "\n",
    "for i in range(N):\n",
    "    # Store read and write addresses for later logging\n",
    "    h0, curr_read, curr_write, _ = tf.split(state, [controller_state_size,memory_address_size,memory_address_size,-1], 1)\n",
    "    read_addresses.append(curr_read[0,:])\n",
    "    write_addresses.append(curr_write[0,:])\n",
    "    \n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    \n",
    "    # DEBUG, getting gammas\n",
    "    with tf.variable_scope(\"NTM\",reuse=True):\n",
    "        W_gamma_write = tf.get_variable(\"W_gamma_write\", [controller_state_size,1])\n",
    "        B_gamma_write = tf.get_variable(\"B_gamma_write\", [])\n",
    "        gamma_write = 1.0 + tf.nn.relu(tf.matmul(h0,W_gamma_write) + B_gamma_write) # shape [batch_size,1]\n",
    "        \n",
    "        W_gamma_read = tf.get_variable(\"W_gamma_read\", [controller_state_size,1])\n",
    "        B_gamma_read = tf.get_variable(\"B_gamma_read\", [])\n",
    "        gamma_read = 1.0 + tf.nn.relu(tf.matmul(h0,W_gamma_read) + B_gamma_read) # shape [batch_size,1]\n",
    "        \n",
    "        W_s = tf.get_variable(\"W_s\", [controller_state_size,len(powers_ring1)])\n",
    "        B_s = tf.get_variable(\"B_s\", [len(powers_ring1)])\n",
    "        s = tf.nn.softmax(tf.matmul(h0,W_s) + B_s) # shape [batch_size,len(powers)]\n",
    "\n",
    "    gamma_writes.append(gamma_write[0,:])\n",
    "    gamma_reads.append(gamma_read[0,:])\n",
    "    ss.append(s[0,:])\n",
    "    reuse = True\n",
    "\n",
    "# We only start recording the outputs of the controller once we have\n",
    "# finished feeding in the input. We feed terminal symbols as input in the second phase.\n",
    "\n",
    "term_symbol_tensor = tf.constant(np.zeros([batch_size,input_size]) + one_hots[term_symbol],\n",
    "                                 dtype=tf.float32,\n",
    "                                 shape=[batch_size,input_size])\n",
    "\n",
    "rnn_outputs = []\n",
    "for i in range(N_out):\n",
    "    output, state = cell(term_symbol_tensor,state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * tf.log(prediction[i])) for i in range(N_out)]\n",
    "\n",
    "if( model_optimizer == 'adam' ):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "elif( model_optimizer == 'rmsprop' ):\n",
    "    optimizer = tf.train.RMSPropOptimizer(1e-4,decay=0.9,momentum=0.9)\n",
    "\n",
    "cross_entropy = -tf.add_n(ce)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "mean_error = tf.scalar_mul(np.true_divide(1,N_out), tf.add_n(errors))\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.05468929]\n",
      "Write address -\n",
      "[  1.00000012e+00   1.15296125e-07   7.47976401e-07   6.12190718e-07\n",
      "   6.41264421e-07   8.66121042e-08   7.83794974e-07   1.38548970e-07\n",
      "   4.65914724e-07   9.84654093e-07   7.81890890e-07   7.50882634e-07\n",
      "   5.25193457e-07   4.21482213e-07   5.16507612e-07   2.37697364e-08\n",
      "   1.19210128e-07   7.50960680e-07   8.96446124e-07   6.12471354e-07\n",
      "   8.49252558e-07   6.19022273e-07   9.36312915e-07   7.93675554e-07\n",
      "   2.18329191e-07   6.15960744e-07   6.57255555e-07   2.20934155e-07\n",
      "   7.91735673e-08   8.83671419e-07   4.25691724e-07   5.51305163e-07\n",
      "   5.59759712e-07   3.57143165e-07   8.45804209e-07   2.88852561e-07\n",
      "   2.76220334e-07   5.25542589e-07   1.83848854e-07   1.03063584e-08\n",
      "   7.42564055e-07   4.50999011e-07   2.01190474e-07   6.91551463e-07\n",
      "   3.42148184e-07   2.97436486e-07   3.13754441e-07   2.69365898e-07\n",
      "   6.12639440e-07   6.91537878e-07   7.62622847e-07   1.41897914e-07\n",
      "   2.06358678e-07   7.45736344e-08   4.35721404e-07   7.68687727e-08\n",
      "   2.35580330e-07   9.27874453e-07   3.07624930e-07   5.63294748e-07\n",
      "   6.24539837e-07   7.95610902e-07   8.14068187e-07   8.14363972e-08\n",
      "   3.62982519e-07   2.28574635e-07   2.83380160e-07   9.82606025e-07\n",
      "   7.65707512e-07   2.77171495e-07   3.82785089e-07   4.88308785e-07\n",
      "   9.91646630e-07   7.93233539e-07   7.14759949e-07   3.36982367e-07\n",
      "   7.77970456e-07   3.79195569e-07   6.68027155e-07   3.92806641e-07\n",
      "   4.30098282e-07   7.66222342e-07   8.74544384e-08   1.56848429e-07\n",
      "   5.28214002e-07   1.30150440e-07   8.39190363e-07   3.04553282e-07\n",
      "   3.32114581e-07   7.77335401e-07   5.93761683e-07   5.23916583e-07\n",
      "   1.23217816e-07   4.47564361e-07   2.88943397e-07   8.30635088e-07\n",
      "   1.25486608e-07   4.29820403e-07   9.65279924e-07   2.84221301e-07\n",
      "   8.66267555e-07   8.80393998e-08   7.71737461e-07   1.04459879e-07\n",
      "   9.07848118e-07   9.71169470e-07   4.82883081e-07   2.03986161e-07\n",
      "   4.84141594e-07   7.99628594e-07   3.19558495e-07   3.71587760e-07\n",
      "   9.48659419e-07   3.65268335e-07   6.67850983e-08   4.88202545e-07\n",
      "   6.86093188e-07   1.76482672e-07   8.50863955e-07   6.49434185e-07\n",
      "   2.20474476e-07   1.65779468e-07   9.13367728e-07   8.32608009e-07\n",
      "   6.43897025e-08   9.98810037e-07   6.67924667e-07   9.48165166e-07]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 1 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.04191685]\n",
      "Write gamma - [ 1.25293851]\n",
      "Write address -\n",
      "[  2.91360646e-01   3.72042209e-01   7.47651200e-07   8.57646114e-07\n",
      "   7.34620244e-07   7.78976869e-07   6.62271191e-07   7.58020178e-07\n",
      "   7.75141075e-07   8.85935378e-07   9.52174275e-07   8.65331344e-07\n",
      "   8.04504225e-07   7.60367243e-07   6.65929690e-07   6.24705763e-07\n",
      "   6.55726865e-07   8.00359544e-07   8.98180986e-07   9.25397899e-07\n",
      "   8.64111541e-07   9.32866442e-07   9.11113887e-07   8.47190051e-07\n",
      "   7.99762347e-07   7.55464328e-07   7.62164120e-07   6.76151387e-07\n",
      "   7.12321309e-07   7.28174257e-07   8.38838218e-07   7.68881534e-07\n",
      "   7.58555245e-07   8.16755232e-07   7.52712367e-07   7.59760837e-07\n",
      "   6.92232049e-07   6.67400911e-07   6.31990417e-07   6.68776067e-07\n",
      "   6.96894631e-07   7.51238929e-07   7.42521536e-07   7.07088702e-07\n",
      "   7.41334190e-07   6.68017321e-07   6.53773327e-07   7.11775499e-07\n",
      "   7.71296357e-07   8.65241077e-07   7.79540244e-07   7.07490756e-07\n",
      "   5.71586895e-07   6.28233749e-07   5.94479332e-07   6.37931691e-07\n",
      "   7.16321551e-07   7.44880310e-07   8.31134628e-07   7.59000500e-07\n",
      "   8.50730316e-07   8.93545405e-07   7.97458313e-07   7.36434572e-07\n",
      "   6.11598978e-07   6.55891142e-07   7.63873004e-07   8.46106843e-07\n",
      "   8.62722800e-07   7.61257979e-07   6.99951784e-07   8.28670750e-07\n",
      "   8.93607137e-07   9.48782656e-07   8.27639269e-07   8.32204080e-07\n",
      "   7.54338316e-07   8.31550949e-07   7.47855381e-07   7.68630457e-07\n",
      "   7.80569110e-07   7.18303397e-07   6.90735646e-07   6.34231696e-07\n",
      "   6.34629430e-07   7.73370346e-07   7.09442418e-07   7.71035218e-07\n",
      "   7.49554090e-07   7.92109233e-07   8.39250788e-07   7.19096818e-07\n",
      "   7.00710643e-07   6.43851195e-07   7.80841503e-07   7.07302206e-07\n",
      "   7.58748854e-07   7.63336800e-07   7.85201053e-07   8.90055787e-07\n",
      "   7.05219918e-07   8.21894105e-07   6.54545204e-07   8.30333761e-07\n",
      "   8.35826825e-07   9.17922591e-07   8.02306488e-07   7.11863777e-07\n",
      "   7.57562816e-07   7.76250431e-07   7.72673275e-07   7.89777971e-07\n",
      "   7.85653810e-07   7.54400162e-07   6.67778465e-07   7.10463155e-07\n",
      "   7.33285162e-07   8.14288910e-07   7.82803113e-07   8.07649883e-07\n",
      "   6.90108948e-07   7.31213788e-07   8.23780852e-07   8.20800267e-07\n",
      "   8.52689311e-07   7.87198360e-07   9.72942644e-07   3.36501569e-01]\n",
      "Argmax - 1\n",
      "\n",
      "\n",
      "Step 2 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[  3.73890340e-01   2.04289034e-01   9.94521081e-02   9.15825424e-08\n",
      "   9.24101400e-08   8.79993252e-08   8.88619525e-08   8.88389238e-08\n",
      "   9.38013685e-08   9.80190791e-08   9.95155673e-08   9.76097851e-08\n",
      "   9.35242710e-08   8.91212295e-08   8.53670130e-08   8.34088851e-08\n",
      "   8.66501537e-08   9.25290351e-08   9.81294335e-08   9.91901530e-08\n",
      "   1.00189986e-07   9.97982994e-08   9.91957521e-08   9.62834079e-08\n",
      "   9.29407236e-08   9.12495324e-08   8.84161224e-08   8.77030288e-08\n",
      "   8.70783268e-08   9.07864433e-08   9.16391869e-08   9.22237149e-08\n",
      "   9.20078875e-08   9.14189044e-08   9.14800182e-08   8.86994016e-08\n",
      "   8.68982113e-08   8.42112158e-08   8.38921679e-08   8.45838386e-08\n",
      "   8.71760193e-08   8.86155078e-08   8.86825475e-08   8.86227127e-08\n",
      "   8.68347385e-08   8.57222702e-08   8.53600781e-08   8.76525235e-08\n",
      "   9.22817236e-08   9.33047986e-08   9.17362399e-08   8.53062474e-08\n",
      "   8.25342283e-08   8.01616835e-08   8.16442380e-08   8.36894998e-08\n",
      "   8.67921983e-08   9.10166449e-08   9.15833667e-08   9.41067668e-08\n",
      "   9.55373807e-08   9.59337569e-08   9.33739770e-08   8.71968240e-08\n",
      "   8.45568309e-08   8.55190052e-08   9.05381299e-08   9.47911758e-08\n",
      "   9.43316039e-08   9.11352132e-08   9.09634394e-08   9.38882039e-08\n",
      "   9.91927323e-08   9.86770416e-08   9.74515046e-08   9.31707831e-08\n",
      "   9.35677349e-08   9.15134919e-08   9.18967800e-08   9.09090474e-08\n",
      "   9.00646882e-08   8.83955309e-08   8.52464197e-08   8.35887448e-08\n",
      "   8.57761151e-08   8.70125803e-08   9.00067576e-08   8.94333425e-08\n",
      "   9.12737548e-08   9.28374462e-08   9.17371423e-08   8.98334065e-08\n",
      "   8.56897557e-08   8.74696084e-08   8.72949570e-08   8.98183004e-08\n",
      "   8.94763161e-08   9.11350355e-08   9.42132914e-08   9.22837060e-08\n",
      "   9.35174000e-08   8.80794744e-08   9.13180429e-08   9.15975846e-08\n",
      "   9.72904317e-08   9.62425872e-08   9.33703674e-08   9.03143587e-08\n",
      "   8.98558241e-08   9.10675695e-08   9.17878822e-08   9.19597625e-08\n",
      "   9.14664895e-08   8.86669511e-08   8.73357848e-08   8.69959536e-08\n",
      "   9.02585882e-08   9.15859104e-08   9.31974071e-08   9.02134047e-08\n",
      "   8.93509267e-08   9.00352006e-08   9.26559593e-08   9.52559702e-08\n",
      "   9.42447400e-08   9.81075630e-08   1.09663889e-01   2.12693721e-01]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 3 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.05785024]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[  2.70976901e-01   2.23901957e-01   1.00743681e-01   3.07419952e-02\n",
      "   1.09063467e-06   1.08949291e-06   1.08844836e-06   1.09025825e-06\n",
      "   1.09344603e-06   1.09704240e-06   1.09831592e-06   1.09678024e-06\n",
      "   1.09326970e-06   1.08916834e-06   1.08577581e-06   1.08488871e-06\n",
      "   1.08734730e-06   1.09232303e-06   1.09658913e-06   1.09903385e-06\n",
      "   1.09961718e-06   1.09958978e-06   1.09833013e-06   1.09599750e-06\n",
      "   1.09330472e-06   1.09074608e-06   1.08892914e-06   1.08758854e-06\n",
      "   1.08829158e-06   1.08976690e-06   1.09141820e-06   1.09183532e-06\n",
      "   1.09175016e-06   1.09147993e-06   1.09045152e-06   1.08885581e-06\n",
      "   1.08647509e-06   1.08480288e-06   1.08406800e-06   1.08504150e-06\n",
      "   1.08668542e-06   1.08805170e-06   1.08850338e-06   1.08794222e-06\n",
      "   1.08690006e-06   1.08581355e-06   1.08604968e-06   1.08825202e-06\n",
      "   1.09103053e-06   1.09235759e-06   1.09007010e-06   1.08628751e-06\n",
      "   1.08251015e-06   1.08122003e-06   1.08168672e-06   1.08388849e-06\n",
      "   1.08701477e-06   1.08974803e-06   1.09205723e-06   1.09363373e-06\n",
      "   1.09507846e-06   1.09487007e-06   1.09209304e-06   1.08814095e-06\n",
      "   1.08553456e-06   1.08665188e-06   1.09017685e-06   1.09319228e-06\n",
      "   1.09333348e-06   1.09192945e-06   1.09179120e-06   1.09450264e-06\n",
      "   1.09725067e-06   1.09831217e-06   1.09635073e-06   1.09447774e-06\n",
      "   1.09266216e-06   1.09212829e-06   1.09132930e-06   1.09081020e-06\n",
      "   1.08966367e-06   1.08778693e-06   1.08556208e-06   1.08464610e-06\n",
      "   1.08534721e-06   1.08742722e-06   1.08876191e-06   1.09004634e-06\n",
      "   1.09105383e-06   1.09186976e-06   1.09134180e-06   1.08898610e-06\n",
      "   1.08738777e-06   1.08672532e-06   1.08799850e-06   1.08879146e-06\n",
      "   1.08996085e-06   1.09144491e-06   1.09251789e-06   1.09312577e-06\n",
      "   1.09129508e-06   1.09063342e-06   1.09026439e-06   1.09315135e-06\n",
      "   1.09506220e-06   1.09552752e-06   1.09316204e-06   1.09097562e-06\n",
      "   1.09023665e-06   1.09077826e-06   1.09147879e-06   1.09161215e-06\n",
      "   1.09060329e-06   1.08897621e-06   1.08750146e-06   1.08798179e-06\n",
      "   1.08952531e-06   1.09153950e-06   1.09162568e-06   1.09072607e-06\n",
      "   1.08969164e-06   1.09050336e-06   1.09251869e-06   1.09399548e-06\n",
      "   1.09562495e-06   3.42967324e-02   1.07974388e-01   2.31232330e-01]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 4 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[  2.40459278e-01   1.92098141e-01   1.13288559e-01   4.13087904e-02\n",
      "   9.76568181e-03   2.08919232e-06   2.08920005e-06   2.09062569e-06\n",
      "   2.09353061e-06   2.09611562e-06   2.09706059e-06   2.09566815e-06\n",
      "   2.09256132e-06   2.08891697e-06   2.08625011e-06   2.08583697e-06\n",
      "   2.08818756e-06   2.09209384e-06   2.09589371e-06   2.09821064e-06\n",
      "   2.09915333e-06   2.09885229e-06   2.09757695e-06   2.09544896e-06\n",
      "   2.09292193e-06   2.09060227e-06   2.08873053e-06   2.08801430e-06\n",
      "   2.08836127e-06   2.08965776e-06   2.09078485e-06   2.09140330e-06\n",
      "   2.09140740e-06   2.09090854e-06   2.08990559e-06   2.08819256e-06\n",
      "   2.08633014e-06   2.08478878e-06   2.08440451e-06   2.08508982e-06\n",
      "   2.08641518e-06   2.08752249e-06   2.08788038e-06   2.08745837e-06\n",
      "   2.08655206e-06   2.08598181e-06   2.08654387e-06   2.08833990e-06\n",
      "   2.09038421e-06   2.09080076e-06   2.08909432e-06   2.08578922e-06\n",
      "   2.08295410e-06   2.08154097e-06   2.08210804e-06   2.08410916e-06\n",
      "   2.08679216e-06   2.08948904e-06   2.09165455e-06   2.09341374e-06\n",
      "   2.09427276e-06   2.09361474e-06   2.09120822e-06   2.08814026e-06\n",
      "   2.08651977e-06   2.08736787e-06   2.08993424e-06   2.09202062e-06\n",
      "   2.09248856e-06   2.09205564e-06   2.09259679e-06   2.09441646e-06\n",
      "   2.09651262e-06   2.09696259e-06   2.09599511e-06   2.09411633e-06\n",
      "   2.09276891e-06   2.09172731e-06   2.09111909e-06   2.09027280e-06\n",
      "   2.08904885e-06   2.08727215e-06   2.08565461e-06   2.08493657e-06\n",
      "   2.08564734e-06   2.08700635e-06   2.08855840e-06   2.08975348e-06\n",
      "   2.09077621e-06   2.09114319e-06   2.09034852e-06   2.08886104e-06\n",
      "   2.08737742e-06   2.08715232e-06   2.08762845e-06   2.08871643e-06\n",
      "   2.08988536e-06   2.09111363e-06   2.09214022e-06   2.09197060e-06\n",
      "   2.09135828e-06   2.09043651e-06   2.09121004e-06   2.09269183e-06\n",
      "   2.09436416e-06   2.09421455e-06   2.09281643e-06   2.09112272e-06\n",
      "   2.09040968e-06   2.09060477e-06   2.09103973e-06   2.09091991e-06\n",
      "   2.09003906e-06   2.08866641e-06   2.08788538e-06   2.08814799e-06\n",
      "   2.08953225e-06   2.09066525e-06   2.09098994e-06   2.09035215e-06\n",
      "   2.09006112e-06   2.09074301e-06   2.09217160e-06   2.09387713e-06\n",
      "   1.30142486e-02   5.13548814e-02   1.31326362e-01   2.07135499e-01]\n",
      "Argmax - 0\n",
      "\n",
      "Epoch - 1, Mean error of final batch in epoch - 0.960667\n",
      "Epoch - 2, Mean error of final batch in epoch - 0.843333\n",
      "Epoch - 3, Mean error of final batch in epoch - 0.747333\n",
      "Epoch - 4, Mean error of final batch in epoch - 0.652\n",
      "Epoch - 5, Mean error of final batch in epoch - 0.564667\n",
      "Epoch - 6, Mean error of final batch in epoch - 0.520667\n",
      "Epoch - 7, Mean error of final batch in epoch - 0.46\n",
      "Epoch - 8, Mean error of final batch in epoch - 0.422667\n",
      "Epoch - 9, Mean error of final batch in epoch - 0.386667\n",
      "Epoch - 10, Mean error of final batch in epoch - 0.363333\n",
      "\n",
      "Step 0 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.19249272]\n",
      "Write gamma - [ 1.24286723]\n",
      "Write address -\n",
      "[  1.00000024e+00   5.83972906e-07   5.17241119e-07   8.57867974e-07\n",
      "   8.31849547e-07   3.57696763e-07   1.72020549e-07   6.58826480e-07\n",
      "   8.89648177e-08   7.46492049e-07   3.96148693e-07   6.43560895e-07\n",
      "   3.00449358e-07   5.77279081e-07   5.45941589e-07   6.58576027e-07\n",
      "   8.59793545e-07   6.93332652e-07   5.18637989e-07   6.96664358e-07\n",
      "   7.97435746e-07   3.96543868e-07   8.60919613e-07   6.64220693e-07\n",
      "   8.23626294e-07   9.30122837e-07   9.64231390e-07   7.22900495e-07\n",
      "   5.03113483e-07   3.35700037e-07   1.75212861e-07   7.69181156e-07\n",
      "   1.86624646e-07   1.12359047e-07   1.76316732e-07   3.66105326e-07\n",
      "   6.05575792e-07   9.59396289e-07   8.28995610e-07   7.61810156e-07\n",
      "   1.94756623e-07   1.53490546e-07   8.72891292e-07   7.27918518e-07\n",
      "   5.30331874e-07   8.25655832e-07   2.33630772e-07   1.48248546e-07\n",
      "   5.96957193e-07   1.26045578e-07   4.34404619e-08   7.16219802e-07\n",
      "   9.21461947e-07   9.44659689e-07   6.37396553e-08   7.31732371e-07\n",
      "   5.09910585e-08   4.24928544e-07   5.18099284e-07   3.53409661e-07\n",
      "   2.74189603e-07   1.13507506e-08   6.03029605e-07   1.20773905e-07\n",
      "   9.50363869e-07   7.87446595e-07   9.37705181e-07   7.38548408e-07\n",
      "   8.66616347e-07   7.79882043e-07   4.47216138e-07   9.54294592e-07\n",
      "   2.41910215e-07   2.27189304e-07   1.42744653e-07   7.34540720e-07\n",
      "   8.52394066e-08   7.77083017e-07   6.98886765e-07   8.56321662e-07\n",
      "   7.54203541e-07   2.33420366e-07   7.95707678e-08   9.75943294e-07\n",
      "   3.45212698e-07   8.79385595e-07   2.01468112e-07   8.38605132e-08\n",
      "   3.05681823e-07   7.77837158e-07   9.90506919e-07   5.32969693e-07\n",
      "   7.94963455e-07   5.35564652e-07   1.84775118e-07   1.91320186e-08\n",
      "   8.58994611e-07   6.00743192e-07   4.29614545e-07   8.13299152e-07\n",
      "   6.20683807e-07   9.96743211e-07   8.34192406e-07   8.96191565e-09\n",
      "   8.10089830e-07   5.58839417e-07   7.79991524e-07   5.79004904e-07\n",
      "   1.95923803e-07   5.73638204e-07   1.13249421e-07   3.54974389e-07\n",
      "   4.65076454e-07   8.28682687e-07   8.93120273e-07   5.09546169e-07\n",
      "   4.51415303e-07   3.07390934e-07   7.07127185e-07   7.27516863e-07\n",
      "   1.90734269e-07   5.93576431e-07   4.96748896e-07   3.58089807e-07\n",
      "   2.57151612e-07   9.55050723e-07   6.56618454e-07   6.81523545e-07]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 1 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.1423471]\n",
      "Write gamma - [ 1.92132854]\n",
      "Write address -\n",
      "[  3.91420484e-01   3.28279704e-01   8.40025294e-08   9.04119091e-08\n",
      "   8.81388189e-08   7.28106997e-08   6.75481147e-08   6.50065957e-08\n",
      "   7.32863796e-08   7.08213079e-08   8.05344271e-08   7.29481044e-08\n",
      "   7.49139275e-08   7.38511758e-08   8.08800991e-08   8.66689405e-08\n",
      "   9.08721276e-08   8.78010411e-08   8.33518428e-08   8.58590852e-08\n",
      "   8.46176746e-08   8.56258850e-08   8.48014494e-08   9.28346893e-08\n",
      "   9.45677527e-08   1.01326897e-07   9.98743843e-08   9.04549395e-08\n",
      "   7.69762281e-08   6.57026362e-08   6.91398725e-08   6.93583928e-08\n",
      "   6.64842617e-08   5.44320784e-08   5.76659005e-08   6.74539251e-08\n",
      "   8.34418543e-08   9.47552792e-08   9.78727286e-08   8.28030906e-08\n",
      "   6.72524081e-08   6.76330316e-08   8.11709100e-08   8.91435974e-08\n",
      "   8.67571615e-08   7.88220049e-08   6.93516924e-08   6.34259720e-08\n",
      "   6.37617390e-08   6.04622912e-08   6.09823871e-08   7.86711993e-08\n",
      "   9.84005766e-08   8.67556906e-08   7.83073659e-08   6.38546709e-08\n",
      "   6.81252672e-08   6.47548646e-08   7.15376629e-08   6.81661803e-08\n",
      "   5.84755853e-08   6.12094979e-08   6.10936084e-08   7.65823245e-08\n",
      "   8.34515745e-08   9.99990135e-08   9.64198961e-08   9.71457510e-08\n",
      "   9.43861238e-08   8.87270275e-08   8.82291644e-08   8.03269558e-08\n",
      "   7.35806225e-08   5.74993742e-08   6.55889281e-08   6.61011654e-08\n",
      "   7.53044560e-08   7.69304620e-08   9.25110299e-08   9.27949060e-08\n",
      "   8.39070466e-08   6.66862974e-08   6.85255870e-08   7.51165032e-08\n",
      "   8.84962503e-08   7.56889946e-08   6.85424411e-08   5.62903288e-08\n",
      "   6.72451108e-08   8.69278196e-08   9.36253670e-08   9.18315379e-08\n",
      "   8.36901037e-08   7.64919434e-08   6.01640764e-08   6.40859739e-08\n",
      "   7.57307461e-08   8.38786178e-08   8.14165233e-08   8.35373299e-08\n",
      "   9.40076887e-08   9.61064188e-08   8.46412931e-08   7.61455468e-08\n",
      "   7.36069623e-08   8.83419062e-08   8.46756123e-08   7.74193296e-08\n",
      "   7.10664239e-08   6.39518234e-08   6.51532019e-08   6.34586712e-08\n",
      "   7.74386635e-08   8.97337316e-08   9.18689622e-08   8.27929583e-08\n",
      "   7.08717067e-08   7.34820844e-08   8.03904712e-08   7.93612003e-08\n",
      "   7.43370947e-08   7.11098735e-08   7.45677156e-08   6.75420821e-08\n",
      "   7.48586686e-08   8.40096277e-08   9.19833667e-08   2.80290067e-01]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 2 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.08831275]\n",
      "Write gamma - [ 2.13157701]\n",
      "Write address -\n",
      "[  4.63472724e-01   2.61282444e-01   1.74681768e-02   1.18494693e-11\n",
      "   1.17732699e-11   1.15575414e-11   1.14216388e-11   1.14108983e-11\n",
      "   1.14854177e-11   1.15372920e-11   1.16043616e-11   1.15642331e-11\n",
      "   1.15513727e-11   1.15811397e-11   1.16878842e-11   1.18057205e-11\n",
      "   1.18642127e-11   1.18230373e-11   1.17700728e-11   1.17754625e-11\n",
      "   1.17759743e-11   1.17787811e-11   1.18126776e-11   1.19133184e-11\n",
      "   1.20005793e-11   1.20814296e-11   1.20451305e-11   1.18600312e-11\n",
      "   1.16068752e-11   1.14369278e-11   1.14326578e-11   1.14351358e-11\n",
      "   1.13441834e-11   1.12084517e-11   1.12447517e-11   1.14488592e-11\n",
      "   1.17317094e-11   1.19469686e-11   1.19561097e-11   1.17133378e-11\n",
      "   1.14712450e-11   1.14786358e-11   1.16781029e-11   1.18159952e-11\n",
      "   1.17812825e-11   1.16313990e-11   1.14592858e-11   1.13536533e-11\n",
      "   1.13178243e-11   1.12833692e-11   1.13615437e-11   1.16668715e-11\n",
      "   1.19157548e-11   1.18166310e-11   1.15984800e-11   1.14158041e-11\n",
      "   1.13910756e-11   1.14018161e-11   1.14510606e-11   1.13920852e-11\n",
      "   1.12790619e-11   1.12713953e-11   1.13557445e-11   1.15696975e-11\n",
      "   1.17949175e-11   1.20024043e-11   1.20315520e-11   1.20117414e-11\n",
      "   1.19537149e-11   1.18725099e-11   1.18044298e-11   1.16758174e-11\n",
      "   1.14860387e-11   1.13105506e-11   1.13424955e-11   1.14255541e-11\n",
      "   1.15432152e-11   1.16758313e-11   1.18683648e-11   1.18918408e-11\n",
      "   1.17028869e-11   1.14734168e-11   1.14573854e-11   1.16072568e-11\n",
      "   1.17291420e-11   1.15976517e-11   1.14026080e-11   1.12842964e-11\n",
      "   1.14577861e-11   1.17665443e-11   1.19179501e-11   1.18838880e-11\n",
      "   1.17445142e-11   1.15467254e-11   1.13459580e-11   1.13822675e-11\n",
      "   1.15739172e-11   1.17051941e-11   1.17197562e-11   1.17862334e-11\n",
      "   1.19297046e-11   1.19410714e-11   1.17715421e-11   1.16120976e-11\n",
      "   1.16197043e-11   1.17652945e-11   1.17464727e-11   1.16148107e-11\n",
      "   1.14762236e-11   1.13733979e-11   1.13505854e-11   1.14033080e-11\n",
      "   1.16200339e-11   1.18328914e-11   1.18639196e-11   1.17064795e-11\n",
      "   1.15420408e-11   1.15580228e-11   1.16448301e-11   1.16362345e-11\n",
      "   1.15572361e-11   1.15158820e-11   1.15091547e-11   1.14764621e-11\n",
      "   1.15783320e-11   1.17550570e-11   1.83739457e-02   2.39402726e-01]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 3 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.37871933]\n",
      "Write gamma - [ 3.18045163]\n",
      "Write address -\n",
      "[  5.84215939e-01   1.91460773e-01   3.94958444e-03   5.29931003e-06\n",
      "   6.42005438e-13   6.42005329e-13   6.42005166e-13   6.42005004e-13\n",
      "   6.42005166e-13   6.42005329e-13   6.42005329e-13   6.42005329e-13\n",
      "   6.42005329e-13   6.42005329e-13   6.42005438e-13   6.42005600e-13\n",
      "   6.42005600e-13   6.42005600e-13   6.42005600e-13   6.42005600e-13\n",
      "   6.42005600e-13   6.42005600e-13   6.42005600e-13   6.42005709e-13\n",
      "   6.42005871e-13   6.42005871e-13   6.42005871e-13   6.42005600e-13\n",
      "   6.42005329e-13   6.42005166e-13   6.42005166e-13   6.42005004e-13\n",
      "   6.42005004e-13   6.42004841e-13   6.42004841e-13   6.42005166e-13\n",
      "   6.42005438e-13   6.42005709e-13   6.42005709e-13   6.42005438e-13\n",
      "   6.42005166e-13   6.42005166e-13   6.42005438e-13   6.42005600e-13\n",
      "   6.42005438e-13   6.42005329e-13   6.42005166e-13   6.42005004e-13\n",
      "   6.42005004e-13   6.42004841e-13   6.42005004e-13   6.42005438e-13\n",
      "   6.42005600e-13   6.42005600e-13   6.42005329e-13   6.42005166e-13\n",
      "   6.42005004e-13   6.42005004e-13   6.42005166e-13   6.42005004e-13\n",
      "   6.42004841e-13   6.42004841e-13   6.42005004e-13   6.42005329e-13\n",
      "   6.42005600e-13   6.42005709e-13   6.42005871e-13   6.42005871e-13\n",
      "   6.42005709e-13   6.42005600e-13   6.42005600e-13   6.42005438e-13\n",
      "   6.42005166e-13   6.42005004e-13   6.42005004e-13   6.42005166e-13\n",
      "   6.42005329e-13   6.42005438e-13   6.42005600e-13   6.42005600e-13\n",
      "   6.42005438e-13   6.42005166e-13   6.42005166e-13   6.42005329e-13\n",
      "   6.42005438e-13   6.42005329e-13   6.42005004e-13   6.42005004e-13\n",
      "   6.42005166e-13   6.42005438e-13   6.42005709e-13   6.42005600e-13\n",
      "   6.42005438e-13   6.42005166e-13   6.42005004e-13   6.42005004e-13\n",
      "   6.42005329e-13   6.42005438e-13   6.42005438e-13   6.42005600e-13\n",
      "   6.42005709e-13   6.42005709e-13   6.42005438e-13   6.42005329e-13\n",
      "   6.42005329e-13   6.42005438e-13   6.42005438e-13   6.42005329e-13\n",
      "   6.42005166e-13   6.42005004e-13   6.42005004e-13   6.42005166e-13\n",
      "   6.42005329e-13   6.42005600e-13   6.42005600e-13   6.42005438e-13\n",
      "   6.42005329e-13   6.42005329e-13   6.42005329e-13   6.42005329e-13\n",
      "   6.42005329e-13   6.42005166e-13   6.42005166e-13   6.42005166e-13\n",
      "   6.42005329e-13   1.67468897e-05   7.48059247e-03   2.12871149e-01]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 4 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.88751209]\n",
      "Write gamma - [ 2.47596741]\n",
      "Write address -\n",
      "[  8.93658936e-01   3.44896987e-02   4.91193077e-06   8.00892599e-12\n",
      "   1.07911201e-18   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   5.56454169e-19   5.56454169e-19   5.56454169e-19   5.56454169e-19\n",
      "   1.19172035e-17   7.40675521e-10   6.93125912e-05   7.17771426e-02]\n",
      "Argmax - 0\n",
      "\n",
      "Epoch - 11, Mean error of final batch in epoch - 0.287333\n",
      "Epoch - 12, Mean error of final batch in epoch - 0.256\n",
      "Epoch - 13, Mean error of final batch in epoch - 0.206\n",
      "Epoch - 14, Mean error of final batch in epoch - 0.162\n",
      "Epoch - 15, Mean error of final batch in epoch - 0.125333\n",
      "Epoch - 16, Mean error of final batch in epoch - 0.102667\n",
      "Epoch - 17, Mean error of final batch in epoch - 0.0746667\n",
      "Epoch - 18, Mean error of final batch in epoch - 0.0766667\n",
      "Epoch - 19, Mean error of final batch in epoch - 0.044\n",
      "Epoch - 20, Mean error of final batch in epoch - 0.0506667\n",
      "\n",
      "Step 0 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.28653538]\n",
      "Write gamma - [ 1.13318086]\n",
      "Write address -\n",
      "[  1.00000036e+00   2.20427509e-07   7.79083109e-07   9.52990035e-07\n",
      "   2.15106127e-07   8.14080352e-07   6.78301831e-07   9.40037125e-07\n",
      "   8.81205438e-07   1.24242064e-07   2.81633135e-08   4.48758726e-07\n",
      "   8.03685282e-07   1.16109611e-07   6.14777321e-07   8.47774970e-07\n",
      "   5.95600511e-07   6.94541598e-07   2.02741628e-08   4.93313451e-07\n",
      "   4.09212113e-08   8.50738047e-07   5.22922619e-07   3.91695863e-07\n",
      "   6.34936328e-07   7.92831997e-07   4.66848121e-07   6.10703466e-07\n",
      "   5.57481542e-07   6.60568332e-07   4.88007402e-07   2.79649129e-07\n",
      "   8.58886381e-07   3.95339498e-07   3.97840978e-07   6.86137582e-07\n",
      "   2.34664199e-07   1.96478013e-07   4.50313081e-07   5.37387621e-07\n",
      "   1.01755496e-07   6.32568003e-07   6.33227842e-07   4.65159900e-07\n",
      "   3.72314446e-07   1.85089117e-08   5.99468933e-07   3.05483923e-07\n",
      "   3.34643971e-07   6.33110403e-07   1.68766377e-07   4.25419806e-08\n",
      "   7.71708358e-07   2.45748055e-07   2.05334061e-07   9.78059575e-07\n",
      "   3.31049932e-07   1.94447637e-07   4.62458615e-07   7.37733728e-07\n",
      "   3.78436908e-07   7.23253720e-07   2.94526217e-07   5.49348840e-07\n",
      "   4.90893342e-07   3.60103712e-07   9.85330985e-07   9.69322173e-07\n",
      "   5.59380169e-07   5.15922309e-08   9.82512205e-08   5.83937776e-07\n",
      "   4.40018169e-07   4.60602052e-07   4.97150154e-07   3.11049348e-07\n",
      "   4.63841076e-07   6.09380834e-07   4.09337275e-07   5.33755895e-07\n",
      "   4.50449591e-07   5.02419255e-07   7.30605734e-07   9.29814917e-07\n",
      "   9.83260520e-07   6.43259284e-07   9.02696229e-07   1.91273813e-07\n",
      "   7.22396351e-07   3.09888115e-08   5.31790256e-08   2.91275981e-09\n",
      "   4.06489022e-07   5.45185571e-07   7.90176728e-07   9.29985902e-07\n",
      "   9.75604962e-07   2.27269524e-07   6.94525454e-07   1.48740654e-07\n",
      "   1.31168491e-07   5.70370787e-07   9.11728762e-07   1.29903313e-07\n",
      "   1.88066011e-07   6.71185489e-07   4.96151927e-07   4.19629458e-07\n",
      "   1.89377658e-07   7.38348717e-07   5.37752157e-07   2.59729973e-07\n",
      "   7.75152444e-07   7.85493626e-07   1.72485120e-08   8.93801442e-08\n",
      "   8.90316585e-07   6.87328679e-07   7.83417079e-07   1.41033524e-07\n",
      "   3.69642606e-07   5.12518739e-07   4.75532289e-07   4.24587739e-07\n",
      "   2.60215046e-07   8.19842228e-07   1.04798318e-07   8.42036229e-07]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 1 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.7568121]\n",
      "Write address -\n",
      "[  3.71898055e-01   3.41838479e-01   3.21649310e-07   3.30392680e-07\n",
      "   3.22125260e-07   3.06339359e-07   3.57823552e-07   3.65193586e-07\n",
      "   3.30844586e-07   2.59079656e-07   2.22375732e-07   2.70908146e-07\n",
      "   2.86945578e-07   2.89985849e-07   2.93644376e-07   3.34021024e-07\n",
      "   3.37522636e-07   2.83008319e-07   2.66412769e-07   2.25893857e-07\n",
      "   2.75822003e-07   2.86247058e-07   3.12147847e-07   2.92484458e-07\n",
      "   3.12493995e-07   3.22686930e-07   3.17361895e-07   3.01019156e-07\n",
      "   3.14194210e-07   3.07418816e-07   2.87812099e-07   2.95174829e-07\n",
      "   2.96622773e-07   3.02792273e-07   2.86703084e-07   2.81252483e-07\n",
      "   2.64096428e-07   2.43772035e-07   2.66625051e-07   2.64784660e-07\n",
      "   2.70100315e-07   2.80280062e-07   3.09388298e-07   2.89854341e-07\n",
      "   2.47498747e-07   2.49171194e-07   2.50692267e-07   2.72093843e-07\n",
      "   2.71544224e-07   2.68102809e-07   2.45237715e-07   2.46935343e-07\n",
      "   2.62395815e-07   2.71895658e-07   2.78683359e-07   2.96554731e-07\n",
      "   2.93474670e-07   2.51462410e-07   2.80190420e-07   2.99467388e-07\n",
      "   3.13268430e-07   2.86614721e-07   2.94171031e-07   2.78895186e-07\n",
      "   2.84870595e-07   3.10056066e-07   3.50292339e-07   3.70079078e-07\n",
      "   3.02122913e-07   2.34079906e-07   2.30993948e-07   2.63842708e-07\n",
      "   2.89787693e-07   2.83018153e-07   2.75622767e-07   2.73145162e-07\n",
      "   2.80813680e-07   2.91174786e-07   2.93906282e-07   2.83630271e-07\n",
      "   2.89548552e-07   3.02145992e-07   3.37616115e-07   3.75103468e-07\n",
      "   3.72486227e-07   3.65473795e-07   3.14435539e-07   3.10443795e-07\n",
      "   2.55890058e-07   2.41565829e-07   1.90172941e-07   2.12642078e-07\n",
      "   2.49327542e-07   3.06141601e-07   3.45794120e-07   3.79214583e-07\n",
      "   3.44551040e-07   3.17086432e-07   2.63776769e-07   2.53746975e-07\n",
      "   2.39839409e-07   2.95288118e-07   3.05705129e-07   2.71930418e-07\n",
      "   2.49285534e-07   2.81079593e-07   2.98242838e-07   2.64332812e-07\n",
      "   2.75088183e-07   2.89245918e-07   2.96384485e-07   2.91926256e-07\n",
      "   3.13461697e-07   3.03631765e-07   2.47384435e-07   2.46960610e-07\n",
      "   3.03635147e-07   3.54144248e-07   3.04819963e-07   2.74825936e-07\n",
      "   2.54522206e-07   2.80559391e-07   2.85001818e-07   2.67691007e-07\n",
      "   2.86352247e-07   2.73455413e-07   3.04882946e-07   2.86227375e-01]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 2 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.89179516]\n",
      "Write gamma - [ 2.61593032]\n",
      "Write address -\n",
      "[  4.35643911e-01   2.65343308e-01   4.37901765e-02   1.36494496e-10\n",
      "   1.35459755e-10   1.36281958e-10   1.40163756e-10   1.41612153e-10\n",
      "   1.35551140e-10   1.26290242e-10   1.22304153e-10   1.25202612e-10\n",
      "   1.28930297e-10   1.30117001e-10   1.32516387e-10   1.36188644e-10\n",
      "   1.35763081e-10   1.30652156e-10   1.24794633e-10   1.23155292e-10\n",
      "   1.25727581e-10   1.30174913e-10   1.31832004e-10   1.32441336e-10\n",
      "   1.33630176e-10   1.35177758e-10   1.34432424e-10   1.33474856e-10\n",
      "   1.33437053e-10   1.32561961e-10   1.30993119e-10   1.30724001e-10\n",
      "   1.31493219e-10   1.31286301e-10   1.30009337e-10   1.27978600e-10\n",
      "   1.25375182e-10   1.24032992e-10   1.24814187e-10   1.25993424e-10\n",
      "   1.26817862e-10   1.29285277e-10   1.31193237e-10   1.28954139e-10\n",
      "   1.24710173e-10   1.22935370e-10   1.24143792e-10   1.25897168e-10\n",
      "   1.26698013e-10   1.25308736e-10   1.23407229e-10   1.23203919e-10\n",
      "   1.24962193e-10   1.26773064e-10   1.28624833e-10   1.30247743e-10\n",
      "   1.28827046e-10   1.26671687e-10   1.27916483e-10   1.31514202e-10\n",
      "   1.32273872e-10   1.31127165e-10   1.29729796e-10   1.29139838e-10\n",
      "   1.30108979e-10   1.34410205e-10   1.39958975e-10   1.40216658e-10\n",
      "   1.32205635e-10   1.23357741e-10   1.21492594e-10   1.25177924e-10\n",
      "   1.28489677e-10   1.28820413e-10   1.27783492e-10   1.27600638e-10\n",
      "   1.28598077e-10   1.29933481e-10   1.30154179e-10   1.29735972e-10\n",
      "   1.30333397e-10   1.33367067e-10   1.38759490e-10   1.43534948e-10\n",
      "   1.44830051e-10   1.41540904e-10   1.36732750e-10   1.31271619e-10\n",
      "   1.25986985e-10   1.19891333e-10   1.16272984e-10   1.17361482e-10\n",
      "   1.23935612e-10   1.32155134e-10   1.39843997e-10   1.42885398e-10\n",
      "   1.40247605e-10   1.33647482e-10   1.27514763e-10   1.23546132e-10\n",
      "   1.24591129e-10   1.28882738e-10   1.30744873e-10   1.27419048e-10\n",
      "   1.25524813e-10   1.27826333e-10   1.29095151e-10   1.27688221e-10\n",
      "   1.27625910e-10   1.29623381e-10   1.30663994e-10   1.31685232e-10\n",
      "   1.32754141e-10   1.30259553e-10   1.25242733e-10   1.25256430e-10\n",
      "   1.32242023e-10   1.36733111e-10   1.33642472e-10   1.27860819e-10\n",
      "   1.26055361e-10   1.27398994e-10   1.28161842e-10   1.27870617e-10\n",
      "   1.27936703e-10   1.29289732e-10   3.26396413e-02   2.22582951e-01]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 3 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 3.82373405]\n",
      "Write gamma - [ 4.97412777]\n",
      "Write address -\n",
      "[  6.62852645e-01   2.09882990e-01   3.51455854e-03   1.32007608e-06\n",
      "   1.32431886e-15   1.32432193e-15   1.32433421e-15   1.32433887e-15\n",
      "   1.32431928e-15   1.32428847e-15   1.32427429e-15   1.32428307e-15\n",
      "   1.32429525e-15   1.32430033e-15   1.32430827e-15   1.32432045e-15\n",
      "   1.32431928e-15   1.32430234e-15   1.32428254e-15   1.32427672e-15\n",
      "   1.32428498e-15   1.32429959e-15   1.32430594e-15   1.32430827e-15\n",
      "   1.32431219e-15   1.32431727e-15   1.32431526e-15   1.32431219e-15\n",
      "   1.32431134e-15   1.32430859e-15   1.32430351e-15   1.32430234e-15\n",
      "   1.32430467e-15   1.32430436e-15   1.32430001e-15   1.32429281e-15\n",
      "   1.32428424e-15   1.32427947e-15   1.32428180e-15   1.32428583e-15\n",
      "   1.32428890e-15   1.32429684e-15   1.32430308e-15   1.32429599e-15\n",
      "   1.32428223e-15   1.32427587e-15   1.32427947e-15   1.32428540e-15\n",
      "   1.32428816e-15   1.32428339e-15   1.32427714e-15   1.32427630e-15\n",
      "   1.32428223e-15   1.32428847e-15   1.32429483e-15   1.32430001e-15\n",
      "   1.32429567e-15   1.32428890e-15   1.32429281e-15   1.32430436e-15\n",
      "   1.32430743e-15   1.32430393e-15   1.32429917e-15   1.32429716e-15\n",
      "   1.32430033e-15   1.32431494e-15   1.32433305e-15   1.32433421e-15\n",
      "   1.32430827e-15   1.32427831e-15   1.32427122e-15   1.32428254e-15\n",
      "   1.32429366e-15   1.32429567e-15   1.32429250e-15   1.32429176e-15\n",
      "   1.32429483e-15   1.32429917e-15   1.32430033e-15   1.32429917e-15\n",
      "   1.32430107e-15   1.32431134e-15   1.32432955e-15   1.32434565e-15\n",
      "   1.32435041e-15   1.32433972e-15   1.32432363e-15   1.32430467e-15\n",
      "   1.32428614e-15   1.32426571e-15   1.32425311e-15   1.32425660e-15\n",
      "   1.32427831e-15   1.32430626e-15   1.32433262e-15   1.32434332e-15\n",
      "   1.32433506e-15   1.32431293e-15   1.32429207e-15   1.32427831e-15\n",
      "   1.32428149e-15   1.32429525e-15   1.32430150e-15   1.32429133e-15\n",
      "   1.32428498e-15   1.32429176e-15   1.32429599e-15   1.32429207e-15\n",
      "   1.32429176e-15   1.32429800e-15   1.32430192e-15   1.32430541e-15\n",
      "   1.32430859e-15   1.32430076e-15   1.32428424e-15   1.32428424e-15\n",
      "   1.32430668e-15   1.32432161e-15   1.32431219e-15   1.32429324e-15\n",
      "   1.32428657e-15   1.32429049e-15   1.32429324e-15   1.32429250e-15\n",
      "   1.32429281e-15   1.40348860e-07   1.21268537e-03   1.22535639e-01]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 4 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 5.01194286]\n",
      "Write gamma - [ 4.2196064]\n",
      "Write address -\n",
      "[  9.96440649e-01   3.32416850e-03   7.02740557e-12   1.87581263e-25\n",
      "   1.12447579e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11453468e-29   1.11453468e-29   1.11453468e-29   1.11453468e-29\n",
      "   1.11597303e-29   4.77140092e-27   5.64377646e-14   2.35200059e-04]\n",
      "Argmax - 0\n",
      "\n",
      "Epoch - 21, Mean error of final batch in epoch - 0.0313333\n",
      "Epoch - 22, Mean error of final batch in epoch - 0.0226667\n",
      "Epoch - 23, Mean error of final batch in epoch - 0.0133333\n",
      "Epoch - 24, Mean error of final batch in epoch - 0.0113333\n",
      "Epoch - 25, Mean error of final batch in epoch - 0.0133333\n",
      "Epoch - 26, Mean error of final batch in epoch - 0.00866667\n",
      "Epoch - 27, Mean error of final batch in epoch - 0.00466667\n",
      "Epoch - 28, Mean error of final batch in epoch - 0.00266667\n",
      "Epoch - 29, Mean error of final batch in epoch - 0.002\n",
      "Epoch - 30, Mean error of final batch in epoch - 0.00266667\n",
      "\n",
      "Step 0 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.34963989]\n",
      "Write gamma - [ 1.13140619]\n",
      "Write address -\n",
      "[  1.00000000e+00   5.08123662e-07   3.42692488e-07   1.04640840e-07\n",
      "   6.01752163e-07   8.69259111e-07   2.97691116e-07   3.88850935e-07\n",
      "   5.26379210e-07   2.43655933e-07   2.96606288e-07   2.82561416e-07\n",
      "   3.93994327e-07   7.63139610e-07   3.96729945e-07   2.27284673e-07\n",
      "   3.52361326e-07   1.76846982e-07   1.36326307e-07   1.16158603e-07\n",
      "   1.25601048e-07   4.71307516e-07   3.91876682e-08   3.74387611e-07\n",
      "   3.23634396e-07   3.86977206e-09   3.33959946e-07   7.20951547e-08\n",
      "   9.75292437e-07   8.68135828e-07   2.57559179e-07   3.32790620e-07\n",
      "   9.13801898e-07   4.90202638e-07   2.68403170e-07   6.29439114e-07\n",
      "   2.65891543e-07   8.28703548e-07   5.78093193e-07   8.44572298e-07\n",
      "   1.40498287e-07   8.65409390e-07   6.23887786e-07   4.24041872e-07\n",
      "   1.75390245e-08   5.04990567e-07   9.07389392e-07   5.08315907e-07\n",
      "   2.52159111e-07   2.58405095e-07   4.41252467e-07   6.14691999e-07\n",
      "   4.38585516e-07   9.51461175e-07   4.06126134e-07   6.61083490e-08\n",
      "   7.89838452e-07   3.37464087e-07   3.40662353e-07   8.31705904e-07\n",
      "   2.97384616e-07   8.49591345e-07   8.23511982e-07   5.90832371e-07\n",
      "   5.38544668e-07   5.48093908e-07   1.94334859e-07   7.83073403e-07\n",
      "   3.51249582e-07   2.34538675e-07   2.71007053e-07   9.82156735e-07\n",
      "   5.66388621e-07   1.02609512e-07   2.95441147e-08   6.89732303e-07\n",
      "   1.13734009e-08   2.63056759e-08   2.94529684e-07   5.91338278e-07\n",
      "   1.12298487e-07   1.39257665e-07   7.69203893e-07   2.37886425e-07\n",
      "   3.13148973e-07   8.68333075e-07   1.74141647e-07   5.75904721e-07\n",
      "   4.51321611e-08   1.39222621e-07   6.46891237e-07   5.59239375e-08\n",
      "   9.41770040e-08   1.48383862e-07   9.59370823e-07   2.58343221e-07\n",
      "   2.39667894e-08   7.78407355e-07   7.97223777e-07   9.37843197e-07\n",
      "   2.95537944e-07   6.72727083e-07   5.40208816e-07   6.07662798e-07\n",
      "   6.88422574e-07   9.32971375e-07   8.35970866e-07   5.65010282e-07\n",
      "   4.22939536e-07   9.63499019e-07   4.45119611e-07   3.69243395e-07\n",
      "   2.24167351e-07   9.01158899e-07   3.66067297e-07   2.72175186e-07\n",
      "   9.48233634e-08   1.80162431e-07   5.04473917e-07   4.72058900e-07\n",
      "   5.00276087e-08   4.93949983e-07   3.44891305e-07   4.12569278e-07\n",
      "   1.50964738e-07   2.86357647e-07   8.43023770e-07   7.25633981e-07]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 1 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.91164088]\n",
      "Write address -\n",
      "[  3.69963974e-01   3.41171652e-01   2.59275879e-07   2.59870234e-07\n",
      "   2.99806715e-07   3.22150385e-07   3.01590831e-07   2.74593077e-07\n",
      "   2.74205462e-07   2.65111026e-07   2.47255400e-07   2.57295483e-07\n",
      "   2.89369780e-07   3.04003692e-07   2.90969552e-07   2.57656552e-07\n",
      "   2.43692313e-07   2.36361331e-07   2.18861445e-07   2.14884494e-07\n",
      "   2.36203277e-07   2.36786477e-07   2.49526863e-07   2.40878649e-07\n",
      "   2.41227752e-07   2.33095676e-07   2.18956970e-07   2.80914833e-07\n",
      "   3.28059969e-07   3.48292957e-07   2.94517605e-07   2.92361733e-07\n",
      "   3.17918932e-07   3.12885987e-07   2.86124305e-07   2.74920694e-07\n",
      "   3.09683600e-07   3.11582056e-07   3.52066451e-07   3.07865804e-07\n",
      "   3.18297310e-07   3.08019821e-07   3.30572306e-07   2.68912544e-07\n",
      "   2.52686334e-07   2.87750623e-07   3.31876294e-07   3.12811153e-07\n",
      "   2.61956586e-07   2.55073815e-07   2.81655787e-07   2.98186507e-07\n",
      "   3.30993345e-07   3.23594065e-07   2.95489684e-07   2.73770866e-07\n",
      "   2.77326706e-07   2.95610960e-07   2.93695734e-07   2.99000902e-07\n",
      "   3.29247229e-07   3.32058420e-07   3.57171984e-07   3.32228552e-07\n",
      "   3.10611085e-07   2.84092096e-07   2.94569872e-07   2.87409222e-07\n",
      "   2.89244355e-07   2.49569212e-07   2.90046955e-07   3.23880954e-07\n",
      "   3.13431144e-07   2.39494369e-07   2.41546473e-07   2.45323236e-07\n",
      "   2.41173154e-07   2.09329514e-07   2.50765197e-07   2.63797915e-07\n",
      "   2.49199587e-07   2.56214577e-07   2.74680332e-07   2.84090049e-07\n",
      "   2.86272069e-07   2.91788638e-07   3.03717741e-07   2.49168551e-07\n",
      "   2.42656284e-07   2.43288611e-07   2.52960405e-07   2.45917334e-07\n",
      "   2.08693407e-07   2.67937423e-07   2.92223405e-07   2.81247281e-07\n",
      "   2.58477286e-07   3.03394899e-07   3.72815748e-07   3.42823029e-07\n",
      "   3.25502810e-07   2.98518074e-07   3.20997628e-07   3.21740913e-07\n",
      "   3.50025488e-07   3.70301819e-07   3.62985816e-07   3.23344096e-07\n",
      "   3.26660029e-07   3.25980693e-07   3.19676190e-07   2.64442747e-07\n",
      "   2.91149348e-07   3.00330157e-07   3.01946244e-07   2.42338587e-07\n",
      "   2.26686282e-07   2.41107870e-07   2.71553631e-07   2.65911837e-07\n",
      "   2.58191250e-07   2.52654843e-07   2.78439302e-07   2.55658136e-07\n",
      "   2.48431718e-07   2.75839426e-07   3.24138142e-07   2.88828880e-01]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 2 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.36661029]\n",
      "Write gamma - [ 3.04418159]\n",
      "Write address -\n",
      "[  4.52667862e-01   2.63448358e-01   3.70924436e-02   1.90397628e-11\n",
      "   1.97495856e-11   2.02022791e-11   1.98868283e-11   1.93624301e-11\n",
      "   1.90769640e-11   1.88161760e-11   1.85910159e-11   1.88325241e-11\n",
      "   1.94472355e-11   1.97909310e-11   1.94688762e-11   1.88198051e-11\n",
      "   1.83278167e-11   1.79951557e-11   1.76871347e-11   1.76661654e-11\n",
      "   1.79123522e-11   1.81767518e-11   1.82794925e-11   1.82670302e-11\n",
      "   1.81443124e-11   1.79355818e-11   1.81624594e-11   1.92253782e-11\n",
      "   2.05066797e-11   2.07270798e-11   2.01396139e-11   1.98888597e-11\n",
      "   2.01762027e-11   2.00972156e-11   1.96095987e-11   1.95246857e-11\n",
      "   1.99167939e-11   2.05425729e-11   2.07543514e-11   2.05590042e-11\n",
      "   2.02655739e-11   2.03899883e-11   2.01203030e-11   1.93440403e-11\n",
      "   1.89261107e-11   1.96089117e-11   2.03280483e-11   2.00141171e-11\n",
      "   1.91329366e-11   1.88567131e-11   1.92820673e-11   1.99695486e-11\n",
      "   2.04859307e-11   2.04212515e-11   1.98098066e-11   1.93285145e-11\n",
      "   1.93498950e-11   1.96057615e-11   1.97654584e-11   2.00618064e-11\n",
      "   2.05368049e-11   2.10222673e-11   2.11884676e-11   2.08715076e-11\n",
      "   2.01637006e-11   1.97196929e-11   1.95952889e-11   1.95962430e-11\n",
      "   1.92535137e-11   1.90597434e-11   1.95521689e-11   2.02428716e-11\n",
      "   1.97811420e-11   1.87381239e-11   1.82309341e-11   1.82635435e-11\n",
      "   1.79969199e-11   1.78716087e-11   1.82622910e-11   1.86345089e-11\n",
      "   1.85979773e-11   1.87199041e-11   1.90888520e-11   1.93734993e-11\n",
      "   1.95191779e-11   1.97038948e-11   1.94764083e-11   1.87986206e-11\n",
      "   1.83029668e-11   1.83355726e-11   1.84116541e-11   1.81104298e-11\n",
      "   1.80288388e-11   1.86979564e-11   1.93895351e-11   1.92547055e-11\n",
      "   1.92193327e-11   2.01896798e-11   2.12588940e-11   2.12659716e-11\n",
      "   2.05662171e-11   2.02416816e-11   2.03377506e-11   2.07557894e-11\n",
      "   2.13172778e-11   2.17715065e-11   2.15081061e-11   2.09266302e-11\n",
      "   2.06475132e-11   2.06139151e-11   2.00798388e-11   1.95015358e-11\n",
      "   1.94978079e-11   1.98426241e-11   1.94658439e-11   1.85730077e-11\n",
      "   1.80293974e-11   1.83282382e-11   1.87905785e-11   1.88908784e-11\n",
      "   1.87038770e-11   1.87713109e-11   1.88901117e-11   1.87341913e-11\n",
      "   1.86767529e-11   1.93564800e-11   2.73276847e-02   2.19463646e-01]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 3 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 3.37578249]\n",
      "Write gamma - [ 4.5907793]\n",
      "Write address -\n",
      "[  7.36673534e-01   1.73403218e-01   1.07994524e-03   1.02590924e-07\n",
      "   5.18526439e-18   5.18527183e-18   5.18526770e-18   5.18526108e-18\n",
      "   5.18525529e-18   5.18525198e-18   5.18524826e-18   5.18525198e-18\n",
      "   5.18526108e-18   5.18526646e-18   5.18526108e-18   5.18525198e-18\n",
      "   5.18524247e-18   5.18523709e-18   5.18523378e-18   5.18523213e-18\n",
      "   5.18523585e-18   5.18524123e-18   5.18524247e-18   5.18524247e-18\n",
      "   5.18524123e-18   5.18523709e-18   5.18524123e-18   5.18525694e-18\n",
      "   5.18527680e-18   5.18528052e-18   5.18527183e-18   5.18526770e-18\n",
      "   5.18527183e-18   5.18527183e-18   5.18526439e-18   5.18526232e-18\n",
      "   5.18526770e-18   5.18527680e-18   5.18528052e-18   5.18527886e-18\n",
      "   5.18527307e-18   5.18527514e-18   5.18527183e-18   5.18525901e-18\n",
      "   5.18525363e-18   5.18526232e-18   5.18527307e-18   5.18526976e-18\n",
      "   5.18525694e-18   5.18525198e-18   5.18525901e-18   5.18526770e-18\n",
      "   5.18527680e-18   5.18527514e-18   5.18526646e-18   5.18525901e-18\n",
      "   5.18525901e-18   5.18526232e-18   5.18526646e-18   5.18526976e-18\n",
      "   5.18527886e-18   5.18528589e-18   5.18528755e-18   5.18528424e-18\n",
      "   5.18527307e-18   5.18526646e-18   5.18526232e-18   5.18526232e-18\n",
      "   5.18525694e-18   5.18525529e-18   5.18526232e-18   5.18527307e-18\n",
      "   5.18526646e-18   5.18524991e-18   5.18524247e-18   5.18524247e-18\n",
      "   5.18523709e-18   5.18523585e-18   5.18524247e-18   5.18524826e-18\n",
      "   5.18524826e-18   5.18524991e-18   5.18525529e-18   5.18525901e-18\n",
      "   5.18526232e-18   5.18526439e-18   5.18526108e-18   5.18525198e-18\n",
      "   5.18524247e-18   5.18524247e-18   5.18524454e-18   5.18523916e-18\n",
      "   5.18523916e-18   5.18524826e-18   5.18525901e-18   5.18525694e-18\n",
      "   5.18525694e-18   5.18527183e-18   5.18528755e-18   5.18528962e-18\n",
      "   5.18527886e-18   5.18527307e-18   5.18527514e-18   5.18528052e-18\n",
      "   5.18528962e-18   5.18529623e-18   5.18529293e-18   5.18528424e-18\n",
      "   5.18528052e-18   5.18527886e-18   5.18527183e-18   5.18526232e-18\n",
      "   5.18526232e-18   5.18526646e-18   5.18526108e-18   5.18524826e-18\n",
      "   5.18523916e-18   5.18524247e-18   5.18524991e-18   5.18525198e-18\n",
      "   5.18524991e-18   5.18524991e-18   5.18525198e-18   5.18524991e-18\n",
      "   5.18524991e-18   5.71848435e-09   2.71599711e-04   8.85716155e-02]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 4 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 3.92385888]\n",
      "Write gamma - [ 3.01333523]\n",
      "Write address -\n",
      "[  9.98588800e-01   1.34804624e-03   3.19294504e-13   2.03643950e-25\n",
      "   1.17473647e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17372661e-27   1.17372661e-27   1.17372661e-27   1.17372661e-27\n",
      "   1.17377370e-27   5.90897840e-27   1.12680708e-15   6.31205985e-05]\n",
      "Argmax - 0\n",
      "\n",
      "Epoch - 31, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 32, Mean error of final batch in epoch - 0.00133333\n",
      "Epoch - 33, Mean error of final batch in epoch - 0.000666667\n",
      "Epoch - 34, Mean error of final batch in epoch - 0.00333333\n",
      "Epoch - 35, Mean error of final batch in epoch - 0.000666667\n",
      "Epoch - 36, Mean error of final batch in epoch - 0.000666667\n",
      "Epoch - 37, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 38, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 39, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 40, Mean error of final batch in epoch - 0.0\n",
      "\n",
      "Step 0 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.38240552]\n",
      "Write gamma - [ 1.17529356]\n",
      "Write address -\n",
      "[  1.00000083e+00   9.78479378e-08   5.25290957e-08   1.73841357e-07\n",
      "   5.30573743e-07   8.75940316e-08   4.61987128e-07   2.18965525e-07\n",
      "   9.15712121e-07   1.09726308e-07   3.94855505e-07   8.20888246e-08\n",
      "   3.26404574e-08   6.80884341e-07   9.32382079e-07   4.12516954e-07\n",
      "   3.86391150e-07   4.65443492e-07   5.98129247e-07   7.45730290e-07\n",
      "   6.67717075e-07   2.78784285e-07   8.48729258e-07   4.84185705e-07\n",
      "   5.82802386e-07   7.04472200e-07   2.38388182e-07   7.31781597e-07\n",
      "   5.30500756e-07   8.37148548e-07   8.47109334e-07   6.52177221e-07\n",
      "   6.55831457e-07   4.76830849e-07   5.16767386e-07   2.10371141e-07\n",
      "   8.32192995e-07   7.83111830e-07   8.01767612e-07   5.08476148e-07\n",
      "   6.71170483e-07   1.90892226e-08   7.72925603e-07   5.95989206e-07\n",
      "   4.35357464e-07   4.52073579e-07   1.40520456e-07   6.31478315e-07\n",
      "   1.53727058e-07   4.25372129e-08   8.86795135e-07   8.49115835e-08\n",
      "   5.13568978e-07   6.38614267e-07   8.95917879e-07   5.68225403e-07\n",
      "   4.36131955e-07   3.43283403e-07   9.38755761e-07   3.92364626e-07\n",
      "   2.65482555e-07   2.67679582e-07   8.99721385e-07   2.42728959e-07\n",
      "   7.21410743e-07   5.97842359e-07   4.99156499e-07   5.06354695e-07\n",
      "   9.61462774e-07   5.07034088e-07   9.03720832e-07   2.36477490e-07\n",
      "   5.91756248e-07   2.59655224e-07   6.40863391e-07   1.23834127e-07\n",
      "   4.16938434e-07   5.16398529e-07   6.04136574e-07   6.24467475e-07\n",
      "   8.89076830e-07   3.80752454e-07   8.23363791e-07   6.35884987e-07\n",
      "   2.54457120e-07   8.31652869e-07   8.16545480e-07   6.32034073e-07\n",
      "   4.89851857e-07   3.37996255e-07   5.08387302e-07   6.45052523e-07\n",
      "   2.37057094e-07   5.15938268e-07   8.86670477e-07   6.00589146e-07\n",
      "   3.36156120e-07   9.85213774e-07   9.01049987e-07   8.13381405e-07\n",
      "   7.19704531e-07   1.25117666e-07   6.85970292e-07   5.14110319e-08\n",
      "   2.12294339e-07   8.24096446e-07   9.99142117e-07   2.19106677e-10\n",
      "   6.16440047e-07   4.41531057e-07   8.95539017e-07   5.17749640e-07\n",
      "   7.64174843e-07   7.65238781e-07   1.40662308e-07   9.36185927e-07\n",
      "   3.86941423e-07   1.61797644e-07   7.42705708e-07   9.62689796e-07\n",
      "   8.02138914e-07   2.26147407e-07   7.16837746e-07   3.98572439e-07\n",
      "   4.26132090e-07   7.19283946e-07   3.93338198e-08   2.34559295e-07]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 1 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 2.28784776]\n",
      "Write address -\n",
      "[  3.67895007e-01   3.38421911e-01   1.20827536e-07   1.38436903e-07\n",
      "   1.43403085e-07   1.52941766e-07   1.41468220e-07   1.74516487e-07\n",
      "   1.64949157e-07   1.68784538e-07   1.33880917e-07   1.29721542e-07\n",
      "   1.38932805e-07   1.78256840e-07   1.99503702e-07   1.84151418e-07\n",
      "   1.62245385e-07   1.70281240e-07   1.86505105e-07   1.96838769e-07\n",
      "   1.83779889e-07   1.84221847e-07   1.79598189e-07   1.91919625e-07\n",
      "   1.84917013e-07   1.76435066e-07   1.79130240e-07   1.73854247e-07\n",
      "   1.99305703e-07   2.05724504e-07   2.12541977e-07   2.03303557e-07\n",
      "   1.86985503e-07   1.79904362e-07   1.61257020e-07   1.73210495e-07\n",
      "   1.87866775e-07   2.15329578e-07   2.01708318e-07   1.94596424e-07\n",
      "   1.62522724e-07   1.68475282e-07   1.68495632e-07   1.87901392e-07\n",
      "   1.72471530e-07   1.53445413e-07   1.58743035e-07   1.49377797e-07\n",
      "   1.43987549e-07   1.50751589e-07   1.54554854e-07   1.71054680e-07\n",
      "   1.60540338e-07   1.96964450e-07   2.02155860e-07   1.92265290e-07\n",
      "   1.66833559e-07   1.80408165e-07   1.83281131e-07   1.78500102e-07\n",
      "   1.47580749e-07   1.67269846e-07   1.71771035e-07   1.87950121e-07\n",
      "   1.76283066e-07   1.88227176e-07   1.77911900e-07   1.92377456e-07\n",
      "   1.96718744e-07   2.11697923e-07   1.82725231e-07   1.82503058e-07\n",
      "   1.56000567e-07   1.71301451e-07   1.53994890e-07   1.57871298e-07\n",
      "   1.52720389e-07   1.74431051e-07   1.84186177e-07   2.00207936e-07\n",
      "   1.93380799e-07   1.98570987e-07   1.89279064e-07   1.84793123e-07\n",
      "   1.80844694e-07   1.91265471e-07   2.09889663e-07   1.93977627e-07\n",
      "   1.72171298e-07   1.65129450e-07   1.72079865e-07   1.70044160e-07\n",
      "   1.67568572e-07   1.77649696e-07   1.97288983e-07   1.89329825e-07\n",
      "   1.89574664e-07   2.06309352e-07   2.29125746e-07   2.16685891e-07\n",
      "   1.83191332e-07   1.72376801e-07   1.47246240e-07   1.48238314e-07\n",
      "   1.51846692e-07   1.96466914e-07   1.92473891e-07   1.76218720e-07\n",
      "   1.53810859e-07   1.91877334e-07   1.90839486e-07   2.03309526e-07\n",
      "   1.98047616e-07   1.83813697e-07   1.85459825e-07   1.73601705e-07\n",
      "   1.73890342e-07   1.61293613e-07   1.88540241e-07   2.20246548e-07\n",
      "   1.98417510e-07   1.82419242e-07   1.67212008e-07   1.75148784e-07\n",
      "   1.73824247e-07   1.61944058e-07   1.50035390e-07   2.93661028e-01]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 2 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.69955814]\n",
      "Write gamma - [ 3.97016454]\n",
      "Write address -\n",
      "[  4.91981328e-01   2.51459748e-01   2.14329660e-02   1.44613352e-13\n",
      "   1.47380128e-13   1.48168262e-13   1.49939821e-13   1.52959378e-13\n",
      "   1.54460808e-13   1.51423673e-13   1.46490120e-13   1.44033900e-13\n",
      "   1.48122644e-13   1.56131957e-13   1.61028946e-13   1.58637182e-13\n",
      "   1.54943427e-13   1.55728864e-13   1.59562278e-13   1.61261602e-13\n",
      "   1.60207175e-13   1.58855662e-13   1.59245230e-13   1.60090745e-13\n",
      "   1.59333985e-13   1.57776895e-13   1.57065333e-13   1.58625175e-13\n",
      "   1.62465202e-13   1.66034395e-13   1.66779635e-13   1.64562754e-13\n",
      "   1.60825536e-13   1.56959284e-13   1.54713061e-13   1.56185327e-13\n",
      "   1.61542573e-13   1.65617587e-13   1.65197432e-13   1.60355114e-13\n",
      "   1.55646045e-13   1.54012951e-13   1.56072217e-13   1.57596958e-13\n",
      "   1.55316840e-13   1.51846580e-13   1.50381118e-13   1.49034024e-13\n",
      "   1.48112845e-13   1.48955785e-13   1.51338008e-13   1.53115503e-13\n",
      "   1.55876288e-13   1.60763953e-13   1.63592759e-13   1.60411032e-13\n",
      "   1.57070117e-13   1.57279531e-13   1.58362689e-13   1.55340394e-13\n",
      "   1.52170906e-13   1.52955759e-13   1.56453775e-13   1.58177209e-13\n",
      "   1.58749112e-13   1.58707344e-13   1.59353691e-13   1.61013103e-13\n",
      "   1.64068737e-13   1.64175233e-13   1.61085338e-13   1.56566776e-13\n",
      "   1.54006324e-13   1.52728253e-13   1.51763910e-13   1.50570244e-13\n",
      "   1.51897971e-13   1.55393222e-13   1.59833315e-13   1.62379916e-13\n",
      "   1.63090461e-13   1.62525660e-13   1.61196387e-13   1.59475799e-13\n",
      "   1.59417089e-13   1.62190641e-13   1.64425467e-13   1.61723025e-13\n",
      "   1.56710216e-13   1.54575192e-13   1.54857193e-13   1.54901401e-13\n",
      "   1.55216944e-13   1.58099933e-13   1.61091220e-13   1.61503894e-13\n",
      "   1.62315812e-13   1.66753628e-13   1.70471615e-13   1.67569883e-13\n",
      "   1.60699280e-13   1.54428933e-13   1.50113822e-13   1.48633560e-13\n",
      "   1.52826536e-13   1.59195004e-13   1.60777736e-13   1.56241177e-13\n",
      "   1.54893459e-13   1.58546584e-13   1.62451839e-13   1.63762951e-13\n",
      "   1.62778875e-13   1.60411371e-13   1.58538872e-13   1.56975737e-13\n",
      "   1.55055114e-13   1.55511305e-13   1.61108648e-13   1.66123015e-13\n",
      "   1.64069415e-13   1.58713700e-13   1.55922096e-13   1.55770118e-13\n",
      "   1.55219208e-13   1.52447662e-13   1.76528618e-02   2.17473075e-01]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 3 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 3.22279239]\n",
      "Write gamma - [ 5.11338377]\n",
      "Write address -\n",
      "[  8.90768230e-01   7.16562122e-02   1.06461430e-05   2.86921723e-12\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   2.47533887e-23   2.47533887e-23   2.47533887e-23\n",
      "   2.47533887e-23   1.52635432e-13   3.21856896e-06   3.75616848e-02]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 4 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 3.19951034]\n",
      "Write gamma - [ 2.3012073]\n",
      "Write address -\n",
      "[  9.99997020e-01   2.88967499e-06   7.94794488e-20   4.29665755e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.83365736e-31   3.83365736e-31   3.83365736e-31\n",
      "   3.83365736e-31   3.91252386e-31   2.01937247e-22   1.07942093e-07]\n",
      "Argmax - 0\n",
      "\n",
      "Epoch - 41, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 42, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 43, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 44, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 45, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 46, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 47, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 48, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 49, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 50, Mean error of final batch in epoch - 0.0\n",
      "\n",
      "Step 0 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.39984226]\n",
      "Write gamma - [ 1.22301888]\n",
      "Write address -\n",
      "[  1.00000024e+00   1.91943641e-07   7.21961499e-07   1.32992739e-07\n",
      "   9.78131879e-07   6.68692564e-07   5.26667009e-07   7.17560283e-07\n",
      "   3.62402197e-08   9.45848114e-07   5.35277479e-07   8.12052633e-07\n",
      "   2.01164596e-07   6.69082738e-07   3.98073183e-07   8.22231186e-07\n",
      "   7.76330694e-07   3.51718910e-07   1.96566347e-07   2.80642865e-07\n",
      "   1.60708311e-07   8.86108523e-07   8.53368306e-07   4.04715422e-07\n",
      "   4.73788504e-07   5.10116820e-07   4.07118904e-07   6.45820876e-07\n",
      "   3.41877950e-07   6.71752787e-07   8.27548149e-07   5.10950429e-07\n",
      "   7.43716981e-08   4.49052095e-07   5.02647254e-07   6.66538824e-07\n",
      "   2.00420743e-07   3.23597902e-07   3.16055548e-07   1.44243242e-10\n",
      "   6.58607973e-07   7.11269593e-07   1.11127015e-07   1.97467685e-07\n",
      "   4.67692615e-07   1.70435897e-08   4.87170837e-07   7.88236832e-07\n",
      "   1.14257929e-07   1.33319503e-07   2.36732362e-07   6.23904214e-07\n",
      "   6.94289213e-08   2.81221759e-07   4.90650280e-07   7.05818422e-07\n",
      "   5.04939180e-07   8.42931286e-07   2.72360325e-07   6.97030202e-07\n",
      "   7.89706576e-07   9.98023893e-07   2.84659137e-07   7.93338188e-07\n",
      "   2.65846012e-07   2.69552601e-07   8.02947511e-07   7.68123243e-07\n",
      "   4.57793107e-07   5.58025590e-07   4.89482261e-07   5.66524363e-07\n",
      "   6.80999051e-07   1.83671709e-08   6.78816434e-07   2.54622819e-07\n",
      "   8.66692801e-07   2.09224216e-09   1.97849985e-07   9.55891210e-07\n",
      "   4.06343702e-07   3.37823991e-07   3.79983902e-07   8.70547638e-07\n",
      "   6.06233925e-07   7.09469703e-07   1.36221530e-07   8.22856407e-07\n",
      "   7.83715279e-08   4.26399339e-07   5.36829248e-07   7.63894334e-08\n",
      "   7.46535420e-07   6.41508223e-07   2.06155768e-08   8.30311535e-07\n",
      "   3.84448043e-07   9.96392373e-07   8.01222313e-07   8.10273889e-07\n",
      "   9.79572974e-07   6.40111921e-07   4.29762594e-07   7.25669281e-07\n",
      "   1.95050362e-07   6.58251906e-07   1.44363767e-07   3.65953326e-07\n",
      "   4.17957295e-08   5.13893838e-07   5.53432926e-07   3.90497448e-08\n",
      "   9.91209745e-08   7.38620258e-07   8.70083795e-07   2.29176763e-07\n",
      "   9.57152452e-07   1.62935265e-08   4.80643735e-07   5.90765808e-07\n",
      "   7.48545517e-07   3.70830065e-07   9.27875647e-07   1.28417497e-07\n",
      "   1.98420409e-07   2.07319857e-07   2.15938570e-07   2.31127146e-07]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 1 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 2.61979485]\n",
      "Write address -\n",
      "[  3.76437545e-01   3.36230904e-01   8.60915961e-08   1.02559277e-07\n",
      "   1.04317266e-07   1.14593185e-07   1.06555689e-07   9.24560268e-08\n",
      "   9.88677584e-08   9.76837953e-08   1.16564983e-07   9.92316416e-08\n",
      "   9.97157841e-08   9.08760214e-08   1.05252184e-07   1.09387841e-07\n",
      "   1.09399188e-07   9.21790786e-08   7.87483927e-08   7.44905293e-08\n",
      "   8.96018122e-08   1.06707759e-07   1.14682408e-07   1.02183705e-07\n",
      "   9.31736679e-08   9.36544851e-08   9.71615819e-08   9.43051219e-08\n",
      "   9.94623477e-08   1.04653218e-07   1.10633984e-07   9.52847188e-08\n",
      "   8.31744273e-08   8.37261709e-08   9.87990987e-08   9.41438145e-08\n",
      "   8.79082620e-08   7.92356758e-08   7.51116360e-08   8.08187792e-08\n",
      "   9.25195849e-08   9.74750947e-08   8.36723899e-08   7.68439392e-08\n",
      "   7.65005268e-08   8.13146812e-08   8.98199843e-08   9.52535117e-08\n",
      "   8.42891907e-08   7.00620433e-08   8.20005184e-08   8.30371931e-08\n",
      "   8.21476078e-08   7.86272807e-08   9.49347054e-08   1.02048148e-07\n",
      "   1.09882315e-07   1.00987364e-07   1.03269699e-07   1.02647626e-07\n",
      "   1.21875388e-07   1.13504498e-07   1.10071376e-07   9.35826989e-08\n",
      "   9.18394676e-08   9.04206487e-08   1.05170130e-07   1.11155295e-07\n",
      "   1.03438687e-07   9.65204734e-08   9.89734943e-08   1.02071709e-07\n",
      "   9.20386469e-08   9.12162079e-08   8.31582199e-08   1.02352821e-07\n",
      "   8.88449705e-08   8.46027675e-08   8.50330508e-08   9.92517712e-08\n",
      "   1.01777339e-07   8.63575238e-08   9.70445626e-08   1.06301911e-07\n",
      "   1.14176125e-07   9.66229905e-08   9.86931781e-08   8.62698855e-08\n",
      "   9.08430238e-08   8.39630019e-08   8.56175149e-08   9.06688555e-08\n",
      "   9.54412727e-08   9.56770023e-08   9.37332629e-08   9.04666422e-08\n",
      "   1.13329236e-07   1.14788087e-07   1.25929404e-07   1.24923531e-07\n",
      "   1.22060868e-07   1.11468687e-07   1.03136166e-07   9.38090778e-08\n",
      "   9.70004663e-08   8.46277430e-08   8.70658781e-08   7.29037168e-08\n",
      "   8.00107074e-08   8.58690115e-08   8.75013342e-08   7.54409157e-08\n",
      "   7.83215626e-08   1.01131093e-07   1.07014934e-07   1.08864270e-07\n",
      "   9.10905911e-08   9.38227842e-08   8.51173922e-08   1.04130756e-07\n",
      "   1.02822412e-07   1.09081995e-07   9.65510338e-08   8.97859778e-08\n",
      "   7.15497706e-08   7.37367358e-08   7.45336379e-08   2.87319690e-01]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 2 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.76069427]\n",
      "Write gamma - [ 4.29725218]\n",
      "Write address -\n",
      "[  5.26683152e-01   2.50589401e-01   1.51388021e-02   2.13466109e-15\n",
      "   2.17497986e-15   2.19146651e-15   2.16656925e-15   2.13037913e-15\n",
      "   2.12537041e-15   2.15657151e-15   2.17728019e-15   2.16131066e-15\n",
      "   2.12730122e-15   2.12603406e-15   2.15442195e-15   2.18390462e-15\n",
      "   2.16607395e-15   2.10654087e-15   2.04649703e-15   2.03884600e-15\n",
      "   2.09145733e-15   2.16341172e-15   2.18895231e-15   2.15705940e-15\n",
      "   2.11927283e-15   2.11313629e-15   2.11834427e-15   2.12308406e-15\n",
      "   2.13864638e-15   2.16622705e-15   2.16690256e-15   2.12153462e-15\n",
      "   2.07314024e-15   2.07847337e-15   2.10884946e-15   2.10940384e-15\n",
      "   2.07689493e-15   2.04289206e-15   2.02945219e-15   2.05276868e-15\n",
      "   2.09447446e-15   2.10337064e-15   2.06809807e-15   2.03361091e-15\n",
      "   2.03023930e-15   2.05219947e-15   2.08576421e-15   2.09532785e-15\n",
      "   2.05770349e-15   2.02576823e-15   2.03653233e-15   2.05334148e-15\n",
      "   2.04798231e-15   2.05998688e-15   2.10341828e-15   2.15279586e-15\n",
      "   2.16931088e-15   2.16138414e-15   2.15409605e-15   2.18180123e-15\n",
      "   2.21660581e-15   2.21757376e-15   2.17517955e-15   2.12833270e-15\n",
      "   2.10034461e-15   2.11434458e-15   2.15600484e-15   2.17997037e-15\n",
      "   2.15988806e-15   2.13613175e-15   2.13695507e-15   2.13418316e-15\n",
      "   2.11314180e-15   2.08716097e-15   2.09244201e-15   2.10950316e-15\n",
      "   2.09706808e-15   2.06978832e-15   2.08405003e-15   2.12189038e-15\n",
      "   2.12620898e-15   2.10696777e-15   2.12443254e-15   2.17178528e-15\n",
      "   2.17957247e-15   2.15039537e-15   2.11519839e-15   2.09440924e-15\n",
      "   2.07974477e-15   2.07167212e-15   2.07322495e-15   2.09370896e-15\n",
      "   2.11209423e-15   2.11640118e-15   2.10769813e-15   2.12791871e-15\n",
      "   2.18083074e-15   2.23091855e-15   2.25919421e-15   2.26813697e-15\n",
      "   2.24485754e-15   2.20328071e-15   2.15590214e-15   2.12655520e-15\n",
      "   2.10522225e-15   2.08325911e-15   2.05408666e-15   2.03339915e-15\n",
      "   2.03927693e-15   2.06445265e-15   2.06007222e-15   2.03768514e-15\n",
      "   2.05865450e-15   2.12448039e-15   2.17181429e-15   2.16009898e-15\n",
      "   2.12340403e-15   2.09470676e-15   2.10292171e-15   2.13512760e-15\n",
      "   2.16597230e-15   2.16238554e-15   2.13138392e-15   2.07432990e-15\n",
      "   2.02557998e-15   2.00815185e-15   1.02074249e-02   1.97381213e-01]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 3 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 2.70385337]\n",
      "Write gamma - [ 5.68861628]\n",
      "Write address -\n",
      "[  9.40051675e-01   4.45715636e-02   8.56612928e-07   2.11033278e-14\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   2.67329430e-25   2.67329430e-25   2.67329430e-25\n",
      "   2.67329430e-25   5.51342216e-16   1.13669920e-07   1.53757548e-02]\n",
      "Argmax - 0\n",
      "\n",
      "\n",
      "Step 4 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 3.49871945]\n",
      "Write gamma - [ 3.04396653]\n",
      "Write address -\n",
      "[  1.00000000e+00   3.27147909e-08   1.92470980e-25   1.06323372e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05852830e-34   1.05852830e-34   1.05852830e-34\n",
      "   1.05852830e-34   1.05895212e-34   7.34633639e-29   8.51831036e-11]\n",
      "Argmax - 0\n",
      "\n",
      "Epoch - 51, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 52, Mean error of final batch in epoch - 0.0\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "# An annoying thing here is that we cannot use a list as a key in a \n",
    "# dictionary. The workaround we found on StackOverflow here:\n",
    "# http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "\n",
    "# epoch is a global var\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences\n",
    "        for z in range(batch_size):\n",
    "            # construct a sequence from 0,...,num_classes - 3 then append initial and terminal symbols\n",
    "            a = [random.randint(0,num_classes-3) for k in range(N-2)]\n",
    "            fa = func_to_learn(a)\n",
    "            a = [init_symbol] + a + [term_symbol]\n",
    "            a_onehot = [one_hots[e] for e in a]\n",
    "            fa_onehot = [one_hots[e] for e in fa]\n",
    "            inp.append(np.array(a_onehot))\n",
    "            out.append(np.array(fa_onehot))        \n",
    "        \n",
    "        feed_dict = {}\n",
    "        for d in range(N):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "        \n",
    "        # for the first batch in an epoch, we have some logging\n",
    "        if( j == 0 and i % 10 == 0 ):\n",
    "            gamma_reads_val, gamma_writes_val, read_addresses_val, write_addresses_val = sess.run([gamma_reads,gamma_writes,read_addresses,write_addresses],feed_dict)\n",
    "    \n",
    "            s = 0\n",
    "            for r in range(len(write_addresses_val)):\n",
    "                print(\"\")\n",
    "                print(\"Step \" + str(s) + \" of the RNN run on the first input of first batch of this epoch\")\n",
    "                print(\"Read gamma  - \" + str(gamma_reads_val[r]))\n",
    "                print(\"Write gamma - \" + str(gamma_writes_val[r]))\n",
    "                print(\"w rotations - \" + str(ss[r]))\n",
    "                print(\"Write address -\")\n",
    "                print(write_addresses_val[r])\n",
    "                print(\"Argmax - \" + str(write_addresses_val[r].argmax()))\n",
    "                print(\"\")\n",
    "                s = s + 1\n",
    "        \n",
    "        # Do gradient descent\n",
    "        summary,_ = sess.run([merged_summaries,minimize], feed_dict)\n",
    "        \n",
    "        # Write out TensorBoard logs\n",
    "        file_writer.add_summary(summary)\n",
    "    current_mean = np.mean(sess.run(errors, feed_dict))\n",
    "    print(\"Epoch - \" + str(i+1) + \", Mean error of final batch in epoch - \" + str(current_mean))\n",
    "    \n",
    "    # DEBUG\n",
    "    #with tf.variable_scope(\"NTM\",reuse=True):\n",
    "    #    H = tf.get_variable(\"H\", [controller_state_size,controller_state_size])\n",
    "    #    print(sess.run(H))\n",
    "\n",
    "# Write out variables to disk\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took\", time.time() - pre_train_time, \"seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "# Note that all the weights will be loaded from the saved training session\n",
    "inputs_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest)]\n",
    "targets_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest_out)]\n",
    "state_test = tf.truncated_normal([batch_size, state_size], 0.0, 0.01, dtype=tf.float32)\n",
    "\n",
    "# Set up test graph\n",
    "reuse = True\n",
    "for i in range(Ntest):\n",
    "    output, state = cell(inputs_test[i],state_test,'NTM',reuse)\n",
    "\n",
    "rnn_outputs_test = []\n",
    "for i in range(Ntest_out):\n",
    "    output, state = cell(tf.zeros([batch_size,input_size]),state_test,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "    \n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.softmax(logit) for logit in logits_test] \n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets_test[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "\n",
    "# Restore the weights from training\n",
    "sess = tf.Session()\n",
    "saver.restore(sess,save_path)\n",
    "\n",
    "# DEBUG\n",
    "#with tf.variable_scope(\"NTM\",reuse=True):\n",
    "#    H = tf.get_variable(\"H\", [controller_state_size,controller_state_size])\n",
    "#    print(sess.run(H))\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "#print(\"Number of batches: \" + str(no_of_batches))\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    # We sample each batch on the fly from the set of all sequences\n",
    "    for z in range(batch_size):\n",
    "        a = [random.randint(0,num_classes-3) for k in range(Ntest-2)]\n",
    "        fa = func_to_learn(a)\n",
    "        a = [init_symbol] + a + [term_symbol]\n",
    "        a_onehot = [one_hots[e] for e in a]\n",
    "        fa_onehot = [one_hots[e] for e in fa]\n",
    "        inp.append(np.array(a_onehot))\n",
    "        out.append(np.array(fa_onehot))        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(Ntest):\n",
    "        in_node = inputs_test[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest_out):\n",
    "        out_node = targets_test[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = np.mean(sess.run(errors_test, feed_dict))\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "# The first three digits of this should match the printout for the\n",
    "# first three test output sequences given earlier\n",
    "#data = sess.run([tf.argmax(targets[0],1), tf.argmax(prediction[0],1)],feed_dict)\n",
    "\n",
    "#print(\"First digits of test outputs (actual)\")\n",
    "#print(data[0])\n",
    "#print(\"First digits of test outputs (predicted)\")\n",
    "#print(data[1])\n",
    "\n",
    "# print the mean of the errors in each digit for the test set.\n",
    "#incorrects = sess.run(errors, feed_dict)\n",
    "# print(incorrects)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"N_out         - \" + str(N_out))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "print(\"Ntest_out     - \" + str(Ntest_out))\n",
    "print(\"ring 1 powers - \" + str(powers_ring1))\n",
    "print(\"ring 2 powers - \" + str(powers_ring2))\n",
    "print(\"# epochs      - \" + str(epoch))\n",
    "print(\"optimizer     - \" + str(model_optimizer))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"(css,mas,mcs) - (\" + str(controller_state_size) + \",\" + str(memory_address_size) + \",\" + str(memory_content_size) + \")\")\n",
    "print(\"train percent - \" + str(training_percent))\n",
    "print(\"num_training  - \" + str(num_training) + \"/\" + str(num_classes**N))\n",
    "print(\"num_test      - \" + str(num_test) + \"/\" + str(num_classes**N))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error         - \" + str(final_error))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
