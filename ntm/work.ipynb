{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of the Linear Logic Recurrent Neural Network (LLRNN)\n",
    "#\n",
    "# Version 10.0\n",
    "\n",
    "###################\n",
    "# HYPERPARAMETERS #\n",
    "###################\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, mult_pattern_ntm\n",
    "task                  = 'copy' # copy, repeat copy, pattern i, mult pattern i, mixed pattern i\n",
    "epoch                 = 100 # number of training epochs, default to 100\n",
    "num_classes           = 10 # number of symbols, INCLUDING initial and terminal symbols, default 10\n",
    "N                     = 30 # length of input sequences for training, default to 30\n",
    "Ntest                 = 35 # length of sequences for testing, default to 35\n",
    "batch_size            = 250 # default 250\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "num_training          = 10000 # default 10000\n",
    "num_test              = num_training\n",
    "term_symbol           = num_classes - 1\n",
    "init_symbol           = num_classes - 2\n",
    "div_symbol            = num_classes - 3\n",
    "learning_rate         = 1e-4 # default 1e-4\n",
    "memory_init_bias      = 1.0 # default 1.0\n",
    "use_curriculum        = True # default True\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "\n",
    "##################\n",
    "# MODEL SPECIFIC #\n",
    "##################\n",
    "\n",
    "ntm_memory_address_size   = 128 # number of memory locations, default 128\n",
    "ntm_memory_content_size   = 20 # size of vector stored at a memory location, default 20\n",
    "ntm_powers                = [0,-1,1] # powers of R used by controller, default [0,-1,1]\n",
    "\n",
    "pattern_ntm_powers               = [[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "pattern_ntm_powers_2_on_1        = [0,1,2] # allowed powers used by ring 2 to manipulate ring 1\n",
    "pattern_ntm_memory_address_sizes = [128, 1] # number of memory locations for the three rings\n",
    "pattern_ntm_memory_content_sizes = [20, 3] # size of content vector for each ring\n",
    "pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "mult_pattern_ntm_powers               = [[0,-1,1],[0,-1,1],[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "mult_pattern_ntm_powers_2_on_1        = [0,1,2] # allowed powers used by rings 2,3 to manipulate ring 1\n",
    "mult_pattern_ntm_memory_address_sizes = [128, 20, 20, 10] # number of memory locations for the rings\n",
    "mult_pattern_ntm_memory_content_sizes = [20, 3, 3, 2] # size of content vector for each ring\n",
    "mult_pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there\n",
    "# 2. The initial and terminal symbols are always from the end of the list of symbols, so they\n",
    "# are respectively num_classes - 2 and num_classes - 1. So the number of symbols which are\n",
    "# not initial or terminal is num_classes - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import pandas\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs\n",
    "\n",
    "assert use_model == 'ntm' or use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[6, 5, 3, 7, 1, 6, 2, 0]\n",
      "is mapped to\n",
      "[6, 6, 5, 5, 3, 3, 1, 2, 6, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "# Below N_out and Ntest_out are the lengths of the outputs in both the training\n",
    "# and testing regimes respectively. Since outputs do not include the initial and terminal\n",
    "# symbols, these default to N - 2 and Ntest - 2 respectively.\n",
    "\n",
    "# Default sampling from space of inputs\n",
    "def generate_input_seq_default(max_symbol,input_length):\n",
    "    return [random.randint(0,max_symbol) for k in range(input_length)]\n",
    "\n",
    "generate_input_seq = generate_input_seq_default\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N - 2\n",
    "    Ntest_out = Ntest - 2\n",
    "    seq_length_min = 7\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "if( task == 'repeat copy' ):\n",
    "    no_of_copies = 2\n",
    "    pattern = [0]*(no_of_copies - 1) + [1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = no_of_copies * (N - 2)\n",
    "    Ntest_out = no_of_copies * (Ntest - 2)\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 1\n",
    "if( task == 'pattern 1' ):\n",
    "    pattern = [0,1,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,c,c,d,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = (N - 2) + divmod(N - 2, 2)[0] # N - 2 plus the number of times 2 divides N - 2\n",
    "    Ntest_out = (Ntest - 2) + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 2\n",
    "if( task == 'pattern 2' ):\n",
    "    pattern = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = N - 2 + divmod(N - 2, 2)[0]\n",
    "    Ntest_out = Ntest - 2 + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 3\n",
    "if( task == 'pattern 3' ):\n",
    "    pattern = [0,2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,b,b,d,c,c,e,d,d,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 4 + (N - 2 - 2) * 3\n",
    "    Ntest_out = 4 + (Ntest - 2 - 2) * 3\n",
    "    seq_length_min = 7\n",
    "\n",
    "################\n",
    "# PATTERN TASK 4\n",
    "if( task == 'pattern 4' ):\n",
    "    pattern = [0,2,1,2,-2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,d,f,d,c,c,e,f,h,f,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 7\n",
    "\n",
    "################\n",
    "# PATTERN TASK 5\n",
    "if( task == 'pattern 5' ):\n",
    "    pattern = [4,1,1,-4] # so (a,b,c,d,e,f,...) goes to (a,e,f,g,k,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 7\n",
    "\n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 1\n",
    "if( task == 'mult pattern 1' or task == 'mult pattern 2'):\n",
    "    pattern1 = [1] # so (a,b,c,d,e,f,...) goes to (a,b,c,d,e,f,...)\n",
    "    pattern2 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 7\n",
    "    \n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 2\n",
    "if( task == 'mult pattern 2' ):\n",
    "    # Almost everything is the same as mult pattern 1, but in pattern 2 we \n",
    "    # make sure there is a div symbol somewhere in the sequence\n",
    "    def generate_input_seq_forcediv(max_symbol,input_length):\n",
    "        t = [random.randint(0,max_symbol) for k in range(input_length)]\n",
    "        div_pos = random.randint(0,len(t)-1)\n",
    "        t[div_pos] = div_symbol\n",
    "        return t\n",
    "    \n",
    "    generate_input_seq = generate_input_seq_forcediv\n",
    "\n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 3\n",
    "if( task == 'mult pattern 3'):\n",
    "    pattern1 = [1] # so (a,b,c,d,e,f,...) goes to (a,b,c,d,e,f,...)\n",
    "    pattern2 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    pattern3 = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2,pattern3],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 7\n",
    "    \n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 4\n",
    "if( task == 'mult pattern 4'):\n",
    "    pattern1 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    pattern2 = [2,-1] # so (a,b,c,d,e,f,...) goes to (a,c,b,d,c,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 7\n",
    "    \n",
    "# Give an example input/output pair\n",
    "a = [random.randint(0,num_classes-3) for i in range(N - 2)]\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# INITIALISE STATE #\n",
    "####################\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "input_size = num_classes # dimension of the input space I\n",
    "\n",
    "#####\n",
    "# NTM\n",
    "\n",
    "def init_state_ntm(batch_size, css, mas, mcs):\n",
    "    state_size = css + 2*mas + mas * mcs\n",
    "    \n",
    "    ra = [0.0]*mas\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,mas]) + ra\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    init_read_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_write_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_memory = tf.truncated_normal([batch_size, mas*mcs], 0.0, 1e-6, dtype=tf.float32)\n",
    "    \n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "#############\n",
    "# PATTERN NTM\n",
    "\n",
    "def init_state_pattern_ntm(batch_size, css, mas, mcs):\n",
    "    # mas and mcs are arrays of address sizes and content sizes for rings\n",
    "    state_size = css\n",
    "    \n",
    "    init_address = []\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        state_size = state_size + mas[i] * mcs[i] # for memory vector\n",
    "        state_size = state_size + 2 * mas[i] # for addresses (read and write)\n",
    "    \n",
    "        ra = [0.0]*mas[i]\n",
    "        ra[0] = 1.0\n",
    "        init_address.append(np.zeros([batch_size,mas[i]]) + ra)\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    \n",
    "    tensor_list = [init_controller_state]\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        init_read_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        init_write_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        tensor_list = tensor_list + [init_read_address,init_write_address]\n",
    "        \n",
    "    for i in range(len(mas)):\n",
    "        # The first ring is initialised to zero, the rest differently\n",
    "        if( i == 0 ):\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "        else:\n",
    "            # This initialisation has the result of biasing the output of rings 2 and 3 to be\n",
    "            # \"no rotation\" and biasing ring 4 to say \"use ring 2\"\n",
    "            ra = [0.0]*mcs[i] \n",
    "            ra[0] = memory_init_bias\n",
    "            ra = np.zeros([batch_size,mas[i],mcs[i]]) + ra\n",
    "            ra = tf.constant(ra,dtype=tf.float32,shape=[batch_size,mas[i],mcs[i]])\n",
    "            ra = tf.reshape(ra,[batch_size,mas[i]*mcs[i]])\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32) + ra\n",
    "            #init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "            \n",
    "        tensor_list = tensor_list + [init_memory]\n",
    "    \n",
    "    state = tf.concat(tensor_list,1)\n",
    "\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "######################\n",
    "# MULTIPLE PATTERN NTM\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_25/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_24/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_23/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_22/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_21/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_20/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_19/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_18/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_17/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_15/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_13/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_11/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_9/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_7/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_5/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_3/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_1/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "\n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "read_addresses2 = []\n",
    "read_addresses3 = []\n",
    "read_addresses4 = []\n",
    "write_addresses = []\n",
    "write_addresses2 = []\n",
    "write_addresses3 = []\n",
    "write_addresses4 = []\n",
    "interps = []\n",
    "rnn_outputs = []\n",
    "m2 = []\n",
    "m3 = []\n",
    "m4 = []\n",
    "    \n",
    "for i in range(N + N_out):\n",
    "    \n",
    "    old_state = state\n",
    "\n",
    "    #### RUN MODEL ####\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "    ###################\n",
    "\n",
    "    reuse = True\n",
    "    \n",
    "    #### SET UP NODES FOR LOGGING #####\n",
    "    if( use_model == 'ntm' ):\n",
    "        h0, curr_read, curr_write, _ = tf.split(old_state, [controller_state_size,ntm_memory_address_size,\n",
    "                                                        ntm_memory_address_size,-1], 1)\n",
    "\n",
    "    if( use_model == 'pattern_ntm' ):\n",
    "        mas = pattern_ntm_memory_address_sizes\n",
    "        mcs = pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(old_state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],mas[0] * mcs[0],mas[1] * mcs[1]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        m1_state = ret[5]\n",
    "        m2_state = ret[6]\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm' ):\n",
    "        mas = mult_pattern_ntm_memory_address_sizes\n",
    "        mcs = mult_pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(old_state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],                        \n",
    "                            mas[2],mas[2],mas[3],mas[3],mas[0] * mcs[0],mas[1] * mcs[1],\n",
    "                            mas[2] * mcs[2],mas[3] * mcs[3]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        curr_read3 = ret[5]\n",
    "        curr_write3 = ret[6]\n",
    "        curr_read4 = ret[7]\n",
    "        curr_write4 = ret[8]\n",
    "        m1_state = ret[9]\n",
    "        m2_state = ret[10]\n",
    "        m3_state = ret[11]\n",
    "        m4_state = ret[12]\n",
    "        \n",
    "    read_addresses.append(curr_read[0,:])\n",
    "    write_addresses.append(curr_write[0,:])\n",
    "    \n",
    "    if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses2.append(curr_read2[0,:])\n",
    "        write_addresses2.append(curr_write2[0,:])\n",
    "        m2_state = tf.reshape(m2_state, [-1,mas[1],mcs[1]])\n",
    "        m2.append(tf.nn.softmax(m2_state[0,:]))\n",
    "        \n",
    "        with tf.variable_scope(\"NTM\",reuse=True):\n",
    "            W_interp = tf.get_variable(\"W_interp\", [controller_state_size,1])\n",
    "            B_interp = tf.get_variable(\"B_interp\", [1])\n",
    "            interp = tf.sigmoid(tf.matmul(h0,W_interp) + B_interp)\n",
    "            interp_matrix = tf.concat([interp,tf.ones_like(interp,dtype=tf.float32) - interp],axis=1) # shape [-1,2]\n",
    "            interps.append(interp_matrix[0,:])\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses3.append(curr_read3[0,:])\n",
    "        write_addresses3.append(curr_write3[0,:])\n",
    "        read_addresses4.append(curr_read4[0,:])\n",
    "        write_addresses4.append(curr_write4[0,:])\n",
    "        m3_state = tf.reshape(m3_state, [-1,mult_pattern_ntm_memory_address_sizes[2],mult_pattern_ntm_memory_content_sizes[2]])\n",
    "        m3.append(tf.nn.softmax(m3_state[0,:]))\n",
    "        m4_state = tf.reshape(m4_state, [-1,mult_pattern_ntm_memory_address_sizes[3],mult_pattern_ntm_memory_content_sizes[3]])\n",
    "        m4_state = m4_state[0,:]\n",
    "        m4_state = tf.concat([tf.nn.softmax(m4_state),tf.zeros([mult_pattern_ntm_memory_address_sizes[3],1])],1)\n",
    "        m4.append(m4_state)\n",
    "    ### END LOGGING ###\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# Note: prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "# Note: we use log_softmax to avoid precision issues with floats causing log(0) to create NaNs\n",
    "\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.log_softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * prediction[i]) for i in range(N + N_out)] # an array of numbers\n",
    "\n",
    "# Note: We allow the length of input sequences to vary between batches, which means\n",
    "# that the cross entropy needs to be masked to the relevant part of the output. The\n",
    "# relevant part consists of those positions that are not terminal symbols in the output\n",
    "# of _every_ input sequence in the batch. We detect such positions as follows. First,\n",
    "# we create a tensor term_detector which detects all the positions which are terminal symbols.\n",
    "# term_detector[i] is a boolean tensor which has False for those elements of the batch with\n",
    "# a terminal symbol in the output position i, and True otherwise.\n",
    "\n",
    "term_detector = [tf.not_equal(tf.argmax(targets[i],1),term_symbol) for i in range(N + N_out)]\n",
    "\n",
    "# We then convert False to 0.0 and True to 1.0, and compute the reduce_max, with the result\n",
    "# that mask is 1.0 in position i if and only if there was SOME element of the batch which\n",
    "# did NOT have a terminal symbol in position i\n",
    "\n",
    "mask = [tf.reduce_max(tf.cast(m, tf.float32)) for m in term_detector]\n",
    "ce_mask = [ce[i] * mask[i] for i in range(N + N_out)]\n",
    "cross_entropy = -tf.add_n(ce_mask)\n",
    "cross_entropy /= tf.add_n(mask)\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate,decay=0.9,momentum=0.9)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N + N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "errors_mask = [errors[i] * mask[i] for i in range(N + N_out)]\n",
    "mean_error = tf.add_n(errors_mask)\n",
    "mean_error /= tf.add_n(mask)\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1, mean error - 0.856042\n",
      "Epoch - 2, mean error - 0.760363\n",
      "\n",
      "It took 30 seconds to train.\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "###################\n",
    "# Note on sequences\n",
    "#\n",
    "# Our sequences are of varying length, in the alphabet {0,...,num_classes - 3}.\n",
    "# Each input sequence begins with an initial symbol and ends with a terminal symbol\n",
    "# (the value of which are num_classes - 2 and num_classes - 1 by default).\n",
    "#\n",
    "# Both input and output sequences are written on a \"tape\" of length N + N_out.\n",
    "# Input sequences are aligned at the BEGINNING of the tape, and all remaining space\n",
    "# is filled with terminal symbols. Output sequences are aligned at the END OF THE \n",
    "# MATCHING INPUT, with all remaining space filled with terminal symbols.\n",
    "#\n",
    "# Example: suppose N = N_out = 10, and num_classes = 10 so that init_symbol = 8\n",
    "# and term_symbol = 9. Then a sequence of length 8 (seq_length = 10 below) is\n",
    "#\n",
    "# a = [4, 4, 5, 6, 3, 3, 6, 7]\n",
    "#\n",
    "# which written on the tape is\n",
    "#\n",
    "# [8, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
    "#\n",
    "# If we are performing the copy task, so that the output sequence is also a, then\n",
    "# the output written on the tape will be (notice the alignment)\n",
    "#\n",
    "# [9, 9, 9, 9, 9, 9, 9, 9, 9, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9]\n",
    "#\n",
    "\n",
    "def io_generator(max_symbol, input_length, total_length):\n",
    "    \"\"\"\n",
    "    Returns a one-hot encoded pair of input and output sequence, with terminal and initial symbols.\n",
    "    \n",
    "    max_symbol - generate sequences in 0,...,max_symbol\n",
    "    input_length - length of input sequences, without initial and terminal symbols\n",
    "    total_length - length of the buffer, so that the sequences are padded to this length\n",
    "    \"\"\"\n",
    "    a = generate_input_seq(max_symbol,input_length)\n",
    "    fa = func_to_learn(a)\n",
    "    a = [init_symbol] + a + [term_symbol]\n",
    "    a = a + [term_symbol for k in range(total_length-len(a))]\n",
    "    a_onehot = [one_hots[e] for e in a]\n",
    "    \n",
    "    # If the output is too long to fit in the buffer, truncate it\n",
    "    if( len(fa) + input_length + 1 > total_length ):\n",
    "        fa = fa[:total_length-input_length-1]\n",
    "        \n",
    "    fa = [term_symbol for k in range(input_length+1)] + fa + \\\n",
    "                [term_symbol for k in range(total_length-(input_length+1)-len(fa))]\n",
    "    fa_onehot = [one_hots[e] for e in fa]\n",
    "    \n",
    "    return a, fa, np.array(a_onehot), np.array(fa_onehot)\n",
    "\n",
    "error_means = []\n",
    "epoch_error_means = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences. Each\n",
    "        # batch has a fixed length of the sequences. Recall that all input seqs\n",
    "        # have an initial and terminal symbol, so if seq_length = 10 then there\n",
    "        # are eight positions for the \"content\" symbols\n",
    "        \n",
    "        # Our version of curriculum training says: spend the first half\n",
    "        # of the epochs ramping up to the full training set. Assuming that\n",
    "        # epoch > N we divide allocate each integer in [seq_length_min,N]\n",
    "        # an equal portion of the first half of the epochs.\n",
    "        if( use_curriculum == True ):\n",
    "            if( 2 * i > epoch ):\n",
    "                seq_length_max = N\n",
    "            else:\n",
    "                curriculum_band = max(1,int(epoch/(2*(N - seq_length_min))))\n",
    "                seq_length_max = min(seq_length_min + int(i/curriculum_band),N)\n",
    "        else:\n",
    "            seq_length_max = N\n",
    "            \n",
    "        seq_length = random.randint(seq_length_min,seq_length_max)\n",
    "        \n",
    "        # Hack: if we are on the final batch of the final epoch, force\n",
    "        # it to use the full sequence length, so we get a good visualisation\n",
    "        if( i + 1 == epoch and j + 1 == no_of_batches ):\n",
    "            seq_length = N\n",
    "        \n",
    "        for z in range(batch_size):\n",
    "            a, fa, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=N+N_out)\n",
    "            \n",
    "            inp.append(a_onehot)\n",
    "            out.append(fa_onehot)\n",
    "            \n",
    "            # Record the first sequence in the last batch of the last epoch\n",
    "            if( i == epoch - 1 and j == no_of_batches - 1 and z == 0):\n",
    "                final_seq = a\n",
    "                final_seq_output = fa\n",
    "        \n",
    "        # An annoying thing here is that we cannot use a list as a key in a \n",
    "        # dictionary. The workaround we found on StackOverflow here:\n",
    "        # http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "        feed_dict = {}\n",
    "        \n",
    "        for d in range(N + N_out):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N + N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "\n",
    "        ##### Do gradient descent #####\n",
    "        mean_error_val,_ = sess.run([mean_error,minimize], feed_dict)\n",
    "        ###############################\n",
    "        \n",
    "        error_means.append(mean_error_val)\n",
    "    \n",
    "    epoch_error = np.mean(error_means[-no_of_batches:])\n",
    "    epoch_error_means.append(epoch_error)\n",
    "    \n",
    "    # Print the mean error of the final batch in the epoch\n",
    "    print_str = \"Epoch - \" + str(i+1) + \", mean error - \" + str(epoch_error)\n",
    "    \n",
    "    if( use_curriculum == True ):\n",
    "        print_str = print_str + \", training at max length - \" + str(seq_length_max)\n",
    "        \n",
    "    print(print_str)\n",
    "\n",
    "# For the final batch of the final epoch, we record the memory states as well\n",
    "seq_length_for_vis = seq_length - 2\n",
    "interps_val = sess.run(interps,feed_dict)\n",
    "m2_val, m3_val, m4_val = sess.run([m2,m3,m4],feed_dict)            \n",
    "r1_val, w1_val = sess.run([read_addresses,write_addresses],feed_dict)\n",
    "r2_val, w2_val = sess.run([read_addresses2,write_addresses2],feed_dict)\n",
    "r3_val, w3_val = sess.run([read_addresses3,write_addresses3],feed_dict)\n",
    "r4_val, w4_val = sess.run([read_addresses4,write_addresses4],feed_dict)\n",
    "errors_mask_val = sess.run(errors_mask,feed_dict)\n",
    "\n",
    "mask_val = sess.run(tf.cast(mask,tf.int64),feed_dict)\n",
    "predicted_seq = [tf.argmax(prediction[i], 1) for i in range(N + N_out)]\n",
    "predicted_seq_val = sess.run(predicted_seq,feed_dict)\n",
    "final_seq_pred_0 = [a[0] for a in predicted_seq_val]\n",
    "final_seq_pred = []\n",
    "\n",
    "for i in range(len(mask_val)):\n",
    "    if( mask_val[i] == 1.0 ):\n",
    "        final_seq_pred.append(final_seq_pred_0[i])\n",
    "    else:\n",
    "        final_seq_pred.append(9)\n",
    "\n",
    "# Write out variables to disk\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took \" + str(int(time.time() - pre_train_time)) + \" seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length used for visualisations - 8\n",
      "\n",
      "Sequence used for visualisations is (Note: initial symbol is 8, terminal symbol is 9)\n",
      "[8, 4, 7, 1, 2, 4, 6, 3, 1, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "\n",
      "Correct output for this sequence:\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 4, 4, 1, 4, 2, 6, 4, 3, 6, 1, 3, 9, 9, 9, 9, 9, 9]\n",
      "\n",
      "Predicted output for this sequence\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 4, 3, 1, 1, 1, 1, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "\n",
      "Mask for output\n",
      "[0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
      "\n",
      "Error probabilities for final batch\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.86400002, 0.86799997, 0.83999997, 0.81599998, 0.82800001, 0.92000002, 0.93199998, 0.94800001, 0.92799997, 0.83999997, 0.83999997, 0.51200002, 0.47999999, 0.43200001, 0.38800001, 0.38800001, 0.0]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAAL8CAYAAAC2zccJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4jdv4TcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a58f9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 1 #\n",
    "###########################\n",
    "\n",
    "print(\"Sequence length used for visualisations - \" + str(seq_length_for_vis))\n",
    "print(\"\")\n",
    "print(\"Sequence used for visualisations is (Note: initial symbol is \" + str(init_symbol) + \", terminal symbol is \" + str(term_symbol) + \")\")\n",
    "print(final_seq)\n",
    "print(\"\")\n",
    "print(\"Correct output for this sequence:\")\n",
    "print(final_seq_output)\n",
    "print(\"\")\n",
    "print(\"Predicted output for this sequence\")\n",
    "print(final_seq_pred)\n",
    "print(\"\")\n",
    "print(\"Mask for output\")\n",
    "print(mask_val)\n",
    "print(\"\")\n",
    "print(\"Error probabilities for final batch\")\n",
    "print(errors_mask_val)\n",
    "print(\"\")\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 9, 13\n",
    "fig_num = 0\n",
    "\n",
    "# RING 1\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "plt.figure(fig_num)\n",
    "ax1 = plt.subplot(2,1,1)\n",
    "ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "ax1.imshow(np.stack(w1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax1.set_title('Write address (ring 1)')\n",
    "ax1.set_xlabel('position')\n",
    "ax1.set_ylabel('time')\n",
    "\n",
    "ax2.imshow(np.stack(r1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax2.set_title('Read address (ring 1)')\n",
    "ax2.set_xlabel('position')\n",
    "ax2.set_ylabel('time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 2 #\n",
    "###########################\n",
    "\n",
    "if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 2)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 2)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Assume that powers2_on_1 has three entries we can use as colour channels\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 2)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    max_xticks = 2\n",
    "    xloc = plt.MaxNLocator(max_xticks)\n",
    "\n",
    "    ax.imshow(np.stack(interps_val), cmap='bone', interpolation='nearest', aspect='auto')\n",
    "    ax.set_title('Interpolation')\n",
    "    ax.set_xlabel('direct vs indirect')\n",
    "    ax.set_ylabel('time')\n",
    "    ax.xaxis.set_major_locator(xloc)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# VISUALISATIONS - OTHER RINGS #\n",
    "################################\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 3)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 3)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 3)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 4)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 4)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax6 = plt.subplot(1,1,1)    \n",
    "    ax6.imshow(np.stack(m4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax6.set_title('Memory contents (ring 4)')\n",
    "    ax6.set_xlabel('position')\n",
    "    ax6.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3kAAAUKCAYAAABblriAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x152efec90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########################\n",
    "# VISUALISATIONS - ERROR #\n",
    "##########################\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "\n",
    "plt.figure(fig_num)\n",
    "ax = plt.subplot(1,1,1)\n",
    "sc = pandas.Series(error_means)\n",
    "ma = sc.rolling(window=500).mean()\n",
    "ax.plot(sc.index, sc, color='lightgray')\n",
    "ax.plot(ma.index, ma, color='red')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(sc.index.min(), sc.index.max())\n",
    "ax.set_title('Error')\n",
    "ax.set_xlabel('batch')\n",
    "ax.set_ylabel('error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on sequences of length 13\n",
      "\n",
      "Batch - 1, Mean error - 0.742615\n",
      "Batch - 2, Mean error - 0.770462\n",
      "Batch - 3, Mean error - 0.756923\n",
      "Batch - 4, Mean error - 0.756615\n",
      "\n",
      "###########\n",
      "# Summary #\n",
      "###########\n",
      "\n",
      "model         - ntm\n",
      "task name     - mult pattern 4\n",
      "epochs        - 2\n",
      "num_classes   - 10\n",
      "N             - 10\n",
      "Ntest         - 15\n",
      "# weights     - 18958\n",
      "\n",
      "\n",
      "error train(test) - 0.760363 (0.756654)\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "# Restore the weights from training\n",
    "sess = tf.Session()\n",
    "saver.restore(sess,save_path)\n",
    "\n",
    "inputs_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "targets_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "# Set up test graph\n",
    "rnn_outputs_test = []\n",
    "reuse = True\n",
    "for i in range(Ntest + Ntest_out):\n",
    "    output, state = cell(inputs_test[i],state,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "\n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size])\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.log_softmax(logit) for logit in logits_test] \n",
    "term_detector = [tf.not_equal(tf.argmax(targets_test[i],1),term_symbol) for i in range(Ntest + Ntest_out)]\n",
    "mask = [tf.reduce_max(tf.cast(m, tf.float32)) for m in term_detector]\n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets_test[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest + Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "errors_test_mask = [errors_test[i] * mask[i] for i in range(Ntest + Ntest_out)]\n",
    "mean_error_test = tf.add_n(errors_test_mask)\n",
    "mean_error_test /= tf.add_n(mask)\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "\n",
    "seq_length = Ntest\n",
    "print(\"Testing on sequences of length \" + str(seq_length-2))\n",
    "print(\"\")\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    for z in range(batch_size):\n",
    "        a, fa, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=Ntest+Ntest_out)\n",
    "            \n",
    "        inp.append(a_onehot)\n",
    "        out.append(fa_onehot)        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        in_node = inputs_test[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        out_node = targets_test[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = sess.run(mean_error_test, feed_dict)\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"epochs        - \" + str(epoch))\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error train(test) - \" + str(epoch_error_means[-1]) + \" (\" + str(final_error) + \")\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
