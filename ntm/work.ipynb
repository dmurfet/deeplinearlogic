{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of the Linear Logic Recurrent Neural Network (LLRNN)\n",
    "#\n",
    "# Version 10.0\n",
    "\n",
    "###################\n",
    "# HYPERPARAMETERS #\n",
    "###################\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, mult_pattern_ntm\n",
    "task                  = 'copy' # copy, repeat copy, pattern i, mult pattern i\n",
    "epoch                 = 100 # number of training epochs, default to 100\n",
    "num_classes           = 10 # number of symbols, INCLUDING initial and terminal symbols, default 10\n",
    "N                     = 30 # length of input sequences for training, default to 30\n",
    "Ntest                 = 35 # length of sequences for testing, default to 35\n",
    "batch_size            = 250 # default 250\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "num_training          = 10000 # default 10000\n",
    "num_test              = num_training\n",
    "term_symbol           = num_classes - 1\n",
    "init_symbol           = num_classes - 2\n",
    "div_symbol            = num_classes - 3\n",
    "learning_rate         = 1e-4 # default 1e-4\n",
    "memory_init_bias      = 1.0 # default 1.0\n",
    "use_curriculum        = True # default True\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "\n",
    "##################\n",
    "# MODEL SPECIFIC #\n",
    "##################\n",
    "\n",
    "ntm_memory_address_size   = 128 # number of memory locations, default 128\n",
    "ntm_memory_content_size   = 20 # size of vector stored at a memory location, default 20\n",
    "ntm_powers                = [0,-1,1] # powers of R used by controller, default [0,-1,1]\n",
    "\n",
    "pattern_ntm_powers               = [[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "pattern_ntm_powers_2_on_1        = [0,1,2] # allowed powers used by ring 2 to manipulate ring 1\n",
    "pattern_ntm_memory_address_sizes = [128, 1] # number of memory locations for the three rings\n",
    "pattern_ntm_memory_content_sizes = [20, 3] # size of content vector for each ring\n",
    "pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "mult_pattern_ntm_powers               = [[0,-1,1],[0,-1,1],[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "mult_pattern_ntm_powers_2_on_1        = [0,1,2] # allowed powers used by rings 2,3 to manipulate ring 1\n",
    "mult_pattern_ntm_memory_address_sizes = [128, 20, 20, 10] # number of memory locations for the rings\n",
    "mult_pattern_ntm_memory_content_sizes = [20, 3, 3, 2] # size of content vector for each ring\n",
    "mult_pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there\n",
    "# 2. The initial and terminal symbols are always from the end of the list of symbols, so they\n",
    "# are respectively num_classes - 2 and num_classes - 1. So the number of symbols which are\n",
    "# not initial or terminal is num_classes - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import pandas\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs\n",
    "\n",
    "assert use_model == 'ntm' or use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[3, 5, 5, 0, 5, 2, 7, 6]\n",
      "is mapped to\n",
      "[3, 5, 5, 0, 5, 2, 7, 6]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "# Below N_out and Ntest_out are the lengths of the outputs in both the training\n",
    "# and testing regimes respectively. Since outputs do not include the initial and terminal\n",
    "# symbols, these default to N - 2 and Ntest - 2 respectively.\n",
    "\n",
    "# Default sampling from space of inputs\n",
    "def generate_input_seq_default(max_symbol,input_length):\n",
    "    return [random.randint(0,max_symbol) for k in range(input_length)]\n",
    "\n",
    "generate_input_seq = generate_input_seq_default\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N - 2\n",
    "    Ntest_out = Ntest - 2\n",
    "    seq_length_min = 7\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "if( task == 'repeat copy' ):\n",
    "    no_of_copies = 2\n",
    "    pattern = [0]*(no_of_copies - 1) + [1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = no_of_copies * (N - 2)\n",
    "    Ntest_out = no_of_copies * (Ntest - 2)\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 1\n",
    "if( task == 'pattern 1' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,1,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,c,c,d,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = (N - 2) + divmod(N - 2, 2)[0] # N - 2 plus the number of times 2 divides N - 2\n",
    "    Ntest_out = (Ntest - 2) + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 2\n",
    "if( task == 'pattern 2' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = N - 2 + divmod(N - 2, 2)[0]\n",
    "    Ntest_out = Ntest - 2 + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 3\n",
    "if( task == 'pattern 3' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,b,b,d,c,c,e,d,d,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 4 + (N - 2 - 2) * 3\n",
    "    Ntest_out = 4 + (Ntest - 2 - 2) * 3\n",
    "    seq_length_min = 7\n",
    "\n",
    "################\n",
    "# PATTERN TASK 4\n",
    "if( task == 'pattern 4' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2,1,2,-2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,d,f,d,c,c,e,f,h,f,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 7\n",
    "\n",
    "################\n",
    "# PATTERN TASK 5\n",
    "if( task == 'pattern 5' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [4,1,1,-4] # so (a,b,c,d,e,f,...) goes to (a,e,f,g,k,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 7\n",
    "\n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 1\n",
    "if( task == 'mult pattern 1' or task == 'mult pattern 2'):\n",
    "    pattern1 = [1] # so (a,b,c,d,e,f,...) goes to (a,b,c,d,e,f,...)\n",
    "    pattern2 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 7\n",
    "    \n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 2\n",
    "if( task == 'mult pattern 2' ):\n",
    "    # Almost everything is the same as mult pattern 1, but in pattern 2 we \n",
    "    # make sure there is a div symbol somewhere in the sequence\n",
    "    def generate_input_seq_forcediv(max_symbol,input_length):\n",
    "        t = [random.randint(0,max_symbol) for k in range(input_length)]\n",
    "        div_pos = random.randint(0,len(t)-1)\n",
    "        t[div_pos] = div_symbol\n",
    "        return t\n",
    "    \n",
    "    generate_input_seq = generate_input_seq_forcediv\n",
    "\n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 3\n",
    "if( task == 'mult pattern 3'):\n",
    "    pattern1 = [1] # so (a,b,c,d,e,f,...) goes to (a,b,c,d,e,f,...)\n",
    "    pattern2 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    pattern3 = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2,pattern3],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 7\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = [random.randint(0,num_classes-3) for i in range(N - 2)]\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# INITIALISE STATE #\n",
    "####################\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "input_size = num_classes # dimension of the input space I\n",
    "\n",
    "#####\n",
    "# NTM\n",
    "\n",
    "def init_state_ntm(batch_size, css, mas, mcs):\n",
    "    state_size = css + 2*mas + mas * mcs\n",
    "    \n",
    "    ra = [0.0]*mas\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,mas]) + ra\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    init_read_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_write_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_memory = tf.truncated_normal([batch_size, mas*mcs], 0.0, 1e-6, dtype=tf.float32)\n",
    "    \n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "#############\n",
    "# PATTERN NTM\n",
    "\n",
    "def init_state_pattern_ntm(batch_size, css, mas, mcs):\n",
    "    # mas and mcs are arrays of address sizes and content sizes for rings\n",
    "    state_size = css\n",
    "    \n",
    "    init_address = []\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        state_size = state_size + mas[i] * mcs[i] # for memory vector\n",
    "        state_size = state_size + 2 * mas[i] # for addresses (read and write)\n",
    "    \n",
    "        ra = [0.0]*mas[i]\n",
    "        ra[0] = 1.0\n",
    "        init_address.append(np.zeros([batch_size,mas[i]]) + ra)\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    \n",
    "    tensor_list = [init_controller_state]\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        init_read_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        init_write_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        tensor_list = tensor_list + [init_read_address,init_write_address]\n",
    "        \n",
    "    for i in range(len(mas)):\n",
    "        # The first ring is initialised to zero, the rest differently\n",
    "        if( i == 0 ):\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "        else:\n",
    "            # This initialisation has the result of biasing the output of rings 2 and 3 to be\n",
    "            # \"no rotation\" and biasing ring 4 to say \"use ring 2\"\n",
    "            ra = [0.0]*mcs[i] \n",
    "            ra[0] = memory_init_bias\n",
    "            ra = np.zeros([batch_size,mas[i],mcs[i]]) + ra\n",
    "            ra = tf.constant(ra,dtype=tf.float32,shape=[batch_size,mas[i],mcs[i]])\n",
    "            ra = tf.reshape(ra,[batch_size,mas[i]*mcs[i]])\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32) + ra\n",
    "            #init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "            \n",
    "        tensor_list = tensor_list + [init_memory]\n",
    "    \n",
    "    state = tf.concat(tensor_list,1)\n",
    "\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "######################\n",
    "# MULTIPLE PATTERN NTM\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_17/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_15/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_13/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_11/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_9/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_7/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_5/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_3/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_1/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "\n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "read_addresses2 = []\n",
    "read_addresses3 = []\n",
    "read_addresses4 = []\n",
    "write_addresses = []\n",
    "write_addresses2 = []\n",
    "write_addresses3 = []\n",
    "write_addresses4 = []\n",
    "interps = []\n",
    "rnn_outputs = []\n",
    "m2 = []\n",
    "m3 = []\n",
    "m4 = []\n",
    "    \n",
    "for i in range(N + N_out):\n",
    "    \n",
    "    old_state = state\n",
    "\n",
    "    #### RUN MODEL ####\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "    ###################\n",
    "\n",
    "    reuse = True\n",
    "    \n",
    "    #### SET UP NODES FOR LOGGING #####\n",
    "    if( use_model == 'ntm' ):\n",
    "        h0, curr_read, curr_write, _ = tf.split(old_state, [controller_state_size,ntm_memory_address_size,\n",
    "                                                        ntm_memory_address_size,-1], 1)\n",
    "\n",
    "    if( use_model == 'pattern_ntm' ):\n",
    "        mas = pattern_ntm_memory_address_sizes\n",
    "        mcs = pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(old_state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],mas[0] * mcs[0],mas[1] * mcs[1]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        m1_state = ret[5]\n",
    "        m2_state = ret[6]\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm' ):\n",
    "        mas = mult_pattern_ntm_memory_address_sizes\n",
    "        mcs = mult_pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(old_state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],                        \n",
    "                            mas[2],mas[2],mas[3],mas[3],mas[0] * mcs[0],mas[1] * mcs[1],\n",
    "                            mas[2] * mcs[2],mas[3] * mcs[3]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        curr_read3 = ret[5]\n",
    "        curr_write3 = ret[6]\n",
    "        curr_read4 = ret[7]\n",
    "        curr_write4 = ret[8]\n",
    "        m1_state = ret[9]\n",
    "        m2_state = ret[10]\n",
    "        m3_state = ret[11]\n",
    "        m4_state = ret[12]\n",
    "        \n",
    "    read_addresses.append(curr_read[0,:])\n",
    "    write_addresses.append(curr_write[0,:])\n",
    "    \n",
    "    if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses2.append(curr_read2[0,:])\n",
    "        write_addresses2.append(curr_write2[0,:])\n",
    "        m2_state = tf.reshape(m2_state, [-1,mas[1],mcs[1]])\n",
    "        m2.append(tf.nn.softmax(m2_state[0,:]))\n",
    "        \n",
    "        with tf.variable_scope(\"NTM\",reuse=True):\n",
    "            W_interp = tf.get_variable(\"W_interp\", [controller_state_size,1])\n",
    "            B_interp = tf.get_variable(\"B_interp\", [1])\n",
    "            interp = tf.sigmoid(tf.matmul(h0,W_interp) + B_interp)\n",
    "            interp_matrix = tf.concat([interp,tf.ones_like(interp,dtype=tf.float32) - interp],axis=1) # shape [-1,2]\n",
    "            interps.append(interp_matrix[0,:])\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses3.append(curr_read3[0,:])\n",
    "        write_addresses3.append(curr_write3[0,:])\n",
    "        read_addresses4.append(curr_read4[0,:])\n",
    "        write_addresses4.append(curr_write4[0,:])\n",
    "        m3_state = tf.reshape(m3_state, [-1,mult_pattern_ntm_memory_address_sizes[2],mult_pattern_ntm_memory_content_sizes[2]])\n",
    "        m3.append(tf.nn.softmax(m3_state[0,:]))\n",
    "        m4_state = tf.reshape(m4_state, [-1,mult_pattern_ntm_memory_address_sizes[3],mult_pattern_ntm_memory_content_sizes[3]])\n",
    "        m4_state = m4_state[0,:]\n",
    "        m4_state = tf.concat([tf.nn.softmax(m4_state),tf.zeros([mult_pattern_ntm_memory_address_sizes[3],1])],1)\n",
    "        m4.append(m4_state)\n",
    "    ### END LOGGING ###\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# Note: prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "# Note: we use log_softmax to avoid precision issues with floats causing log(0) to create NaNs\n",
    "\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.log_softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * prediction[i]) for i in range(N + N_out)] # an array of numbers\n",
    "\n",
    "# Note: We allow the length of input sequences to vary between batches, which means\n",
    "# that the cross entropy needs to be masked to the relevant part of the output. The\n",
    "# relevant part consists of those positions that are not terminal symbols in the output\n",
    "# of _every_ input sequence in the batch. We detect such positions as follows. First,\n",
    "# we create a tensor term_detector which detects all the positions which are terminal symbols.\n",
    "# term_detector[i] is a boolean tensor which has False for those elements of the batch with\n",
    "# a terminal symbol in the output position i, and True otherwise.\n",
    "\n",
    "term_detector = [tf.not_equal(tf.argmax(targets[i],1),term_symbol) for i in range(N + N_out)]\n",
    "\n",
    "# We then convert False to 0.0 and True to 1.0, and compute the reduce_max, with the result\n",
    "# that mask is 1.0 in position i if and only if there was SOME element of the batch which\n",
    "# did NOT have a terminal symbol in position i\n",
    "\n",
    "mask = [tf.reduce_max(tf.cast(m, tf.float32)) for m in term_detector]\n",
    "ce_mask = [ce[i] * mask[i] for i in range(N + N_out)]\n",
    "cross_entropy = -tf.add_n(ce_mask)\n",
    "cross_entropy /= tf.add_n(mask)\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate,decay=0.9,momentum=0.9)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N + N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "errors_mask = [errors[i] * mask[i] for i in range(N + N_out)]\n",
    "mean_error = tf.add_n(errors_mask)\n",
    "mean_error /= tf.add_n(mask)\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1, mean error - 0.871271\n",
      "Epoch - 2, mean error - 0.826775\n",
      "\n",
      "It took 22 seconds to train.\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "###################\n",
    "# Note on sequences\n",
    "#\n",
    "# Our sequences are of varying length, in the alphabet {0,...,num_classes - 3}.\n",
    "# Each input sequence begins with an initial symbol and ends with a terminal symbol\n",
    "# (the value of which are num_classes - 2 and num_classes - 1 by default).\n",
    "#\n",
    "# Both input and output sequences are written on a \"tape\" of length N + N_out.\n",
    "# Input sequences are aligned at the BEGINNING of the tape, and all remaining space\n",
    "# is filled with terminal symbols. Output sequences are aligned at the END OF THE \n",
    "# MATCHING INPUT, with all remaining space filled with terminal symbols.\n",
    "#\n",
    "# Example: suppose N = N_out = 10, and num_classes = 10 so that init_symbol = 8\n",
    "# and term_symbol = 9. Then a sequence of length 8 (seq_length = 10 below) is\n",
    "#\n",
    "# a = [4, 4, 5, 6, 3, 3, 6, 7]\n",
    "#\n",
    "# which written on the tape is\n",
    "#\n",
    "# [8, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
    "#\n",
    "# If we are performing the copy task, so that the output sequence is also a, then\n",
    "# the output written on the tape will be (notice the alignment)\n",
    "#\n",
    "# [9, 9, 9, 9, 9, 9, 9, 9, 9, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9]\n",
    "#\n",
    "\n",
    "def io_generator(max_symbol, input_length, total_length):\n",
    "    \"\"\"\n",
    "    Returns a one-hot encoded pair of input and output sequence, with terminal and initial symbols.\n",
    "    \n",
    "    max_symbol - generate sequences in 0,...,max_symbol\n",
    "    input_length - length of input sequences, without initial and terminal symbols\n",
    "    total_length - length of the buffer, so that the sequences are padded to this length\n",
    "    \"\"\"\n",
    "    a = generate_input_seq(max_symbol,input_length)\n",
    "    fa = func_to_learn(a)\n",
    "    a = [init_symbol] + a + [term_symbol]\n",
    "    a = a + [term_symbol for k in range(total_length-len(a))]\n",
    "    a_onehot = [one_hots[e] for e in a]\n",
    "    fa = [term_symbol for k in range(input_length+1)] + fa + \\\n",
    "                [term_symbol for k in range(total_length-(input_length+1)-len(fa))]\n",
    "    fa_onehot = [one_hots[e] for e in fa]\n",
    "    \n",
    "    return a, fa, np.array(a_onehot), np.array(fa_onehot)\n",
    "\n",
    "error_means = []\n",
    "epoch_error_means = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences. Each\n",
    "        # batch has a fixed length of the sequences. Recall that all input seqs\n",
    "        # have an initial and terminal symbol, so if seq_length = 10 then there\n",
    "        # are eight positions for the \"content\" symbols\n",
    "        \n",
    "        # Our version of curriculum training says: spend the first half\n",
    "        # of the epochs ramping up to the full training set. Assuming that\n",
    "        # epoch > N we divide allocate each integer in [seq_length_min,N]\n",
    "        # an equal portion of the first half of the epochs.\n",
    "        if( use_curriculum == True ):\n",
    "            if( 2 * i > epoch ):\n",
    "                seq_length_max = N\n",
    "            else:\n",
    "                curriculum_band = max(1,int(epoch/(2*(N - seq_length_min))))\n",
    "                seq_length_max = min(seq_length_min + int(i/curriculum_band),N)\n",
    "        else:\n",
    "            seq_length_max = N\n",
    "            \n",
    "        seq_length = random.randint(seq_length_min,seq_length_max)\n",
    "        \n",
    "        # Hack: if we are on the final batch of the final epoch, force\n",
    "        # it to use the full sequence length, so we get a good visualisation\n",
    "        if( i + 1 == epoch and j + 1 == no_of_batches ):\n",
    "            seq_length = N\n",
    "        \n",
    "        for z in range(batch_size):\n",
    "            a, fa, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=N+N_out)\n",
    "            \n",
    "            inp.append(a_onehot)\n",
    "            out.append(fa_onehot)\n",
    "            \n",
    "            # Record the first sequence in the last batch of the last epoch\n",
    "            if( i == epoch - 1 and j == no_of_batches - 1 and z == 0):\n",
    "                final_seq = a\n",
    "                final_seq_output = fa\n",
    "        \n",
    "        # An annoying thing here is that we cannot use a list as a key in a \n",
    "        # dictionary. The workaround we found on StackOverflow here:\n",
    "        # http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "        feed_dict = {}\n",
    "        \n",
    "        for d in range(N + N_out):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N + N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "\n",
    "        ##### Do gradient descent #####\n",
    "        mean_error_val,_ = sess.run([mean_error,minimize], feed_dict)\n",
    "        ###############################\n",
    "        \n",
    "        error_means.append(mean_error_val)\n",
    "    \n",
    "    epoch_error = np.mean(error_means[-no_of_batches:])\n",
    "    epoch_error_means.append(epoch_error)\n",
    "    \n",
    "    # Print the mean error of the final batch in the epoch\n",
    "    print_str = \"Epoch - \" + str(i+1) + \", mean error - \" + str(epoch_error)\n",
    "    \n",
    "    if( use_curriculum == True ):\n",
    "        print_str = print_str + \", training at max length - \" + str(seq_length_max)\n",
    "        \n",
    "    print(print_str)\n",
    "\n",
    "# For the final batch of the final epoch, we record the memory states as well\n",
    "seq_length_for_vis = seq_length - 2\n",
    "interps_val = sess.run(interps,feed_dict)\n",
    "m2_val, m3_val, m4_val = sess.run([m2,m3,m4],feed_dict)            \n",
    "r1_val, w1_val = sess.run([read_addresses,write_addresses],feed_dict)\n",
    "r2_val, w2_val = sess.run([read_addresses2,write_addresses2],feed_dict)\n",
    "r3_val, w3_val = sess.run([read_addresses3,write_addresses3],feed_dict)\n",
    "r4_val, w4_val = sess.run([read_addresses4,write_addresses4],feed_dict)\n",
    "errors_mask_val = sess.run(errors_mask,feed_dict)\n",
    "\n",
    "mask_val = sess.run(tf.cast(mask,tf.int64),feed_dict)\n",
    "predicted_seq = [tf.argmax(prediction[i], 1) for i in range(N + N_out)]\n",
    "predicted_seq_val = sess.run(predicted_seq,feed_dict)\n",
    "final_seq_pred_0 = [a[0] for a in predicted_seq_val]\n",
    "final_seq_pred = []\n",
    "\n",
    "for i in range(len(mask_val)):\n",
    "    if( mask_val[i] == 1.0 ):\n",
    "        final_seq_pred.append(final_seq_pred_0[i])\n",
    "    else:\n",
    "        final_seq_pred.append(9)\n",
    "\n",
    "# Write out variables to disk\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took \" + str(int(time.time() - pre_train_time)) + \" seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length used for visualisations - 8\n",
      "\n",
      "Sequence used for visualisations is (Note: initial symbol is 8, terminal symbol is 9)\n",
      "[8, 0, 4, 2, 4, 1, 5, 1, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "\n",
      "Correct output for this sequence:\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 0, 4, 2, 4, 1, 5, 1, 6, 9]\n",
      "\n",
      "Predicted output for this sequence\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 4, 4, 6, 4, 6, 6, 6, 6, 9]\n",
      "\n",
      "Mask for output\n",
      "[0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0]\n",
      "\n",
      "Error probabilities for final batch\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78399998, 0.796, 0.76800001, 0.78799999, 0.74400002, 0.75599998, 0.84799999, 0.80400002, 0.0]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAAKyCAYAAACDn6wVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a694b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 1 #\n",
    "###########################\n",
    "\n",
    "print(\"Sequence length used for visualisations - \" + str(seq_length_for_vis))\n",
    "print(\"\")\n",
    "print(\"Sequence used for visualisations is (Note: initial symbol is \" + str(init_symbol) + \", terminal symbol is \" + str(term_symbol) + \")\")\n",
    "print(final_seq)\n",
    "print(\"\")\n",
    "print(\"Correct output for this sequence:\")\n",
    "print(final_seq_output)\n",
    "print(\"\")\n",
    "print(\"Predicted output for this sequence\")\n",
    "print(final_seq_pred)\n",
    "print(\"\")\n",
    "print(\"Mask for output\")\n",
    "print(mask_val)\n",
    "print(\"\")\n",
    "print(\"Error probabilities for final batch\")\n",
    "print(errors_mask_val)\n",
    "print(\"\")\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 9, 13\n",
    "fig_num = 0\n",
    "\n",
    "# RING 1\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "plt.figure(fig_num)\n",
    "ax1 = plt.subplot(2,1,1)\n",
    "ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "ax1.imshow(np.stack(w1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax1.set_title('Write address (ring 1)')\n",
    "ax1.set_xlabel('position')\n",
    "ax1.set_ylabel('time')\n",
    "\n",
    "ax2.imshow(np.stack(r1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax2.set_title('Read address (ring 1)')\n",
    "ax2.set_xlabel('position')\n",
    "ax2.set_ylabel('time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 2 #\n",
    "###########################\n",
    "\n",
    "if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 2)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 2)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Assume that powers2_on_1 has three entries we can use as colour channels\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 2)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    max_xticks = 2\n",
    "    xloc = plt.MaxNLocator(max_xticks)\n",
    "\n",
    "    ax.imshow(np.stack(interps_val), cmap='bone', interpolation='nearest', aspect='auto')\n",
    "    ax.set_title('Interpolation')\n",
    "    ax.set_xlabel('direct vs indirect')\n",
    "    ax.set_ylabel('time')\n",
    "    ax.xaxis.set_major_locator(xloc)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# VISUALISATIONS - OTHER RINGS #\n",
    "################################\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 3)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 3)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 3)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 4)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 4)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax6 = plt.subplot(1,1,1)    \n",
    "    ax6.imshow(np.stack(m4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax6.set_title('Memory contents (ring 4)')\n",
    "    ax6.set_xlabel('position')\n",
    "    ax6.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3kAAAUKCAYAAABblriAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niMgDAABoROQBAAA08r/nQT77TU830wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14484d750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########################\n",
    "# VISUALISATIONS - ERROR #\n",
    "##########################\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "\n",
    "plt.figure(fig_num)\n",
    "ax = plt.subplot(1,1,1)\n",
    "sc = pandas.Series(error_means)\n",
    "ma = sc.rolling(window=500).mean()\n",
    "ax.plot(sc.index, sc, color='lightgray')\n",
    "ax.plot(ma.index, ma, color='red')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(sc.index.min(), sc.index.max())\n",
    "ax.set_title('Error')\n",
    "ax.set_xlabel('batch')\n",
    "ax.set_ylabel('error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on sequences of length 13\n",
      "\n",
      "Batch - 1, Mean error - 0.853539\n",
      "Batch - 2, Mean error - 0.845846\n",
      "Batch - 3, Mean error - 0.841846\n",
      "Batch - 4, Mean error - 0.841846\n",
      "\n",
      "###########\n",
      "# Summary #\n",
      "###########\n",
      "\n",
      "model         - ntm\n",
      "task name     - copy\n",
      "epochs        - 2\n",
      "num_classes   - 10\n",
      "N             - 10\n",
      "Ntest         - 15\n",
      "# weights     - 18958\n",
      "\n",
      "\n",
      "error train(test) - 0.826775 (0.845769)\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "# Restore the weights from training\n",
    "sess = tf.Session()\n",
    "saver.restore(sess,save_path)\n",
    "\n",
    "inputs_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "targets_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "# Set up test graph\n",
    "rnn_outputs_test = []\n",
    "reuse = True\n",
    "for i in range(Ntest + Ntest_out):\n",
    "    output, state = cell(inputs_test[i],state,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "\n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size])\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.log_softmax(logit) for logit in logits_test] \n",
    "term_detector = [tf.not_equal(tf.argmax(targets_test[i],1),term_symbol) for i in range(Ntest + Ntest_out)]\n",
    "mask = [tf.reduce_max(tf.cast(m, tf.float32)) for m in term_detector]\n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets_test[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest + Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "errors_test_mask = [errors_test[i] * mask[i] for i in range(Ntest + Ntest_out)]\n",
    "mean_error_test = tf.add_n(errors_test_mask)\n",
    "mean_error_test /= tf.add_n(mask)\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "\n",
    "seq_length = Ntest\n",
    "print(\"Testing on sequences of length \" + str(seq_length-2))\n",
    "print(\"\")\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    for z in range(batch_size):\n",
    "        a, fa, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=Ntest+Ntest_out)\n",
    "            \n",
    "        inp.append(a_onehot)\n",
    "        out.append(fa_onehot)        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        in_node = inputs_test[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        out_node = targets_test[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = sess.run(mean_error_test, feed_dict)\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"epochs        - \" + str(epoch))\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error train(test) - \" + str(epoch_error_means[-1]) + \" (\" + str(final_error) + \")\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
