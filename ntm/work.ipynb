{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of the Linear Logic Recurrent Neural Network (LLRNN)\n",
    "#\n",
    "# Version 11.0\n",
    "\n",
    "###################\n",
    "# HYPERPARAMETERS #\n",
    "###################\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, mult_pattern_ntm\n",
    "task                  = 'copy' # copy, repeat copy, pattern i, mult pattern i, variable pattern i\n",
    "epoch                 = 200 # number of training epochs, default to 200\n",
    "num_classes           = 10 # number of symbols, INCLUDING initial and terminal symbols, default 10\n",
    "N                     = 30 # length of input sequences for training, default to 30\n",
    "Ntest                 = 35 # length of sequences for testing, default to 35\n",
    "batch_size            = 250 # default 250\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "num_training          = 10000 # default 10000\n",
    "num_test              = num_training\n",
    "term_symbol           = num_classes - 1\n",
    "init_symbol           = num_classes - 2\n",
    "div_symbol            = num_classes - 3\n",
    "learning_rate         = 1e-4 # default 1e-4\n",
    "memory_init_bias      = 1.0 # default 1.0\n",
    "use_curriculum        = True # default True\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "\n",
    "##################\n",
    "# MODEL SPECIFIC #\n",
    "##################\n",
    "\n",
    "ntm_memory_address_size   = 128 # number of memory locations, default 128\n",
    "ntm_memory_content_size   = 20 # size of vector stored at a memory location, default 20\n",
    "ntm_powers                = [0,-1,1] # powers of R used by controller, default [0,-1,1]\n",
    "\n",
    "pattern_ntm_powers               = [[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "pattern_ntm_powers_2_on_1        = [0,1,2] # allowed powers used by ring 2 to manipulate ring 1\n",
    "pattern_ntm_memory_address_sizes = [128, 20] # number of memory locations for the three rings\n",
    "pattern_ntm_memory_content_sizes = [20, 3] # size of content vector for each ring\n",
    "pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "mult_pattern_ntm_powers               = [[0,-1,1],[0,-1,1],[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "mult_pattern_ntm_powers_2_on_1        = [0,1,2] # allowed powers used by rings 2,3 to manipulate ring 1\n",
    "mult_pattern_ntm_memory_address_sizes = [128, 20, 20, 10] # number of memory locations for the rings\n",
    "mult_pattern_ntm_memory_content_sizes = [20, 3, 3, 2] # size of content vector for each ring\n",
    "mult_pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there\n",
    "# 2. The initial and terminal symbols are always from the end of the list of symbols, so they\n",
    "# are respectively num_classes - 2 and num_classes - 1. So the number of symbols which are\n",
    "# not initial or terminal is num_classes - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import pandas\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs\n",
    "\n",
    "assert use_model == 'ntm' or use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[1, 2, 0, 3, 3, 2, 3, 2, 6, 0, 4, 4, 4, 3, 0, 2, 4, 6]\n",
      "is mapped to\n",
      "[1, 2, 0, 3, 3, 2, 3, 2, 6, 0, 4, 4, 4, 3, 0, 2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "# Below N_out and Ntest_out are the lengths of the outputs in both the training\n",
    "# and testing regimes respectively. Since outputs do not include the initial and terminal\n",
    "# symbols, these default to N - 2 and Ntest - 2 respectively.\n",
    "\n",
    "# Default sampling from space of inputs\n",
    "def generate_input_seq_default(max_symbol,input_length):\n",
    "    return [random.randint(0,max_symbol) for k in range(input_length)]\n",
    "\n",
    "generate_input_seq = generate_input_seq_default\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "#\n",
    "# In this task the input is simply copied to the output (although we\n",
    "# require the RNN to output the first output symbol after the last\n",
    "# input symbol has been read, so this effectively requires the system\n",
    "# to store the input and later retrieve it)\n",
    "\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N - 2\n",
    "    Ntest_out = Ntest - 2\n",
    "    seq_length_min = 7\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "#\n",
    "# In this task every digit of the input is repeated.\n",
    "#\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "\n",
    "if( task == 'repeat copy' ):\n",
    "    no_of_copies = 2\n",
    "    pattern = [0]*(no_of_copies - 1) + [1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = no_of_copies * (N - 2)\n",
    "    Ntest_out = no_of_copies * (Ntest - 2)\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 1\n",
    "if( task == 'pattern 1' ):\n",
    "    pattern = [0,1,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,c,c,d,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = (N - 2) + divmod(N - 2, 2)[0] # N - 2 plus the number of times 2 divides N - 2\n",
    "    Ntest_out = (Ntest - 2) + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 2\n",
    "if( task == 'pattern 2' ):\n",
    "    pattern = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = N - 2 + divmod(N - 2, 2)[0]\n",
    "    Ntest_out = Ntest - 2 + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 3\n",
    "if( task == 'pattern 3' ):\n",
    "    pattern = [0,2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,b,b,d,c,c,e,d,d,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 4 + (N - 2 - 2) * 3\n",
    "    Ntest_out = 4 + (Ntest - 2 - 2) * 3\n",
    "    seq_length_min = 7\n",
    "\n",
    "################\n",
    "# PATTERN TASK 4\n",
    "if( task == 'pattern 4' ):\n",
    "    pattern = [0,2,1,2,-2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,d,f,d,c,c,e,f,h,f,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 7\n",
    "\n",
    "################\n",
    "# PATTERN TASK 5\n",
    "if( task == 'pattern 5' ):\n",
    "    pattern = [4,1,1,-4] # so (a,b,c,d,e,f,...) goes to (a,e,f,g,k,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 7\n",
    "\n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 1\n",
    "if( task == 'mult pattern 1' or task == 'mult pattern 2'):\n",
    "    pattern1 = [1] # so (a,b,c,d,e,f,...) goes to (a,b,c,d,e,f,...)\n",
    "    pattern2 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 7\n",
    "    \n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 2\n",
    "if( task == 'mult pattern 2' ):\n",
    "    # Almost everything is the same as mult pattern 1, but in pattern 2 we \n",
    "    # make sure there is a div symbol somewhere in the sequence\n",
    "    def generate_input_seq_forcediv(max_symbol,input_length):\n",
    "        t = [random.randint(0,max_symbol) for k in range(input_length)]\n",
    "        div_pos = random.randint(0,len(t)-1)\n",
    "        t[div_pos] = div_symbol\n",
    "        return t\n",
    "    \n",
    "    generate_input_seq = generate_input_seq_forcediv\n",
    "\n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 3\n",
    "if( task == 'mult pattern 3'):\n",
    "    pattern1 = [1] # so (a,b,c,d,e,f,...) goes to (a,b,c,d,e,f,...)\n",
    "    pattern2 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    pattern3 = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2,pattern3],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 7\n",
    "    \n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 4\n",
    "if( task == 'mult pattern 4'):\n",
    "    pattern1 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    pattern2 = [2,-1] # so (a,b,c,d,e,f,...) goes to (a,c,b,d,c,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 7\n",
    "\n",
    "#########################\n",
    "# VARIABLE PATTERN TASK 1\n",
    "#\n",
    "# The input is a pattern together with a string to which we are supposed to apply the\n",
    "# pattern, separated by an initial symbol. There is no division symbol.\n",
    "\n",
    "def generate_input_seq_varpattern1(max_symbol,input_length):\n",
    "    varpatterns = [[1],[2],[0,1],[0,2],[1,2]]\n",
    "    vp = varpatterns[random.randint(0,len(varpatterns)-1)]\n",
    "    t = vp + [init_symbol] + [random.randint(0,max_symbol) for k in range(input_length-len(vp)-1)]\n",
    "    return t\n",
    "\n",
    "if( task == 'variable pattern 1'):\n",
    "    generate_input_seq = generate_input_seq_varpattern1\n",
    "    func_to_learn = lambda s: learnfuncs.f_varpattern(s,init_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 10\n",
    "    \n",
    "#########################\n",
    "# VARIABLE PATTERN TASK 2\n",
    "\n",
    "def generate_input_seq_varpattern2(max_symbol,input_length):\n",
    "    varpatterns = [[1],[2]]\n",
    "    varpatterns = varpatterns + [[0,1],[0,2],[1,2]]\n",
    "    varpatterns = varpatterns + [[0,1,0],[0,1,1],[0,1,2],[0,2,0],[0,2,1],[0,2,2],[1,1,2],[1,2,2]]\n",
    "    varpatterns = varpatterns + [[0,0,0,1],[0,0,0,2],[0,0,1,2],[0,1,1,2],[0,1,0,2],[0,2,0,2]]\n",
    "    vp = varpatterns[random.randint(0,len(varpatterns)-1)]\n",
    "    t = vp + [init_symbol] + [random.randint(0,max_symbol) for k in range(input_length-len(vp)-1)]\n",
    "    return t\n",
    "\n",
    "if( task == 'variable pattern 2'):\n",
    "    generate_input_seq = generate_input_seq_varpattern2\n",
    "    func_to_learn = lambda s: learnfuncs.f_varpattern(s,init_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 13\n",
    "\n",
    "#########################\n",
    "# VARIABLE PATTERN TASK 3\n",
    "#\n",
    "# In this task we randomly generate the pattern from the alphabet 0,1,2\n",
    "# We also generate longer sequences than in task 1 or 2. By default\n",
    "# we generate patterns between length 1 and 8\n",
    "\n",
    "def generate_input_seq_varpattern3(max_symbol,input_length):\n",
    "    while( True ):\n",
    "        vp_length = random.randint(1,8)\n",
    "        vp = [random.randint(0,2) for k in range(vp_length)]\n",
    "        \n",
    "        # We cannot allow patterns that are all zeros\n",
    "        if( reduce( lambda x,y : x + y, vp) > 0 ):\n",
    "            break\n",
    "    \n",
    "    t = vp + [init_symbol] + [random.randint(0,max_symbol) for k in range(input_length-len(vp)-1)]\n",
    "    return t\n",
    "\n",
    "if( task == 'variable pattern 3'):\n",
    "    generate_input_seq = generate_input_seq_varpattern3\n",
    "    func_to_learn = lambda s: learnfuncs.f_varpattern(s,init_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 20\n",
    "\n",
    "# Make sure the given N is above the minimum sequence length\n",
    "assert N >= seq_length_min\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = generate_input_seq(num_classes-3,N-2)\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# INITIALISE STATE #\n",
    "####################\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "input_size = num_classes # dimension of the input space I\n",
    "\n",
    "#####\n",
    "# NTM\n",
    "\n",
    "def init_state_ntm(batch_size, css, mas, mcs):\n",
    "    state_size = css + 2*mas + mas * mcs\n",
    "    \n",
    "    ra = [0.0]*mas\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,mas]) + ra\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    init_read_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_write_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_memory = tf.truncated_normal([batch_size, mas*mcs], 0.0, 1e-6, dtype=tf.float32)\n",
    "    \n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "#############\n",
    "# PATTERN NTM\n",
    "\n",
    "def init_state_pattern_ntm(batch_size, css, mas, mcs):\n",
    "    # mas and mcs are arrays of address sizes and content sizes for rings\n",
    "    state_size = css\n",
    "    \n",
    "    init_address = []\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        state_size = state_size + mas[i] * mcs[i] # for memory vector\n",
    "        state_size = state_size + 2 * mas[i] # for addresses (read and write)\n",
    "    \n",
    "        ra = [0.0]*mas[i]\n",
    "        ra[0] = 1.0\n",
    "        init_address.append(np.zeros([batch_size,mas[i]]) + ra)\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    \n",
    "    tensor_list = [init_controller_state]\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        init_read_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        init_write_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        tensor_list = tensor_list + [init_read_address,init_write_address]\n",
    "        \n",
    "    for i in range(len(mas)):\n",
    "        # The first ring is initialised to zero, the rest differently\n",
    "        if( i == 0 ):\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "        else:\n",
    "            # This initialisation has the result of biasing the output of rings 2 and 3 to be\n",
    "            # \"no rotation\" and biasing ring 4 to say \"use ring 2\"\n",
    "            ra = [0.0]*mcs[i] \n",
    "            ra[0] = memory_init_bias\n",
    "            ra = np.zeros([batch_size,mas[i],mcs[i]]) + ra\n",
    "            ra = tf.constant(ra,dtype=tf.float32,shape=[batch_size,mas[i],mcs[i]])\n",
    "            ra = tf.reshape(ra,[batch_size,mas[i]*mcs[i]])\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32) + ra\n",
    "            #init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "            \n",
    "        tensor_list = tensor_list + [init_memory]\n",
    "    \n",
    "    state = tf.concat(tensor_list,1)\n",
    "\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "######################\n",
    "# MULTIPLE PATTERN NTM\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_37/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_36/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_35/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_34/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_33/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_32/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_31/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_30/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_29/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_28/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_27/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_26/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_25/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_24/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_23/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_22/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_21/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_20/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_19/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_18/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_17/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_15/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_13/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_11/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_9/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_7/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_5/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_3/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_1/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "\n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "read_addresses2 = []\n",
    "read_addresses3 = []\n",
    "read_addresses4 = []\n",
    "write_addresses = []\n",
    "write_addresses2 = []\n",
    "write_addresses3 = []\n",
    "write_addresses4 = []\n",
    "interps = []\n",
    "rnn_outputs = []\n",
    "m2 = []\n",
    "m3 = []\n",
    "m4 = []\n",
    "    \n",
    "for i in range(N + N_out):\n",
    "    \n",
    "    old_state = state\n",
    "\n",
    "    #### RUN MODEL ####\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "    ###################\n",
    "\n",
    "    reuse = True\n",
    "    \n",
    "    #### SET UP NODES FOR LOGGING #####\n",
    "    if( use_model == 'ntm' ):\n",
    "        h0, curr_read, curr_write, _ = tf.split(old_state, [controller_state_size,ntm_memory_address_size,\n",
    "                                                        ntm_memory_address_size,-1], 1)\n",
    "\n",
    "    if( use_model == 'pattern_ntm' ):\n",
    "        mas = pattern_ntm_memory_address_sizes\n",
    "        mcs = pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(old_state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],mas[0] * mcs[0],mas[1] * mcs[1]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        m1_state = ret[5]\n",
    "        m2_state = ret[6]\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm' ):\n",
    "        mas = mult_pattern_ntm_memory_address_sizes\n",
    "        mcs = mult_pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(old_state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],                        \n",
    "                            mas[2],mas[2],mas[3],mas[3],mas[0] * mcs[0],mas[1] * mcs[1],\n",
    "                            mas[2] * mcs[2],mas[3] * mcs[3]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        curr_read3 = ret[5]\n",
    "        curr_write3 = ret[6]\n",
    "        curr_read4 = ret[7]\n",
    "        curr_write4 = ret[8]\n",
    "        m1_state = ret[9]\n",
    "        m2_state = ret[10]\n",
    "        m3_state = ret[11]\n",
    "        m4_state = ret[12]\n",
    "        \n",
    "    read_addresses.append(curr_read[0,:])\n",
    "    write_addresses.append(curr_write[0,:])\n",
    "    \n",
    "    if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses2.append(curr_read2[0,:])\n",
    "        write_addresses2.append(curr_write2[0,:])\n",
    "        m2_state = tf.reshape(m2_state, [-1,mas[1],mcs[1]])\n",
    "        m2.append(tf.nn.softmax(m2_state[0,:]))\n",
    "        \n",
    "        with tf.variable_scope(\"NTM\",reuse=True):\n",
    "            W_interp = tf.get_variable(\"W_interp\", [controller_state_size,1])\n",
    "            B_interp = tf.get_variable(\"B_interp\", [1])\n",
    "            interp = tf.sigmoid(tf.matmul(h0,W_interp) + B_interp)\n",
    "            interp_matrix = tf.concat([interp,tf.ones_like(interp,dtype=tf.float32) - interp],axis=1) # shape [-1,2]\n",
    "            interps.append(interp_matrix[0,:])\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses3.append(curr_read3[0,:])\n",
    "        write_addresses3.append(curr_write3[0,:])\n",
    "        read_addresses4.append(curr_read4[0,:])\n",
    "        write_addresses4.append(curr_write4[0,:])\n",
    "        m3_state = tf.reshape(m3_state, [-1,mult_pattern_ntm_memory_address_sizes[2],mult_pattern_ntm_memory_content_sizes[2]])\n",
    "        m3.append(tf.nn.softmax(m3_state[0,:]))\n",
    "        m4_state = tf.reshape(m4_state, [-1,mult_pattern_ntm_memory_address_sizes[3],mult_pattern_ntm_memory_content_sizes[3]])\n",
    "        m4_state = m4_state[0,:]\n",
    "        m4_state = tf.concat([tf.nn.softmax(m4_state),tf.zeros([mult_pattern_ntm_memory_address_sizes[3],1])],1)\n",
    "        m4.append(m4_state)\n",
    "    ### END LOGGING ###\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# Note: prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "# Note: we use log_softmax to avoid precision issues with floats causing log(0) to create NaNs\n",
    "\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.log_softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * prediction[i]) for i in range(N + N_out)] # an array of numbers\n",
    "\n",
    "# Note: We allow the length of input sequences to vary between batches, which means\n",
    "# that the cross entropy needs to be masked to the relevant part of the output. The\n",
    "# relevant part consists of those positions that are not terminal symbols in the output\n",
    "# of _every_ input sequence in the batch. We detect such positions as follows. First,\n",
    "# we create a tensor term_detector which detects all the positions which are terminal symbols.\n",
    "# term_detector[i] is a boolean tensor which has False for those elements of the batch with\n",
    "# a terminal symbol in the output position i, and True otherwise.\n",
    "\n",
    "term_detector = [tf.not_equal(tf.argmax(targets[i],1),term_symbol) for i in range(N + N_out)]\n",
    "\n",
    "# We then convert False to 0.0 and True to 1.0, and compute the reduce_max, with the result\n",
    "# that mask is 1.0 in position i if and only if there was SOME element of the batch which\n",
    "# did NOT have a terminal symbol in position i\n",
    "\n",
    "mask = [tf.reduce_max(tf.cast(m, tf.float32)) for m in term_detector]\n",
    "ce_mask = [ce[i] * mask[i] for i in range(N + N_out)]\n",
    "cross_entropy = -tf.add_n(ce_mask)\n",
    "cross_entropy /= tf.add_n(mask)\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate,decay=0.9,momentum=0.9)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N + N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "errors_mask = [errors[i] * mask[i] for i in range(N + N_out)]\n",
    "mean_error = tf.add_n(errors_mask)\n",
    "mean_error /= tf.add_n(mask)\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1, mean error - 0.898333\n",
      "Epoch - 2, mean error - 0.853513\n",
      "\n",
      "It took 47 seconds to train.\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "###################\n",
    "# Note on sequences\n",
    "#\n",
    "# Our sequences are of varying length, in the alphabet {0,...,num_classes - 3}.\n",
    "# Each input sequence begins with an initial symbol and ends with a terminal symbol\n",
    "# (the value of which are num_classes - 2 and num_classes - 1 by default).\n",
    "#\n",
    "# Both input and output sequences are written on a \"tape\" of length N + N_out.\n",
    "# Input sequences are aligned at the BEGINNING of the tape, and all remaining space\n",
    "# is filled with terminal symbols. Output sequences are aligned at the END OF THE \n",
    "# MATCHING INPUT, with all remaining space filled with terminal symbols.\n",
    "#\n",
    "# Example: suppose N = N_out = 10, and num_classes = 10 so that init_symbol = 8\n",
    "# and term_symbol = 9. Then a sequence of length 8 (seq_length = 10 below) is\n",
    "#\n",
    "# a = [4, 4, 5, 6, 3, 3, 6, 7]\n",
    "#\n",
    "# which written on the tape is\n",
    "#\n",
    "# [8, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
    "#\n",
    "# If we are performing the copy task, so that the output sequence is also a, then\n",
    "# the output written on the tape will be (notice the alignment)\n",
    "#\n",
    "# [9, 9, 9, 9, 9, 9, 9, 9, 9, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9]\n",
    "#\n",
    "\n",
    "def io_generator(max_symbol, input_length, total_length):\n",
    "    \"\"\"\n",
    "    Returns a one-hot encoded pair of input and output sequence, with terminal and initial symbols.\n",
    "    \n",
    "    max_symbol - generate sequences in 0,...,max_symbol\n",
    "    input_length - length of input sequences, without initial and terminal symbols\n",
    "    total_length - length of the buffer, so that the sequences are padded to this length\n",
    "    \"\"\"\n",
    "    a = generate_input_seq(max_symbol,input_length)\n",
    "    fa = func_to_learn(a)\n",
    "    a = [init_symbol] + a + [term_symbol]\n",
    "    a = a + [term_symbol for k in range(total_length-len(a))]\n",
    "    a_onehot = [one_hots[e] for e in a]\n",
    "    \n",
    "    # If the output is too long to fit in the buffer, truncate it\n",
    "    if( len(fa) + input_length + 1 > total_length ):\n",
    "        fa = fa[:total_length-input_length-1]\n",
    "        \n",
    "    fa = [term_symbol for k in range(input_length+1)] + fa + \\\n",
    "                [term_symbol for k in range(total_length-(input_length+1)-len(fa))]\n",
    "    fa_onehot = [one_hots[e] for e in fa]\n",
    "    \n",
    "    return a, fa, np.array(a_onehot), np.array(fa_onehot)\n",
    "\n",
    "error_means = []\n",
    "epoch_error_means = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences. Each\n",
    "        # batch has a fixed length of the sequences. Recall that all input seqs\n",
    "        # have an initial and terminal symbol, so if seq_length = 10 then there\n",
    "        # are eight positions for the \"content\" symbols\n",
    "        \n",
    "        # Our version of curriculum training says: spend the first half\n",
    "        # of the epochs ramping up to the full training set. Assuming that\n",
    "        # epoch > N we divide allocate each integer in [seq_length_min,N]\n",
    "        # an equal portion of the first half of the epochs.\n",
    "        if( use_curriculum == True ):\n",
    "            if( 2 * i > epoch ):\n",
    "                seq_length_max = N\n",
    "            else:\n",
    "                curriculum_band = max(1,int(epoch/(2*(N - seq_length_min))))\n",
    "                seq_length_max = min(seq_length_min + int(i/curriculum_band),N)\n",
    "        else:\n",
    "            seq_length_max = N\n",
    "            \n",
    "        seq_length = random.randint(seq_length_min,seq_length_max)\n",
    "        \n",
    "        # Hack: if we are on the final batch of the final epoch, force\n",
    "        # it to use the full sequence length, so we get a good visualisation\n",
    "        if( i + 1 == epoch and j + 1 == no_of_batches ):\n",
    "            seq_length = N\n",
    "        \n",
    "        for z in range(batch_size):\n",
    "            a, fa, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=N+N_out)\n",
    "            \n",
    "            inp.append(a_onehot)\n",
    "            out.append(fa_onehot)\n",
    "            \n",
    "            # Record the first sequence in the last batch of the last epoch\n",
    "            if( i == epoch - 1 and j == no_of_batches - 1 and z == 0):\n",
    "                final_seq = a\n",
    "                final_seq_output = fa\n",
    "        \n",
    "        # An annoying thing here is that we cannot use a list as a key in a \n",
    "        # dictionary. The workaround we found on StackOverflow here:\n",
    "        # http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "        feed_dict = {}\n",
    "        \n",
    "        for d in range(N + N_out):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N + N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "\n",
    "        ##### Do gradient descent #####\n",
    "        mean_error_val,_ = sess.run([mean_error,minimize], feed_dict)\n",
    "        ###############################\n",
    "        \n",
    "        error_means.append(mean_error_val)\n",
    "    \n",
    "    epoch_error = np.mean(error_means[-no_of_batches:])\n",
    "    epoch_error_means.append(epoch_error)\n",
    "    \n",
    "    # Print the mean error of the final batch in the epoch\n",
    "    print_str = \"Epoch - \" + str(i+1) + \", mean error - \" + str(epoch_error)\n",
    "    \n",
    "    if( use_curriculum == True ):\n",
    "        print_str = print_str + \", training at max length - \" + str(seq_length_max)\n",
    "        \n",
    "    print(print_str)\n",
    "\n",
    "# For the final batch of the final epoch, we record the memory states as well\n",
    "seq_length_for_vis = seq_length - 2\n",
    "interps_val = sess.run(interps,feed_dict)\n",
    "m2_val, m3_val, m4_val = sess.run([m2,m3,m4],feed_dict)            \n",
    "r1_val, w1_val = sess.run([read_addresses,write_addresses],feed_dict)\n",
    "r2_val, w2_val = sess.run([read_addresses2,write_addresses2],feed_dict)\n",
    "r3_val, w3_val = sess.run([read_addresses3,write_addresses3],feed_dict)\n",
    "r4_val, w4_val = sess.run([read_addresses4,write_addresses4],feed_dict)\n",
    "errors_mask_val = sess.run(errors_mask,feed_dict)\n",
    "\n",
    "mask_val = sess.run(tf.cast(mask,tf.int64),feed_dict)\n",
    "predicted_seq = [tf.argmax(prediction[i], 1) for i in range(N + N_out)]\n",
    "predicted_seq_val = sess.run(predicted_seq,feed_dict)\n",
    "final_seq_pred_0 = [a[0] for a in predicted_seq_val]\n",
    "final_seq_pred = []\n",
    "\n",
    "for i in range(len(mask_val)):\n",
    "    if( mask_val[i] == 1.0 ):\n",
    "        final_seq_pred.append(final_seq_pred_0[i])\n",
    "    else:\n",
    "        final_seq_pred.append(9)\n",
    "\n",
    "# Write out variables to disk\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took \" + str(int(time.time() - pre_train_time)) + \" seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length used for visualisations - 18\n",
      "\n",
      "Sequence used for visualisations is (Note: initial symbol is 8, terminal symbol is 9)\n",
      "[8, 7, 4, 6, 7, 6, 2, 0, 0, 4, 7, 7, 0, 0, 6, 0, 6, 0, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "\n",
      "Correct output for this sequence:\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 7, 4, 6, 7, 6, 2, 0, 0, 4, 7, 7, 0, 0, 6, 0, 6, 0, 5, 9]\n",
      "\n",
      "Predicted output for this sequence\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 2, 2, 5, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 9]\n",
      "\n",
      "Correct digits (1 means correct)\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      "Mask for output\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0]\n",
      "\n",
      "Error probabilities for final batch\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.86799997, 0.85600001, 0.87599999, 0.86799997, 0.85600001, 0.88, 0.86400002, 0.86799997, 0.91600001, 0.90799999, 0.83600003, 0.85600001, 0.852, 0.83600003, 0.852, 0.852, 0.86400002, 0.88, 0.0]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAANqCAYAAADbhTnjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a63df50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 1 #\n",
    "###########################\n",
    "\n",
    "print(\"Sequence length used for visualisations - \" + str(seq_length_for_vis))\n",
    "print(\"\")\n",
    "print(\"Sequence used for visualisations is (Note: initial symbol is \" + str(init_symbol) + \", terminal symbol is \" + str(term_symbol) + \")\")\n",
    "print(final_seq)\n",
    "print(\"\")\n",
    "print(\"Correct output for this sequence:\")\n",
    "print(final_seq_output)\n",
    "print(\"\")\n",
    "print(\"Predicted output for this sequence\")\n",
    "print(final_seq_pred)\n",
    "print(\"\")\n",
    "print(\"Correct digits (1 means correct)\")\n",
    "print([int(a==b) for a,b in zip(final_seq_output,final_seq_pred)])\n",
    "print(\"\")\n",
    "print(\"Mask for output\")\n",
    "print(mask_val)\n",
    "print(\"\")\n",
    "print(\"Error probabilities for final batch\")\n",
    "print(errors_mask_val)\n",
    "print(\"\")\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 9, 13\n",
    "fig_num = 0\n",
    "\n",
    "# RING 1\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "plt.figure(fig_num)\n",
    "ax1 = plt.subplot(2,1,1)\n",
    "ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "ax1.imshow(np.stack(w1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax1.set_title('Write address (ring 1)')\n",
    "ax1.set_xlabel('position')\n",
    "ax1.set_ylabel('time')\n",
    "\n",
    "ax2.imshow(np.stack(r1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax2.set_title('Read address (ring 1)')\n",
    "ax2.set_xlabel('position')\n",
    "ax2.set_ylabel('time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 2 #\n",
    "###########################\n",
    "\n",
    "if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 2)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 2)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Assume that powers2_on_1 has three entries we can use as colour channels\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 2)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    max_xticks = 2\n",
    "    xloc = plt.MaxNLocator(max_xticks)\n",
    "\n",
    "    ax.imshow(np.stack(interps_val), cmap='bone', interpolation='nearest', aspect='auto')\n",
    "    ax.set_title('Interpolation')\n",
    "    ax.set_xlabel('direct vs indirect')\n",
    "    ax.set_ylabel('time')\n",
    "    ax.xaxis.set_major_locator(xloc)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# VISUALISATIONS - OTHER RINGS #\n",
    "################################\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 3)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 3)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 3)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 4)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 4)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax6 = plt.subplot(1,1,1)    \n",
    "    ax6.imshow(np.stack(m4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax6.set_title('Memory contents (ring 4)')\n",
    "    ax6.set_xlabel('position')\n",
    "    ax6.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3kAAAUKCAYAAABblriAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a775310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########################\n",
    "# VISUALISATIONS - ERROR #\n",
    "##########################\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "\n",
    "plt.figure(fig_num)\n",
    "ax = plt.subplot(1,1,1)\n",
    "sc = pandas.Series(error_means)\n",
    "ma = sc.rolling(window=500).mean()\n",
    "ax.plot(sc.index, sc, color='lightgray')\n",
    "ax.plot(ma.index, ma, color='red')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(sc.index.min(), sc.index.max())\n",
    "ax.set_title('Error')\n",
    "ax.set_xlabel('batch')\n",
    "ax.set_ylabel('error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on sequences of length 23\n",
      "\n",
      "Batch - 1, Mean error - 0.875304\n",
      "Batch - 2, Mean error - 0.86487\n",
      "Batch - 3, Mean error - 0.869565\n",
      "Batch - 4, Mean error - 0.873043\n",
      "\n",
      "###########\n",
      "# Summary #\n",
      "###########\n",
      "\n",
      "model         - ntm\n",
      "task name     - copy\n",
      "epochs        - 2\n",
      "num_classes   - 10\n",
      "N             - 20\n",
      "Ntest         - 25\n",
      "# weights     - 18958\n",
      "\n",
      "\n",
      "error train(test) - 0.853513 (0.870696)\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "# Restore the weights from training\n",
    "sess = tf.Session()\n",
    "saver.restore(sess,save_path)\n",
    "\n",
    "inputs_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "targets_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "# Set up test graph\n",
    "rnn_outputs_test = []\n",
    "reuse = True\n",
    "for i in range(Ntest + Ntest_out):\n",
    "    output, state = cell(inputs_test[i],state,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "\n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size])\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.log_softmax(logit) for logit in logits_test] \n",
    "term_detector = [tf.not_equal(tf.argmax(targets_test[i],1),term_symbol) for i in range(Ntest + Ntest_out)]\n",
    "mask = [tf.reduce_max(tf.cast(m, tf.float32)) for m in term_detector]\n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets_test[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest + Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "errors_test_mask = [errors_test[i] * mask[i] for i in range(Ntest + Ntest_out)]\n",
    "mean_error_test = tf.add_n(errors_test_mask)\n",
    "mean_error_test /= tf.add_n(mask)\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "\n",
    "seq_length = Ntest\n",
    "print(\"Testing on sequences of length \" + str(seq_length-2))\n",
    "print(\"\")\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    for z in range(batch_size):\n",
    "        a, fa, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=Ntest+Ntest_out)\n",
    "            \n",
    "        inp.append(a_onehot)\n",
    "        out.append(fa_onehot)        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        in_node = inputs_test[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        out_node = targets_test[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = sess.run(mean_error_test, feed_dict)\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"epochs        - \" + str(epoch))\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error train(test) - \" + str(epoch_error_means[-1]) + \" (\" + str(final_error) + \")\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
