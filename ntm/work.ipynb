{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of the Linear Logic Recurrent Neural Network (LLRNN)\n",
    "#\n",
    "\n",
    "###################\n",
    "# HYPERPARAMETERS #\n",
    "###################\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, mult_pattern_ntm\n",
    "task                  = 'copy' # copy, repeat copy, pattern i, mult pattern i\n",
    "epoch                 = 100 # number of training epochs, default to 100\n",
    "num_classes           = 10 # number of symbols, INCLUDING initial and terminal symbols, default 10\n",
    "N                     = 30 # length of input sequences for training, default to 30, INCLUDING initial and terminal symbols\n",
    "Ntest                 = 35 # length of sequences for testing, default to 35, INCLUDING initial and terminal symbols\n",
    "batch_size            = 250 # default 250\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "num_training          = 10000 # default 10000\n",
    "num_test              = num_training\n",
    "term_symbol           = num_classes - 1\n",
    "init_symbol           = num_classes - 2\n",
    "div_symbol            = num_classes - 3\n",
    "\n",
    "##################\n",
    "# MODEL SPECIFIC #\n",
    "##################\n",
    "\n",
    "ntm_memory_address_size   = 128 # number of memory locations, default 128\n",
    "ntm_memory_content_size   = 20 # size of vector stored at a memory location, default 20\n",
    "ntm_powers                = [0,-1,1] # powers of R used by controller, default [0,-1,1]\n",
    "\n",
    "pattern_ntm_powers               = [[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "pattern_ntm_powers_2_on_1        = [0,2,4] # allowed powers used by ring 2 to manipulate ring 1\n",
    "pattern_ntm_memory_address_sizes = [128, 128] # number of memory locations for the three rings\n",
    "pattern_ntm_memory_content_sizes = [20, 3] # size of content vector for each ring\n",
    "pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "mult_pattern_ntm_powers               = [[0,-1,1],[0,-1,1],[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "mult_pattern_ntm_powers_2_on_1        = [0,1,-1] # allowed powers used by rings 2,3 to manipulate ring 1\n",
    "mult_pattern_ntm_memory_address_sizes = [128, 20, 20, 10] # number of memory locations for the rings\n",
    "mult_pattern_ntm_memory_content_sizes = [20, 3, 3, 2] # size of content vector for each ring\n",
    "mult_pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there\n",
    "# 2. The initial and terminal symbols are always from the end of the list of symbols, so they\n",
    "# are respectively num_classes - 2 and num_classes - 1. So the number of symbols which are\n",
    "# not initial or terminal is num_classes - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import pandas\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs\n",
    "\n",
    "assert use_model == 'ntm' or use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[7, 6, 1, 5, 2, 4, 5, 0]\n",
      "is mapped to\n",
      "[7, 6, 1, 5, 2, 4, 5, 0]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "# Below N_out and Ntest_out are the lengths of the outputs in both the training\n",
    "# and testing regimes respectively. Since outputs do not include the initial and terminal\n",
    "# symbols, these default to N - 2 and Ntest - 2 respectively.\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N - 2\n",
    "    Ntest_out = Ntest - 2\n",
    "    seq_length_min = 4\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "if( task == 'repeat copy' ):\n",
    "    no_of_copies = 2\n",
    "    pattern = [0]*(no_of_copies - 1) + [1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = no_of_copies * (N - 2)\n",
    "    Ntest_out = no_of_copies * (Ntest - 2)\n",
    "    seq_length_min = 4\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 1\n",
    "if( task == 'pattern 1' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,1,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,c,c,d,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = (N - 2) + divmod(N - 2, 2)[0] # N - 2 plus the number of times 2 divides N - 2\n",
    "    Ntest_out = (Ntest - 2) + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 4\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 2\n",
    "if( task == 'pattern 2' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = N - 2 + divmod(N - 2, 2)[0]\n",
    "    Ntest_out = Ntest - 2 + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 4\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 3\n",
    "if( task == 'pattern 3' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,b,b,d,c,c,e,d,d,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 4 + (N - 2 - 2) * 3\n",
    "    Ntest_out = 4 + (Ntest - 2 - 2) * 3\n",
    "    seq_length_min = 4\n",
    "\n",
    "################\n",
    "# PATTERN TASK 4\n",
    "if( task == 'pattern 4' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2,1,2,-2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,d,f,d,c,c,e,f,h,f,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 4\n",
    "\n",
    "################\n",
    "# PATTERN TASK 5\n",
    "if( task == 'pattern 5' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [4,1,1,-4] # so (a,b,c,d,e,f,...) goes to (a,e,f,g,k,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 4\n",
    "\n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 1\n",
    "if( task == 'mult pattern 1' ):\n",
    "    pattern1 = [1] # so (a,b,c,d,e,f,...) goes to (a,b,c,d,e,f,...)\n",
    "    pattern2 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,pattern1,pattern2,div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 4\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = [random.randint(0,num_classes-3) for i in range(N - 2)]\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# INITIALISE STATE #\n",
    "####################\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "input_size = num_classes # dimension of the input space I\n",
    "\n",
    "#####\n",
    "# NTM\n",
    "\n",
    "def init_state_ntm(batch_size, css, mas, mcs):\n",
    "    state_size = css + 2*mas + mas * mcs\n",
    "    \n",
    "    ra = [0.0]*mas\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,mas]) + ra\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    init_read_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_write_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_memory = tf.truncated_normal([batch_size, mas*mcs], 0.0, 1e-6, dtype=tf.float32)\n",
    "    \n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "#############\n",
    "# PATTERN NTM\n",
    "\n",
    "def init_state_pattern_ntm(batch_size, css, mas, mcs):\n",
    "    # mas and mcs are arrays of address sizes and content sizes for rings\n",
    "    state_size = css\n",
    "    \n",
    "    init_address = []\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        state_size = state_size + mas[i] * mcs[i] # for memory vector\n",
    "        state_size = state_size + 2 * mas[i] # for addresses (read and write)\n",
    "    \n",
    "        ra = [0.0]*mas[i]\n",
    "        ra[0] = 1.0\n",
    "        init_address.append(np.zeros([batch_size,mas[i]]) + ra)\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    \n",
    "    tensor_list = [init_controller_state]\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        init_read_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        init_write_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        tensor_list = tensor_list + [init_read_address,init_write_address]\n",
    "        \n",
    "    for i in range(len(mas)):\n",
    "        # The first ring is initialised to zero, the rest differently\n",
    "        if( i == 0 ):\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "        else:\n",
    "            ra = [0.0]*mcs[i] \n",
    "            ra[0] = 1.0\n",
    "            ra = np.zeros([batch_size,mas[i],mcs[i]]) + ra\n",
    "            ra = tf.constant(ra,dtype=tf.float32,shape=[batch_size,mas[i],mcs[i]])\n",
    "            ra = tf.reshape(ra,[batch_size,mas[i]*mcs[i]])\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32) + ra\n",
    "            #init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "            \n",
    "        tensor_list = tensor_list + [init_memory]\n",
    "    \n",
    "    state = tf.concat(tensor_list,1)\n",
    "\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "######################\n",
    "# MULTIPLE PATTERN NTM\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_17/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_15/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_13/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_11/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_9/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_7/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_5/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_3/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_1/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "\n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "read_addresses2 = []\n",
    "read_addresses3 = []\n",
    "read_addresses4 = []\n",
    "write_addresses = []\n",
    "write_addresses2 = []\n",
    "write_addresses3 = []\n",
    "write_addresses4 = []\n",
    "interps = []\n",
    "rnn_outputs = []\n",
    "m2 = []\n",
    "m3 = []\n",
    "m4 = []\n",
    "    \n",
    "for i in range(N + N_out):\n",
    "    # Logging\n",
    "    if( use_model == 'ntm' ):\n",
    "        h0, curr_read, curr_write, _ = tf.split(state, [controller_state_size,ntm_memory_address_size,\n",
    "                                                        ntm_memory_address_size,-1], 1)\n",
    "\n",
    "    if( use_model == 'pattern_ntm' ):\n",
    "        mas = pattern_ntm_memory_address_sizes\n",
    "        mcs = pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],mas[0] * mcs[0],mas[1] * mcs[1]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        m1_state = ret[5]\n",
    "        m2_state = ret[6]\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm' ):\n",
    "        mas = mult_pattern_ntm_memory_address_sizes\n",
    "        mcs = mult_pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],                        \n",
    "                            mas[2],mas[2],mas[3],mas[3],mas[0] * mcs[0],mas[1] * mcs[1],\n",
    "                            mas[2] * mcs[2],mas[3] * mcs[3]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        curr_read3 = ret[5]\n",
    "        curr_write3 = ret[6]\n",
    "        curr_read4 = ret[7]\n",
    "        curr_write4 = ret[8]\n",
    "        m1_state = ret[9]\n",
    "        m2_state = ret[10]\n",
    "        m3_state = ret[11]\n",
    "        m4_state = ret[12]\n",
    "\n",
    "    #### RUN MODEL ####\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "    ###################\n",
    "    \n",
    "    # More logging\n",
    "    read_addresses.append(curr_read[0,:])\n",
    "    write_addresses.append(curr_write[0,:])\n",
    "    \n",
    "    if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses2.append(curr_read2[0,:])\n",
    "        write_addresses2.append(curr_write2[0,:])\n",
    "        m2_state = tf.reshape(m2_state, [-1,mas[1],mcs[1]])\n",
    "        m2.append(tf.nn.softmax(m2_state[0,:]))\n",
    "        \n",
    "        with tf.variable_scope(\"NTM\",reuse=True):\n",
    "            W_interp = tf.get_variable(\"W_interp\", [controller_state_size,1])\n",
    "            B_interp = tf.get_variable(\"B_interp\", [1])\n",
    "            interp = tf.sigmoid(tf.matmul(h0,W_interp) + B_interp)\n",
    "            interp_matrix = tf.concat([interp,tf.ones_like(interp,dtype=tf.float32) - interp],axis=1) # shape [-1,2]\n",
    "            interps.append(interp_matrix[0,:])\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses3.append(curr_read3[0,:])\n",
    "        write_addresses3.append(curr_write3[0,:])\n",
    "        read_addresses4.append(curr_read4[0,:])\n",
    "        write_addresses4.append(curr_write4[0,:])\n",
    "        m3_state = tf.reshape(m3_state, [-1,mult_pattern_ntm_memory_address_sizes[2],mult_pattern_ntm_memory_content_sizes[2]])\n",
    "        m3.append(tf.nn.softmax(m3_state[0,:]))\n",
    "        m4_state = tf.reshape(m4_state, [-1,mult_pattern_ntm_memory_address_sizes[3],mult_pattern_ntm_memory_content_sizes[3]])\n",
    "        m4_state = m4_state[0,:]\n",
    "        m4_state = tf.nn.softmax(tf.concat([m4_state,tf.zeros([mult_pattern_ntm_memory_address_sizes[3],1])],1))\n",
    "        m4.append(m4_state)\n",
    "\n",
    "    reuse = True\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# Note: prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "\n",
    "# Note: We allow the length of input sequences to vary between batches, which means\n",
    "# that the cross entropy needs to be masked to the relevant part of the output\n",
    "\n",
    "# Note: we use log_softmax to avoid precision issues with floats causing log(0) to create NaNs\n",
    "\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.log_softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * prediction[i]) for i in range(N + N_out)] # an array of numbers\n",
    "mask = [tf.sign(tf.reduce_max(tf.abs(targets[i]))) for i in range(N + N_out)]\n",
    "ce_mask = [ce[i] * mask[i] for i in range(N + N_out)]\n",
    "cross_entropy = -tf.add_n(ce_mask)\n",
    "cross_entropy /= tf.add_n(mask) # DEBUG do we really need this?\n",
    "# NOTE: here in creating the mask we are assuming that batches have the same sequence length\n",
    "                    \n",
    "optimizer = tf.train.RMSPropOptimizer(1e-4,decay=0.9,momentum=0.9)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N + N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "errors_mask = [errors[i] * mask[i] for i in range(N + N_out)]\n",
    "mean_error = tf.add_n(errors_mask)\n",
    "mean_error /= tf.add_n(mask)\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 0 r [0] w [0]\n",
      " Step 1 r [1] w [127]\n",
      " Step 2 r [0] w [0]\n",
      " Step 3 r [0] w [0]\n",
      " Step 4 r [0] w [0]\n",
      " Step 5 r [0] w [0]\n",
      " Step 6 r [0] w [0]\n",
      " Step 7 r [1] w [0]\n",
      " Step 8 r [1] w [0]\n",
      " Step 9 r [1] w [0]\n",
      " Step 10 r [1] w [1]\n",
      " Step 11 r [0] w [1]\n",
      " Step 12 r [0] w [1]\n",
      " Step 13 r [0] w [2]\n",
      " Step 14 r [0] w [2]\n",
      " Step 15 r [0] w [2]\n",
      " Step 16 r [0] w [2]\n",
      " Step 17 r [0] w [3]\n",
      "Epoch - 1, mean error - 0.957933\n",
      "Epoch - 2, mean error - 0.952119\n",
      "\n",
      "It took 17 seconds to train.\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "###################\n",
    "# Note on sequences\n",
    "#\n",
    "# Our sequences are of varying length, in the alphabet {0,...,num_classes - 3}.\n",
    "# Each input sequence begins with an initial symbol and ends with a terminal symbol\n",
    "# (the value of which are num_classes - 2 and num_classes - 1 by default). Output\n",
    "# sequences do not have either an initial nor a terminal symbol.\n",
    "#\n",
    "# Both input and output sequences are written on a \"tape\" of length N + N_out.\n",
    "# Input sequences are aligned at the BEGINNING of the tape, and all remaining space\n",
    "# is filled with terminal symbols. Output sequences are aligned at the END OF THE \n",
    "# MATCHING INPUT, with all remaining space filled with zero vectors.\n",
    "#\n",
    "# Example: suppose N = N_out = 10, and num_classes = 10 so that init_symbol = 8\n",
    "# and term_symbol = 9. Then a sequence of length 8 (seq_length = 10 below) is\n",
    "#\n",
    "# a = [4, 4, 5, 6, 3, 3, 6, 7]\n",
    "#\n",
    "# which written on the tape is\n",
    "#\n",
    "# [8, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
    "#\n",
    "# If we are performing the copy task, so that the output sequence is also a, then\n",
    "# the output written on the tape will be (notice the alignment)\n",
    "#\n",
    "# [-, -, -, -, -, -, -, -, -, 4, 4, 5, 6, 3, 3, 6, 7, -, -, -]\n",
    "#\n",
    "# where - is a symbol whose encoding is the zero vector.\n",
    "\n",
    "def io_generator(max_symbol, input_length, total_length):\n",
    "    \"\"\"\n",
    "    Returns a one-hot encoded pair of input and output sequence, with terminal and initial symbols.\n",
    "    \n",
    "    max_symbol - generate sequences in 0,...,max_symbol\n",
    "    input_length - length of input sequences, without initial and terminal symbols\n",
    "    total_length - length of the buffer, so that the sequences are padded to this length\n",
    "    \"\"\"\n",
    "    a = [random.randint(0,max_symbol) for k in range(input_length)]\n",
    "    fa = func_to_learn(a)\n",
    "    a = [init_symbol] + a + [term_symbol]\n",
    "    a = a + [term_symbol for k in range(total_length-len(a))]\n",
    "    a_onehot = [one_hots[e] for e in a]\n",
    "    fa_onehot = [[0.0]*num_classes for k in range(input_length+1)] + \\\n",
    "                [one_hots[e] for e in fa] + \\\n",
    "                [[0.0]*num_classes for k in range(total_length-(input_length+1)-len(fa))]\n",
    "    return a, np.array(a_onehot), np.array(fa_onehot)\n",
    "\n",
    "error_means = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences. Each\n",
    "        # batch has a fixed length of the sequences. Recall that all input seqs\n",
    "        # have an initial and terminal symbol, so if seq_length = 10 then there\n",
    "        # are eight positions for the \"content\" symbols\n",
    "        seq_length = random.randint(seq_length_min,N)\n",
    "        \n",
    "        for z in range(batch_size):\n",
    "            a, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=N+N_out)\n",
    "            \n",
    "            inp.append(a_onehot)\n",
    "            out.append(fa_onehot)\n",
    "            \n",
    "            # Record the first sequence in the last batch of the last epoch\n",
    "            if( i == epoch - 1 and j == no_of_batches - 1 and z == 0):\n",
    "                final_seq = a\n",
    "        \n",
    "        # An annoying thing here is that we cannot use a list as a key in a \n",
    "        # dictionary. The workaround we found on StackOverflow here:\n",
    "        # http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "        feed_dict = {}\n",
    "        \n",
    "        for d in range(N + N_out):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N + N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "                \n",
    "        # for the final batch in every 25th epoch, we have some logging\n",
    "        if( j == no_of_batches - 1 and i % 25 == 0 ):\n",
    "            r1_val, w1_val = sess.run([read_addresses,write_addresses],feed_dict)\n",
    "            \n",
    "            if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm' ):\n",
    "                r2_val, w2_val = sess.run([read_addresses2,write_addresses2],feed_dict)\n",
    "            \n",
    "            if( use_model == 'mult_pattern_ntm' ):\n",
    "                r3_val, w3_val, r4_val, w4_val = sess.run([read_addresses3,write_addresses3,read_addresses4,write_addresses4],feed_dict)\n",
    "                \n",
    "            s = 0\n",
    "            for r in range(len(w1_val)):\n",
    "                print_str = \" Step \" + str(s) + \" r [\" + str(r1_val[r].argmax()) + \"]\" + \\\n",
    "                            \" w [\" + str(w1_val[r].argmax()) + \"]\"\n",
    "                    \n",
    "                if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "                    print_str = print_str + \" r2 [\" + str(r2_val[r].argmax()) + \"]\" + \\\n",
    "                          \" w2 [\" + str(w2_val[r].argmax()) + \"]\" + \\\n",
    "                          \" interp \" + str(interps_val[r])     \n",
    "\n",
    "                if( use_model == 'mult_pattern_ntm' ):\n",
    "                    print_str = print_str + \" r3 [\" + str(r3_val[r].argmax()) + \"]\" + \\\n",
    "                          \" w3 [\" + str(w3_val[r].argmax()) + \"]\" + \\\n",
    "                          \" r4 [\" + str(r4_val[r].argmax()) + \"]\" + \" w4 [\" + str(w4_val[r].argmax()) + \"]\"\n",
    "\n",
    "                print(print_str)\n",
    "                                                            \n",
    "                s = s + 1\n",
    "        \n",
    "        # For the final batch of the final epoch, we record the memory states as well\n",
    "        if( j == no_of_batches - 1 and i == epoch - 1 ):\n",
    "            seq_length_for_vis = seq_length - 2\n",
    "            interps_val = sess.run(interps,feed_dict)\n",
    "            m2_val = sess.run(m2,feed_dict)\n",
    "            m3_val = sess.run(m3,feed_dict)\n",
    "            m4_val = sess.run(m4,feed_dict)\n",
    "        \n",
    "        ##### Do gradient descent #####\n",
    "        #summary,mean_error_val,_ = sess.run([merged_summaries,mean_error,minimize], feed_dict)\n",
    "        mean_error_val,_ = sess.run([mean_error,minimize], feed_dict)\n",
    "        ########\n",
    "        \n",
    "        error_means.append(mean_error_val)\n",
    "        \n",
    "        # Write out TensorBoard logs\n",
    "        #file_writer.add_summary(summary)\n",
    "    \n",
    "    # Print the mean error of the final batch in the epoch\n",
    "    print(\"Epoch - \" + str(i+1) + \", mean error - \" + str(np.mean(error_means[-no_of_batches:])))\n",
    "\n",
    "# Write out variables to disk\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took \" + str(int(time.time() - pre_train_time)) + \" seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length used for visualisations - 4\n",
      "Sequence used is\n",
      "[8, 6, 2, 2, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "Note: initialisation symbol is 8 and terminal symbol is 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAAKyCAYAAACDn6wVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12b24e650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 1 #\n",
    "###########################\n",
    "\n",
    "print(\"Sequence length used for visualisations - \" + str(seq_length_for_vis))\n",
    "print(\"Sequence used is\")\n",
    "print(final_seq)\n",
    "print(\"Note: initialisation symbol is \" + str(init_symbol) + \" and terminal symbol is \" + str(term_symbol))\n",
    "print(\"\")\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 9, 13\n",
    "fig_num = 0\n",
    "\n",
    "# RING 1\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "plt.figure(fig_num)\n",
    "ax1 = plt.subplot(2,1,1)\n",
    "ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "ax1.imshow(np.stack(w1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax1.set_title('Write address (ring 1)')\n",
    "ax1.set_xlabel('position')\n",
    "ax1.set_ylabel('time')\n",
    "\n",
    "ax2.imshow(np.stack(r1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax2.set_title('Read address (ring 1)')\n",
    "ax2.set_xlabel('position')\n",
    "ax2.set_ylabel('time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 2 #\n",
    "###########################\n",
    "\n",
    "if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 2)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 2)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Assume that powers2_on_1 has three entries we can use as colour channels\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 2)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    max_xticks = 2\n",
    "    xloc = plt.MaxNLocator(max_xticks)\n",
    "\n",
    "    ax.imshow(np.stack(interps_val), cmap='bone', interpolation='nearest', aspect='auto')\n",
    "    ax.set_title('Interpolation')\n",
    "    ax.set_xlabel('direct vs indirect')\n",
    "    ax.set_ylabel('time')\n",
    "    ax.xaxis.set_major_locator(xloc)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# VISUALISATIONS - OTHER RINGS #\n",
    "################################\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 3)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 3)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 3)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 4)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 4)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax6 = plt.subplot(1,1,1)    \n",
    "    ax6.imshow(np.stack(m4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax6.set_title('Memory contents (ring 4)')\n",
    "    ax6.set_xlabel('position')\n",
    "    ax6.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3kAAAUKCAYAAABblriAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzs3X9s5Xte1/HXpz+n7b3TO/eHd2GFVSAh13+UFiEYBAmJ\nRLIxRv5YG0hQDMYgCSn6hxqI0QjyY2UFE4gKBg3aoP6hGyRAJLgmCIJtEFf2jwVZXCHg3l/dZaYz\n02k//tH5dk877b3t6Znb9t3HI2l65nvacz8zZ+aePs/38/18Wu89AAAA1DB12QMAAABgckQeAABA\nISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAhIg+AG6+19vWt\ntf1TPvZaa1902WMEgLOauewBAMAV0ZN8e5KPnXDfr7+zQwGA8Yk8APi0n+q9b531i1tr00mmeu+7\nJ9w3n+Rh772PO5hJPAYAN4/pmgBwBq219zyevvmtrbVvaa39epL7SV5prX354/ve11r7+621/5vk\nbpJnH3/vH26t/dvW2muttbuttV9orX31scd/y8cAgLNyJg8APm25tfbCsWO99/76yK+/Icl8kn+S\n5EGS15PceXzftz8+9r2Pv+Zha+0PJPmFJLeSfP/jr//6JB9srX1N7/0/HPvvPfEYE/q9AXBDiDwA\nONCS/OwJx+8nWRz59buTfO5o+LXWPvfxzfkkK733hyP3/YMkLyX50t77Lzw+9sNJfjXJ9yU5HnlP\nPAYAnIfIA4ADPck3JfnoseN7x379746d2Rv1oyfE2Z9J8ktD4CVJ7/1ua+2fJvnO1tof6b3/2ts8\nBgCcmcgDgE/75TMsvPKxc973niS/eMLxj4zcPxp5b/X4APC2LLwCAOezM+Z9k3h8AHhbIg8Anq7f\nSvL5Jxx/ZeR+AJgYkQcAT9dPJvmi1toXDwdaa0tJ/kqS3zx2PR4AXJhr8gDgQEvy1a21V0647+dz\nsDDLOL4ryVqSn2qt/UAOtlD4izm4Fu/Pj/mYAHCqspHXWntvkvfn4EX7e3rvP3LJQwLgautJ/u4p\n9/2lJB96/DWnxd6Jx3vv/6+19iVJvjvJN+dgv7xfTfLe3vtPneUxAOA8Wu/1Xk9aa9M5WKnsy5P8\nfpKtJF/ce3/jUgcGAADwlFW9Ju+Lkny49/67vfffT/Ifk/zpSx4TAADAU1c18j4zyW+P/Pq3k7z7\nksYCAADwjrlykdda+5OttQ+21n67tbbfWvuzJ3zNX2ut/WZrbae19outtT9+GWMFAAC4aq5c5CVZ\nSvIrSb4pJ1yA3lp7X5J/mOTvJPmCJP8jyU+31l4c+bLfSfIHR3797sfHAAAASrvSC6+01vaT/Lne\n+wdHjv1ikv/We/+Wx79uST6e5Ad679/z+Niw8MqfSvKpJL+c5E9YeAUAAKjuWm2h0FqbTbKa5DuH\nY7333lr7T0m+ZOTYXmvtryf5zznYQuG73yrwWmsvJPmqJB9Lcv+pDB4AAODsbiX5Q0l+uvf+2nm+\n8VpFXpIXk0wn+b1jx38vyeePHui9/0SSnzjj435Vkn914dEBAABM1tcm+dfn+YbrFnlPy8eS5Md+\n7MfyyiuvXPJQuKj19fV84AMfuOxhMCGezzo8l7V4PmvxfNbhuazjIx/5SL7u674uedwq53HdIu/V\nJHtJXj52/OUkv3uBx72fJK+88kpWVlYu8DBcBcvLy57HQjyfdXgua/F81uL5rMNzWdK5Lye7iqtr\nnqr3vptkM8lXDsceL7zylUn+62WNCwAA4Kq4cmfyWmtLST4vBwumJMnntNb+aJLXe+8fT/J9SX60\ntbaZ5JeSrCdZTPKjlzBcAACAK+XKRV6SL0zycznYI6/nYE+8JPkXSb6h9/5vHu+J9/dyME3zV5J8\nVe/9E5cxWAAAgKvkykVe7/1DeZtppL33H0zyg+/MiLhu1tbWLnsITJDnsw7PZS2ez1o8n3V4Lkmu\n+Gbo75TW2kqSzc3NTReqXnP7+/vZ3d3N3NxcDi7XBACA62drayurq6tJstp73zrP9165M3lwHnt7\ne7l3717u3r2bu3fvZmdnJ0nSWsv8/Hxu3bp15GNmxl95AABq8xMv18qjR49y9+7dw7C7f/9gRdmZ\nmZksLS3lzp07mZuby4MHD3L//v3cv38/29vbGc5YT09PPxF+8/PzmZq6VgvNAgDAqUQeV9ru7u7h\nWbp79+7lwYMHSZLZ2dksLS3lhRdeyOLi4hPTM5955pnD2733PHz48DD6Hjx4kE996lN57bXXDr9m\nfn7+iTN/s7OzpnwCAHDtiDyujCHGRqdf7u7uJjmIsMXFxbz00ktZWlrK7OzsmR93mLo5Pz+f5eXl\nw+P7+/uH4TfE32uvvZa9vb0kydTU1JGzfcPt6enpyf7GAeAUvffDj/39/ezv71/o9kn39d4zOzub\nW7duZWFhwesdFCDyuDS99zx48ODI9MtHjx4lSW7dupXbt29ncXExS0tLT+VauqmpqSwuLmZxcfHI\nmB49enQk/u7du5c33njjcMrn8EI4Gn7z8/PO+gHcIJMMr7e7fR5TU1OZmppKa+3E29PT008cb60d\nXuYweonD3Nzc4evcEH8zMzNe7+AaEHm8Y3rvuX///pGoG86aLSws5LnnnsvS0lIWFxcv7d3D1lpm\nZ2czOzubZ5999vD4/v7+kSmf9+/fz5tvvnkYpccXehluezEEeGcNYTSJM1xvdfs8q5O31k6NruH2\n8HrxdpH2VreHj4vY398/cl37zs5OXn311cPYnJ6ePnK2b2FhwYrWcAWJPJ6aYTrk6DV1+/v7aa1l\nYWEhzz///GHUXfWFT0anbo569OjRkRfD+/fv55Of/OSRF0MLvXDTDFuZDB8PHz48vJ3k8IfB0R8K\njx877fOkvuaqf201k55yeFqcDf+Ns3onwmv4fF1MTU1lYWEhCwsLh8d679nd3T2MvuGM36uvvprk\n4M/x+FTPW7duea2DSyTymJj9/f3cu3fv8CzdvXv30ns/nBb54osvZmlpKQsLC2X+xz8zM3O4sudg\nuLZwNP6OL/QyOgXGQi9cN0PEDfE2GnEPHz48PMM9mJmZydzc3OEPzMMP4aOfR384H/0h/aSvfavP\nZ73vqnvaoTncvsjjnjfazuPtwumkKYdnja3jZ754e621zM3NZW5uLrdv3z48Pnp5w87OTu7evZvX\nX3/98P7RGS6j0z2Bp8+/NMY2ukfdvXv3srOzk957pqens7i4mJdffjmLi4tZWFi4US+kowu9jL4Y\nDmc2R+Pv+EIvJ+3t58J33ml7e3snxtvwefg7OximOM/NzWVpaSlzc3OHx2ZnZ6/cmzrnCcLzxuNF\nIvSyv/a8oZ08ef3XVZlyyDtjZmYmzzzzzJEVrYfpnsMZv+GNziH0Z2Zmnpju6U1OmDyRx5k9evTo\niahLDv6Hvbi4mHe9611ZWlqyCMkpzrLQy/DC+Oabbx5Z6OV4/PkzZly997eNuNGzLqPXqd66dSvP\nPvvsYdANx6/b38WTzl4Bk3HadM/huvYh/t54443Ds/7DJRGj8efSBrgYkcepdnd3j2xnMLpH3eLi\nYu7cuXP4rr0flMZz2kIvw8qjo/F3/PqHk/b2s9ALwxsHJ8XbcOx4xA3Btri4mOXl5SMR5+8UcFGn\nbWU0XOc3xN/opQ3HFzQbAtDsFjgbkcehhw8fHln58uHDh0lyOAVruKZubm7ukkda33AR+/GFXvb2\n9o4s8nJ8GszoQi+jL47eDa1jWADhrSLu+HS6IdhGp1IOn6enp0UccClOepNzb2/viemeo9s6zM7O\nPjHd05tR8CSRd0MNUydGV74c3Xj8mWeeOVz58jwbj/N0TU9PZ2lp6YmFXkbfDT3LQi/z8/POwF5R\nw/N52qImw7/TwfT09GG03bp168g0yrm5Oe96A9fKcF3/8UsbhtktQ/yNXtM+vMF5fLqn1zhuMpF3\nQ4xuPD5E3TAXfmFhIbdv3z6MOitfXS+nrXp2fK+jsyz0Mj8/7/l/yo6vTHk86E5amXIItmGBgtGQ\nE3FAdaOzW5577rkkT77BubOzc+K2Dsene5rZwk3hp7mieu/Z2dk5slDK3t7e4R51V2HjcZ6u0y5+\nP7633/GFXmZmZp5Y4XNubs4L4xkNi5qcdjbueMSNrkK5tLT0RMT5cwd40mlvcO7t7R2Z6rmzs5M3\n3njj8P7hDbPj0z2hGn+ri9jf3z/co2bYq27YeHxxcTEvvPDC4fQHPzTeXKMLvYwueT2c6R2Nv+Pv\niJ60t99Nuw5i2O/rraZSjm4v0Fo73CNufn7+cGXK0ZC7SX9+AE/b9PT0qds6jE73PL6tw/Ezfi5p\n4LoTedfUsPH4MP1y2KNuWKb/pZdeytLSkqkJnMnotJbRlc9GF3oZXiCPL/Ry0t5+1/Xv3LC9wGlT\nKU9amXJ0KuXt27ePLGxy0yIY4Coandly586dJEe3dRg+3nzzzcM3N4dtHUbjz7YOXCci75oY3Xh8\niLrk0wtxvPzyy4dR54dKJuWsC73cvXs3r7/++uHXDGf9RgPwKrwrOrq9wGln446vTDm6yffoGbhh\nUZPL/j0BcH6nbesw7F07nPE7/vo2Pz9/ZLrnMKsFrhp/K6+oR48eHdnO4P79+0kOphQsLS0dXlNn\n9SjeaedZ6OX1118/nL54/CL4IQAn+eI4ur3ASRF3fHuBYWXKYfrqSYua+PcFcHPMzMycON1z9Bq/\nk7Z1OD7d03R8LpvIuyJ2d3ePrHw5uvH40tLS4TV1V+FsCJzkpIVekk+/KzqJhV6GlSlPux7upO0F\nhmgbVqY8vkccALyV4VKY07Z1GF7bRt/YHN23dnS6p5/heKeIvEswzAMfnX45ukfdcE3dEHVwnZ30\nruhJ10KMLvSS5PBMX5LDiDtpe4Eh2IZ/L6MR59oJAJ6G0dkpg+GSgOMLvAz71g5TRI9P9/SGI0+D\nyHsHDO/2jEbd8MPqrVu3cvv27SwuLmZpacm8bm6E066F2Nvbe2LKZ3IQfM8888yRqZQiDoCrZHQF\n62efffbw+OgiZjs7O0/MaBldvXoIQAt3cVGK4inovR9erDuE3XD63h51cLrp6eknpsQAwHV20iJm\nx69j39nZyauvvnpk9erj+/m5ZIfzEHkTMFyQO3pN3bBH3cLCQp5//vksLS1lYWFB1AEA3HAnXcc+\nunr16AIvo3vWjp7tu+7bFvF0ibwxDHvUDWfp7t27d2SPuhdffPEw6vzDAwDg7Zy2evXoAmY7Ozsn\nbuswGn/z8/OHq0M783dzibwzGPaoG6JudOPxYY+6xcXFLCws+McEAMDEnLatw4MHDw7P+A2LvAzT\nPZODaJyamsr09HSmp6cPb5907LTbfq69vkTeCR49enTkLN3xjcff9a532aMOAIBLcdp0z4cPH+bB\ngwfZ39/P3t5e9vb2Dm8Pn3d3d4/cP7p/7En/nZPi8KzRODU15WflSyLyRnziE5/IRz/60SN71C0u\nLubOnTtZWlpywSsAAFfS6MrV59F7fyIIT7u9v7+fR48ePRGSb2Xcs4ijocj5ibwR9+7dO3JNnT3q\nAACorLWWmZmZsbfx6r2fOxQfPnz4xLG3Gt9FQ/EmnqQReSPe85735N3vfvdlDwMAAK6FYYGXqamp\nzM7OjvUYvfe3jcPjt3d3d48ce7tpp6eF4FlC8TouYiPyAACASzOcrbvIVmP7+/vnCsXd3d08ePDg\nyLG3Mu51iaOh+E4SeQAAwLU2XL93kWmnZwnF42cTR4+fZdrpeeJwWCdkHCIPAAC40UYj7CLTTs86\n3XT0jOLosdFppx//+MfH/v2IPAAAgAsaFrEZ1+giNvv7+9ne3h77sUQeAADAJRtdxCbJkX0Qz8vG\nEwAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACA\nQkQeAABAISIPAACgkJnLHsBVsr6+nuXl5aytrWVtbe2yhwMAANwwGxsb2djYyPb29tiP0XrvExzS\n9dRaW0myubm5mZWVlcseDgAAcMNtbW1ldXU1SVZ771vn+V7TNQEAAAoReQAAAIWIPAAAgEJEHgAA\nQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEH\nAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE\n5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAA\nChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwA\nAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEi\nDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABAITOXPYCrZH19PcvL\ny1lbW8va2tplDwcAALhhNjY2srGxke3t7bEfo/XeJzik66m1tpJkc3NzMysrK5c9HAAA4Ibb2trK\n6upqkqz23rfO872mawIAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAA\nKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIA\nAAAKEXkAAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWI\nPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABA\nISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcA\nAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETk\nAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAK\nEXkAAACFiDwAAIBCZi57AFfJ+vp6lpeXs7a2lrW1tcseDgAAcMNsbGxkY2Mj29vbYz9G671PcEjX\nU2ttJcnm5uZmVlZWLns4AADADbe1tZXV1dUkWe29b53ne03XBAAAKETkAQAAFCLyAAAAChF5AAAA\nhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4A\nAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCR\nBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgDAAAo\nROQBAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAA\nAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8\nAACAQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAh\nIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAA\nUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACgkJnLHsBVsr6+nuXl5ayt\nrWVtbe2yhwMAANwwGxsb2djYyPb29tiP0XrvExzS9dRaW0myubm5mZWVlcseDgAAcMNtbW1ldXU1\nSVZ771vn+V7TNQEAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLy\nAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACF\niDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAA\nQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEH\nAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE\n5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAA\nChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwA\nAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEi\nDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQ\niMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcAAFDIzGUP4CpZX1/P\n8vJy1tbWsra2dtnDAQAAbpiNjY1sbGxke3t77MdovfcJDul6aq2tJNnc3NzMysrKZQ8HAAC44ba2\ntrK6upokq733rfN8r+maAAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIA\nAAAKEXkAAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWI\nPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABA\nISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcA\nAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETk\nAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAK\nEXkAAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAA\ngEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABAISIP\nAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCI\nyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAA\nFCLyAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkA\nAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJE\nHgAAQCEzlz2Aq2R9fT3Ly8tZW1vL2traZQ8HAAC4YTY2NrKxsZHt7e2xH6P13ic4pOuptbaSZHNz\nczMrKyuXPRwAAOCG29rayurqapKs9t63zvO9pmsCAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAh\nIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAA\nUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQB\nAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoR\neQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACA\nQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAhIg8A\nAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjI\nAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAU\nIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAA\nAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQkQe\nAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAhIg8AAKAQ\nkQcAAFCIyAMAAChE5AEAABQi8gAAAAo5d+S11qZba1/WWnvuaQwIAACA8Z078nrve0l+JsmdyQ8H\nAACAixh3uuaHk3zOJAcCAADAxY0bed+W5P2ttfe21j6jtXZ79GOSAwQAAODsZsb8vp98/PmDSfrI\n8fb419MXGRQAAADjGTfyvmKiowAAAGAixoq83vuHJj0QAAAALm7cM3l5vIXCX07yyuND/yvJP++9\nb09iYAAAAJzfWAuvtNa+MMlvJFlP8vzjj29N8huttZXJDQ8AAIDzGPdM3gdysOjKN/beHyVJa20m\nyQ8n+UdJvmwywwMAAOA8xo28L8xI4CVJ7/1Ra+17kvz3iYwMAACAcxt3n7xPJvnsE45/VpJPjT8c\nAAAALmInsES6AAAaYElEQVTcyPvxJD/SWntfa+2zHn/8hRxM19yY3PAAAAA4j3Gna/6NHGx6/i9H\nHmM3yQ8l+ZsTGBcAAABjGHefvIdJvqW19reSfO7jw7/Re783sZEBAABwbueOvNbabJKdJH+s9/7h\nJP9z4qMCAABgLOe+Jq/3vpvk/ySZnvxwAAAAuIhxF175jiTf2Vp7fpKDAQAA4GLGXXjlm5N8XpLf\naa39VpK7o3f23lcuOjAAAADOb9zI+/cTHQUAAAATMc7CK9NJfi7Jr/be35z8kAAAABjXOAuv7CX5\nmSR3Jj8cAAAALmLchVc+nORzJjkQAAAALm7cyPu2JO9vrb23tfYZrbXbox+THCAAAABnN+7CKz/5\n+PMHk/SR4+3xr+2hBwAAcAnGjbyvmOgoAAAAmIixpmv23j+UZD/JNyb5riS//vjYZyfZm9zwAAAA\nOI+xIq+19jVJfjrJTpIvSDL/+K7lJH97MkMDAADgvC6y8Mpf7b1/Y5LdkeM/n2TlwqMCAABgLONG\n3ucn+S8nHN9O8tz4wwEAAOAixo28303yeScc/9Ik/3v84QAAAHAR40beP0vy/a21L87Blgmf2Vr7\n2iTvT/JDkxocAAAA5zPuFgrflYNA/NkkizmYuvkgyft77/94QmMDAADgnMaKvN57T/IdrbXvzcG0\nzWeS/Frv/fcnOTgAAADOZ9wzeUmS3vvDJL82obEAAABwQeNekwcAAMAVJPIAAAAKEXkAAACFiDwA\nAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAACjkQpuhV7O+vp7l5eWsra1lbW3tsocDAADcMBsbG9nY\n2Mj29vbYj9F67xMc0vXUWltJsrm5uZmVlZXLHg4AAHDDbW1tZXV1NUlWe+9b5/le0zUBAAAKEXkA\nAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJE\nHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACg\nEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMA\nAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLy\nAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACF\niDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAA\nQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEH\nAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE\n5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAA\nChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwA\nAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEi\nDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQ\niMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEA\nABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5\nAAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwAAIBC\nRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAA\noBCRBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgD\nAAAoROQBAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi\n8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAA\nhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkAAACFiDwAAIBCRB4A\nAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCR\nBwAAUIjIAwAAKETkAQAAFCLyAAAAChF5AAAAhYg8AACAQmYuewBXyfr6epaXl7O2tpa1tbXLHg4A\nAHDDbGxsZGNjI9vb22M/Ruu9T3BI11NrbSXJ5ubmZlZWVi57OAAAwA23tbWV1dXVJFntvW+d53tN\n1wQAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJEHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQAA\nFCLyAAAAChF5AAAAhYg8AACAQkQeAABAISIPAACgEJEHAABQiMgDAAAoROQBAAAUIvIAAAAKEXkA\nAACFiDwAAIBCRB4AAEAhIg8AAKAQkQcAAFCIyAMAAChE5AEAABQi8gAAAAoReQAAAIWIPAAAgEJE\nHgAAQCEiDwAAoBCRBwAAUIjIAwAAKETkAQD/v727D7atrus4/vniE4SWjSBaCkqg0RCk4iBTSg0+\nNDZgTJapOdnNGsKSURuLKYfRGlNKZKRomh5EfMDhj5pwJqUMzAEFgguUiekUCmNgPOilUJS8v/5Y\n+3aPRyDOvse7zvme12vmDGevvdfe38Oae8997/WwAWhE5AEAADQi8gAAABoReQAAAI2IPAAAgEZE\nHgAAQCMiDwAAoBGRBwAA0IjIAwAAaETkAQAANCLyAAAAGhF5AAAAjYg8AACARkQeAABAIyIPAACg\nEZEHAADQiMgDAABoROQBAAA0IvIAAAAaEXkAAACNiDwAAIBGRB4AAEAjIg8AAKARkQcAANCIyAMA\nAGhE5AEAADQi8gAAABoReQAAAI2IPAAAgEZEHgAAQCMiDwAAoBGRBwAA0IjIAwAAaETkAQAANCLy\nAAAAGhF5AAAAjYg8AACARkQeAABAIyIPAACgEZEHAADQiMgDAABoROQBAAA0IvIAAAAaEXkAAACN\niDwAAIBGRB4AAEAjIg8AAKARkQcAANCIyAMAAGhE5AEAADQi8gAAABoReQAAAI2IPAAAgEZEHgAA\nQCMiDwAAoBGRBwAA0IjIAwAAaETkAQAANCLyAAAAGhF5AAAAjYg8AACARkQeAABAIyIPAACgEZEH\nAADQiMgDAABoROQBAAA0IvIAAAAaEXkAAACNiDwAAIBGRB4AAEAjIg8AAKARkQcAANCIyAMAAGhE\n5AEAADQi8gAAABoReQAAAI2IPAAAgEZEHgAAQCMiDwAAoBGRBwAA0IjIAwAAaETkAQAANCLyAAAA\nGhF5AAAAjYg8AACARkQeAABAIyIPAACgEZEHAADQiMgDAABoROQBAAA0IvIAAAAaEXkAAACNiDwA\nAIBGRB4AAEAjIg8AAKARkQcAANCIyAMAAGhE5AEAADQi8gAAABoReQAAAI2IPAAAgEZEHgAAQCMi\nDwAAoBGRBwAA0IjIAwAAaETkAQAANCLyAAAAGhF5AAAAjYg8AACARkQeAABAIyIPAACgEZEHAADQ\niMgDAABoROQBAAA0IvIAAAAaEXkAAACNiDwAAIBGRB4AAEAjIg8AAKARkQcAANCIyAMAAGhE5AEA\nADQi8gAAABoReQAAAI2IPAAAgEZEHgAAQCMiDwAAoBGRBwAA0IjIAwAAaETkAQAANCLyAAAAGhF5\nAAAAjYg8AACARkQeAABAIyIPAACgEZEHAADQiMgDAABoROQBAAA0IvIAAAAaEXkAAACNiDwAAIBG\nRB4AAEAjIg8AAKARkQcAANCIyAMAAGhE5AEAADQi8gAAABoReQAAAI2IPAAAgEZEHgAAQCMiDwAA\noBGRBwAA0IjIAwAAaETkAQAANCLyAAAAGhF5AAAAjYg8AACARkQeAABAIyIPAACgEZEHAADQiMgD\nAABoROQBAAA0IvIAAAAaEXkAAACNiDwAAIBGRB4AAEAjIg8AAKARkQcAANCIyAMAAGhE5AEAADQi\n8gAAABoReQAAAI2IPAAAgEZEHgAAQCMiDwAAoBGRBwAA0IjIAwAAaETkAQAANCLyAAAAGhF5AAAA\njYg8AACARkQeAABAIyIPAACgEZEHAADQiMgDAABoROQBAAA0IvIAAAAaEXkAAACNiDwAAIBGRB4A\nAEAjIg8AAKARkQcAANCIyAMAAGhE5AEAADQi8gAAABoReQAAAI2IPAAAgEZEHgAAQCMiDwAAoBGR\nBwAA0IjIAwAAaETkAQAANCLyAAAAGhF5AAAAjYg8AACARkQeAABAIyIPAACgEZEHAADQiMgDAABo\nROQBAAA0IvIAAAAaEXkAAACNiDwAAIBGRB4AAEAjIg8AAKARkQcAANCIyAMAAGhE5AEAADQi8gAA\nABoReQAAAI2IPAAAgEZEHgAAQCMiDwAAoBGRBwAA0IjIAwAAaETkAQAANCLyAAAAGhF5AAAAjYg8\nAACARkQeAABAIyIPAACgEZEHAADQiMgDAABoROQBAAA0IvIAAAAaEXkAAACNiDwAAIBGRB4AAEAj\nIg8AAKARkQcAANCIyAMAAGhE5AEAADQi8gAAABoReQAAAI2IPAAAgEZEHgAAQCMiDwAAoBGRBwAA\n0IjIAwAAaETkAQAANCLyAAAAGhF5AAAAjYg8AACARkQeAABAIyIPAACgEZEHAADQiMgDAABoROQB\nAAA0IvIAAAAaEXkAAACNiDwAAIBGRB4AAEAjIg8AAKARkQcAANCIyAMAAGhE5AEAADQi8gAAABoR\neQAAAI2IPAAAgEZEHgAAQCMiDwAAoBGRBwAA0IjIAwAAaETkAQAANCLyAAAAGhF5AAAAjYg8AACA\nRkQeAABAIyIPAACgEZEHAADQiMgDAABoROQBAAA0IvIAAAAaEXkAAACNiDwAAIBGRB4AAEAjIg8A\nAKARkQcAANCIyAMAAGhE5AEAADQi8gAAABoReQAAAI2IPAAAgEZEHgAAQCMiDwAAoBGRBwAA0IjI\nAwAAaETkAQAANCLyAAAAGhF5AAAAjYg8AACARkQeAABAIyIPAACgEZEHAADQiMgDAABoROQBAAA0\nIvIAAAAaEXkAAACNiDwAAIBGRB4AAEAjIg8AAKARkQcAANCIyAMAAGhE5AEAADQi8gAAABoReQAA\nAI2IPAAAgEZEHgAAQCMiDwAAoBGRBwAA0IjIAwAAaKRt5FXVX1bVnVV14dyzAAAA7C1tIy/J2Ule\nMfcQAAAAe1PbyBtjfCzJf889B3vfBRdcMPcIrCPbsw/bshfbsxfbsw/bkqRx5LF1+cutF9uzD9uy\nF9uzF9uzD9uSZINEXlU9u6ouqqovVNXOqjrpPh7z6qq6saq+WlVXVNUz55gVAABgI9sQkZdk/yTX\nJTk1yVh9Z1W9JMnbk5yR5GlJrk9ycVUdsOIxp1bVtVW1vaoesXfGBgAA2FgeOvcASTLG+HCSDydJ\nVdV9POS1Sf5kjHH+4jGnJPmJJNuSnLl4jnOTnLtqvVp8AQAAbAkbIvIeSFU9LMkzkrxl17Ixxqiq\njyQ57gHW+7skRyXZv6puSvLTY4wr7+fh+ybJDTfcsG5zM58dO3Zk+/btc4/BOrE9+7Ate7E9e7E9\n+7At+1jRJvuudd0a41uOjpxVVe1M8pNjjIsWtx+f5AtJjlsZaVX1tiTPGWPcb+it4TVfluR9e/o8\nAAAA6+zlY4z3r2WFDb8nby+5OMnLk3wuyT3zjgIAAJB9kzwpU6usyWaIvNuTfCPJQauWH5Tk1vV4\ngTHGHUnWVMcAAADfZh9fZqWNcnXN+zXGuDfJNUlO2LVscXGWE7LkDw0AANDVhtiTV1X7Jzksu6+E\neWhVHZ3kzjHGzUnOSnJeVV2T5KpMV9v8jiTnzTAuAADAhrUhLrxSVccnuTTf+hl57x5jbFs85tQk\nb8h0mOZ1SX5tjHH1Xh0UAABgg9sQh2uOMf5hjLHPGOMhq762rXjMuWOMJ40x9htjHLdegVdVr66q\nG6vqq1V1RVU9cz2el72rqp5dVRdV1ReqamdVnTT3TCynqk6vqquq6q6q+mJV/VVVPWXuuVhOVZ1S\nVddX1Y7F18er6sfnnos9V1W/ufj79qy5Z2HtquqMxfZb+fWpuedieVX1PVX1nqq6vaq+svi79+lz\nz8XaLdpk9Z/PnVV1zoN9jg0ReXOpqpckeXuSM5I8Lcn1SS6uqgNmHYxl7J9pD++p+dY9wmwuz05y\nTpJjkzw3ycOS/G1V7TfrVCzr5iS/keTpmT7z9JIkf11VR8w6FXtk8YboL2f6vcnm9clMR0g9bvH1\nI/OOw7Kq6tFJLk/ytSQvSHJEktcn+dKcc7G0Y7L7z+Xjkjwv079vL3ywT7AhDtecS1VdkeTKMcZp\ni9uV6R8k7xxjnDnrcCxt9Wctsrkt3nT5z0yfi3nZ3POw56rqjiS/PsZ419yzsHZV9chMF0T7lSRv\nTHLtGON1807FWlXVGUleNMawp6eBqnprps+UPn7uWVh/VXV2kheOMR70kU1bdk9eVT0s07vKf79r\n2ZiK9yNJ9vgD1oF18+hM717dOfcg7Jmq2qeqfjbThbM+Mfc8LO2PknxwjHHJ3IOwxw5fnObwb1X1\n3qp64twDsbQTk1xdVRcuTnXYXlWvmnso9tyiWV6e5M/Xst6WjbwkByR5SJIvrlr+xUy7RYGZLfau\nn53ksjGGc0U2qao6sqr+K9NhROcmOXmM8emZx2IJi0j/oSSnzz0Le+yKJK/MdGjfKUmenORjiyue\ns/kcmmnv+r8meX6SP07yzqp6xaxTsR5OTvJdSd69lpU2xEcoANyPc5P8QJIfnnsQ9sinkxyd6ZfU\ni5OcX1XPEXqbS1U9IdObLs9dfIYtm9gY4+IVNz9ZVVcl+XySn0niUOrNZ58kV40x3ri4fX1VHZkp\n4N8z31isg21JPjTGuHUtK23lPXm3J/lGphOOVzooyZr+JwLrr6r+MMkLk/zoGOOWuedheWOM/xlj\n/PsY49oxxm9luljHaXPPxZo9I8mBSbZX1b1VdW+S45OcVlVfX+x5Z5MaY+xI8plMn1vM5nNLkhtW\nLbshycEzzMI6qaqDM12E7k/Xuu6WjbzFu5DXJDlh17LFL6gTknx8rrmA/wu8FyX5sTHGTXPPw7rb\nJ8kj5h6CNftIkh/MdLjm0Yuvq5O8N8nRYytfya2BxQV1DssUC2w+lyd56qplT820d5bNa1umU8n+\nZq0rbvXDNc9Kcl5VXZPkqiSvzXRBgPPmHIq1W5xDcFiSXe8kH1pVRye5c4xx83yTsVZVdW6SlyY5\nKcndVbVrb/uOMcY9803GMqrqLUk+lOSmJI/KdPL48ZnOGWETGWPcneSbzo2tqruT3DHGWL0HgQ2u\nqn4/yQczRcD3JnlTknuTXDDnXCztHUkur6rTM11m/9gkr0ryS7NOxdIWO59emeS8McbOta6/pSNv\njHHh4vLsb850mOZ1SV4wxrht3slYwjFJLs10FcaR6fMPk+kk1W1zDcVSTsm0DT+6avkvJDl/r0/D\nnnpspj+Hj0+yI8k/JXm+KzO2Ye/d5vWEJO9P8pgktyW5LMmzxhh3zDoVSxljXF1VJyd5a6aPNrkx\nyWljjA/MOxl74LlJnpglz5Hd0p+TBwAA0M2WPScPAACgI5EHAADQiMgDAABoROQBAAA0IvIAAAAa\nEXkAAACNiDwAAIBGRB4AAEAjIg8AAKARkQcASarq0qo6ay+/5iFVtbOqjtqbrwtAbyIPANZBVR2/\nCLbvXOOq49syEABblsgDgPVRmYKtllgPANaNyAOA3R5aVedU1Zer6raqevOuO6rq56rqH6vqrqq6\npareV1UHLu47JMkli4d+qaq+UVV/sbivquoNVfXZqrqnqj5XVaevet3vq6pLquruqrquqp61V35a\nAFoSeQCw2yuT3JvkmUlek+R1VfWLi/semuS3kxyV5EVJDknyrsV9Nyf5qcX3hyd5fJLTFrffmuQN\nSd6U5IgkL0ly66rX/d0kZyY5Oslnkry/qvyOBmApNYZTAQCgqi5NcuAY48gVy34vyYkrl62475gk\nVyZ51BjjK1V1fKa9ed89xrhr8ZhHJrktyaljjHfdx3MckuTGJNvGGOctlh2R5JNJjhhjfGadf0wA\ntgDvEgLAblesuv2JJIcvDrl8RlVdVFWfr6q7knx08ZiDH+D5jkjy8Ow+lPP+/POK72/JdJ7eYx/8\n2ACwm8gDgP/ffkk+nOTLSV6W5JgkJy/ue/gDrPfVB/n89674ftchNn5HA7AUv0AAYLdjV90+Lsln\nk3x/ksckOX2McfniMMqDVj3264v/PmTFss8muSfJCQ/wms6bAGBdiTwA2O3gqvqDqnpKVb00ya8m\nOTvJTZki7jVV9eSqOinTRVhW+nymYDuxqg6oqv3HGF9L8rYkZ1bVK6rq0Ko6tqq2rVjPRygAsK5E\nHgBMRpLzMx2aeVWSc5K8Y4zxZ2OM25P8fJIXJ/mXTFfLfP03rTzGfyQ5I9PVNG9drJ8kv5Pk7Zmu\nrvmpJB9IcuCq172vWQBgKa6uCQAA0Ig9eQAAAI2IPAAAgEZEHgAAQCMiDwAAoBGRBwAA0IjIAwAA\naETkAQAANCLyAAAAGhF5AAAAjYg8AACARkQeAABAIyIPAACgkf8FzmLS7je9aPAAAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1418bcb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########################\n",
    "# VISUALISATIONS - ERROR #\n",
    "##########################\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "\n",
    "plt.figure(fig_num)\n",
    "ax = plt.subplot(1,1,1)\n",
    "sc = pandas.Series(error_means)\n",
    "ma = sc.rolling(window=500).mean()\n",
    "ax.plot(sc.index, sc, color='lightgray')\n",
    "ax.plot(ma.index, ma, color='red')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(sc.index.min(), sc.index.max())\n",
    "ax.set_title('Error')\n",
    "ax.set_xlabel('batch')\n",
    "ax.set_ylabel('error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on sequences of length 13\n",
      "\n",
      "Batch - 1, Mean error - 0.879077\n",
      "Batch - 2, Mean error - 0.896\n",
      "Batch - 3, Mean error - 0.892308\n",
      "Batch - 4, Mean error - 0.883077\n",
      "\n",
      "###########\n",
      "# Summary #\n",
      "###########\n",
      "\n",
      "model         - ntm\n",
      "task name     - copy\n",
      "epochs        - 2\n",
      "num_classes   - 10\n",
      "N             - 10\n",
      "Ntest         - 15\n",
      "# weights     - 18958\n",
      "\n",
      "\n",
      "test error    - 0.887615\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "# Restore the weights from training\n",
    "sess = tf.Session()\n",
    "saver.restore(sess,save_path)\n",
    "\n",
    "inputs_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "targets_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "# Set up test graph\n",
    "rnn_outputs_test = []\n",
    "reuse = True\n",
    "for i in range(Ntest + Ntest_out):\n",
    "    output, state = cell(inputs_test[i],state,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "\n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size])\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.log_softmax(logit) for logit in logits_test] \n",
    "mask = [tf.sign(tf.reduce_max(tf.abs(targets_test[i]))) for i in range(Ntest + Ntest_out)]\n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets_test[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest + Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "errors_test_mask = [errors_test[i] * mask[i] for i in range(Ntest + Ntest_out)]\n",
    "mean_error_test = tf.add_n(errors_test_mask)\n",
    "mean_error_test /= tf.add_n(mask)\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "\n",
    "seq_length = Ntest\n",
    "print(\"Testing on sequences of length \" + str(seq_length-2))\n",
    "print(\"\")\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    for z in range(batch_size):\n",
    "        a, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=Ntest+Ntest_out)\n",
    "            \n",
    "        inp.append(a_onehot)\n",
    "        out.append(fa_onehot)        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        in_node = inputs_test[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        out_node = targets_test[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = sess.run(mean_error_test, feed_dict)\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"epochs        - \" + str(epoch))\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"test error    - \" + str(final_error))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
