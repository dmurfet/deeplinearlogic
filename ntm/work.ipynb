{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of the Linear Logic Recurrent Neural Network (LLRNN)\n",
    "#\n",
    "# The architecture is a modified RNN, see the paper \"Linear logic and recurrent neural networks\".\n",
    "# Our inputs are sequences of symbols taken from an alphabet of size num_classes. The length\n",
    "# of the sequences is N. Our outputs are also sequences of length N from the same alphabet.\n",
    "\n",
    "################\n",
    "# GLOBAL FLAGS #\n",
    "################\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, pattern_ntm_alt\n",
    "task                  = 'copy' # copy, repeat copy, pattern\n",
    "epoch                 = 200 # number of training epochs, default to 200\n",
    "num_classes           = 10 # number of symbols, INCLUDING initial and terminal symbols\n",
    "N                     = 30 # length of input sequences for training, default to 20, INCLUDING initial and terminal symbols\n",
    "Ntest                 = 35 # length of sequences for testing, default to N, INCLUDING initial and terminal symbols\n",
    "batch_size            = 250 # default to 500 (too large does not fit on GPUs)\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "memory_address_size   = 128 # number of memory locations, default 20\n",
    "memory_content_size   = 20 # size of vector stored at a memory location, default 5\n",
    "powers_ring1          = [0,-1,1] # powers of R used on ring 1, default [0,-1,1]\n",
    "powers_ring2          = [0,-1,1] # powers of R used on ring 2, default [0,-1,1]\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "num_training          = 10000 # default to int(training_percent * (num_classes-2)**N)\n",
    "num_test              = num_training\n",
    "init_symbol           = num_classes - 2\n",
    "term_symbol           = num_classes - 1\n",
    "seq_length_min        = 3\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there\n",
    "# 2. The initial and terminal symbols are always from the end of the list of symbols, so they\n",
    "# are respectively num_classes - 2 and num_classes - 1. So the number of symbols which are\n",
    "# not initial or terminal is num_classes - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[4, 1, 5, 6, 4, 5, 1, 1, 1, 0, 0, 1, 6, 3, 1, 5, 0, 1, 5, 6, 1, 3, 1, 7, 6, 3, 3, 5, 5, 7]\n",
      "is mapped to\n",
      "[4, 1, 5, 6, 4, 5, 1, 1, 1, 0, 0, 1, 6, 3, 1, 5, 0, 1, 5, 6, 1, 3, 1, 7, 6, 3, 3, 5, 5, 7]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "# Below N_out and Ntest_out are the lengths of the outputs in both the training\n",
    "# and testing regimes respectively. Since outputs do not include the initial and terminal\n",
    "# symbols, these default to N - 2 and Ntest - 2 respectively.\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N - 2\n",
    "    Ntest_out = Ntest - 2\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "if( task == 'repeat copy' ):\n",
    "    pattern = [0,1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 2 * (N - 2)\n",
    "    Ntest_out = 2 * (Ntest - 2)\n",
    "\n",
    "##############\n",
    "# PATTERN TASK\n",
    "if( task == 'pattern' ):\n",
    "    pattern = [1,0,0,2,0]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 2 * (N - 2)\n",
    "    Ntest_out = 2 * (Ntest - 2)\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = [random.randint(0,num_classes-3) for i in range(N)]\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# INITIALISE STATE #\n",
    "####################\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "input_size = num_classes # dimension of the input space I\n",
    "state_size = 0\n",
    "\n",
    "#####\n",
    "# NTM\n",
    "\n",
    "def init_state_ntm(batch_size, css, mas, mcs):\n",
    "    state_size = controller_state_size + 2*memory_address_size + memory_address_size * memory_content_size\n",
    "    \n",
    "    ra = [0.0]*mas\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,mas]) + ra\n",
    "    \n",
    "    # DEBUG at the moment the read and write addresses are not distributions, i.e. they do not\n",
    "    # sum to 1, but after one step the gamma sharpening will normalise them. We should probably start\n",
    "    # with things that sum to 1, though.\n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)\n",
    "    #init_controller_state = tf.get_variable(\"init_ccs\", shape=[batch_size, controller_state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    init_read_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ \\\n",
    "                       #tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    \n",
    "    init_write_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ \\\n",
    "                       #tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    \n",
    "    init_memory = tf.truncated_normal([batch_size, mas*mcs], 0.0, 1e-6, dtype=tf.float32)\n",
    "    #init_memory = tf.get_variable(\"init_mem\", shape=[batch_size, memory_address_size*memory_content_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "    return state_size, state\n",
    "\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size,\n",
    "                                  memory_address_size, memory_content_size)\n",
    "    \n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,memory_address_size,memory_content_size, powers_ring1)\n",
    "        \n",
    "#############\n",
    "# PATTERN NTM\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size = controller_state_size + 4*memory_address_size + \\\n",
    "                memory_address_size * memory_content_size + \\\n",
    "                memory_address_size * len(powers_ring1)\n",
    "\n",
    "    cell = ntm.PatternNTM(state_size,input_size,controller_state_size,\n",
    "                          memory_address_size,memory_content_size, powers_ring1, powers_ring2)\n",
    "    \n",
    "    state = tf.truncated_normal([batch_size, state_size], 0.0, 0.01, dtype=tf.float32)\n",
    "    \n",
    "#################\n",
    "# PATTERN NTM ALT\n",
    "if( use_model == 'pattern_ntm_alt' ):\n",
    "    state_size = controller_state_size + 4*memory_address_size + \\\n",
    "                memory_address_size * memory_content_size + \\\n",
    "                memory_address_size * len(powers_ring1)\n",
    "\n",
    "    cell = ntm.PatternNTM_alt(state_size,input_size,controller_state_size,\n",
    "                          memory_address_size,memory_content_size, powers_ring1, powers_ring2)\n",
    "    \n",
    "    state = tf.truncated_normal([batch_size, state_size], 0.0, 0.01, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_114/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_112/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_110/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_108/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_106/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_104/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_102/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_100/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_98/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_96/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_94/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_92/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_90/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_88/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_86/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_84/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_82/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_80/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_78/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_76/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_74/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_72/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_70/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_68/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_66/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_64/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_62/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_60/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_58/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_56/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_54/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_52/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_50/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_48/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_46/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_44/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_42/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_40/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_38/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_36/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_34/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_32/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_30/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_28/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_26/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_24/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_22/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_20/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_18/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "\n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "write_addresses = []\n",
    "gamma_writes = []\n",
    "gamma_reads = []\n",
    "ss = []\n",
    "rnn_outputs = []\n",
    "    \n",
    "for i in range(N + N_out):\n",
    "    # Logging\n",
    "    h0, curr_read, curr_write, _ = tf.split(state, [controller_state_size,\n",
    "                                                    memory_address_size,\n",
    "                                                    memory_address_size,-1], 1)\n",
    "    \n",
    "    #### RUN MODEL ####\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "    ###################\n",
    "    \n",
    "    # More logging\n",
    "    read_addresses.append(curr_read[0,:])\n",
    "    write_addresses.append(curr_write[0,:])\n",
    "    \n",
    "    with tf.variable_scope(\"NTM\",reuse=True):\n",
    "        W_gamma_write = tf.get_variable(\"W_gamma_write\", [controller_state_size,1])\n",
    "        B_gamma_write = tf.get_variable(\"B_gamma_write\", [])\n",
    "        gamma_write = 1.0 + tf.nn.relu(tf.matmul(h0,W_gamma_write) + B_gamma_write) # shape [batch_size,1]\n",
    "        \n",
    "        W_gamma_read = tf.get_variable(\"W_gamma_read\", [controller_state_size,1])\n",
    "        B_gamma_read = tf.get_variable(\"B_gamma_read\", [])\n",
    "        gamma_read = 1.0 + tf.nn.relu(tf.matmul(h0,W_gamma_read) + B_gamma_read) # shape [batch_size,1]\n",
    "        \n",
    "        W_s = tf.get_variable(\"W_s\", [controller_state_size,len(powers_ring1)])\n",
    "        B_s = tf.get_variable(\"B_s\", [len(powers_ring1)])\n",
    "        s = tf.nn.softmax(tf.matmul(h0,W_s) + B_s) # shape [batch_size,len(powers)]\n",
    "\n",
    "    gamma_writes.append(gamma_write[0,:])\n",
    "    gamma_reads.append(gamma_read[0,:])\n",
    "    ss.append(s[0,:])\n",
    "    reuse = True\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "# Note: we use log_softmax to avoid precision issues with floats causing log(0) to create NaNs\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.log_softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * prediction[i]) for i in range(N + N_out)]\n",
    "cross_entropy = -tf.add_n(ce[seq_length_min-1:])\n",
    "                    \n",
    "# optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "optimizer = tf.train.RMSPropOptimizer(1e-4,decay=0.9,momentum=0.9)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "# DEBUG: how to make this work properly for sequences of varying length?\n",
    "# We could divide sequences into buckets, and have one optimizer per bucket\n",
    "\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N + N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "mean_error = tf.scalar_mul(np.true_divide(1,N + N_out), tf.add_n(errors))\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 0 r-argmax [0] w-argmax [0] r-gamma [ 1.00000262] w-gamma [ 1.00000119]\n",
      " Step 1 r-argmax [1] w-argmax [0] r-gamma [ 1.0755856] w-gamma [ 1.2258954]\n",
      " Step 2 r-argmax [0] w-argmax [0] r-gamma [ 1.24140728] w-gamma [ 1.13187671]\n",
      " Step 3 r-argmax [0] w-argmax [0] r-gamma [ 1.] w-gamma [ 1.]\n",
      " Step 4 r-argmax [0] w-argmax [0] r-gamma [ 1.] w-gamma [ 1.]\n",
      " Step 5 r-argmax [1] w-argmax [0] r-gamma [ 1.] w-gamma [ 1.36034167]\n",
      " Step 6 r-argmax [1] w-argmax [0] r-gamma [ 1.55323601] w-gamma [ 1.]\n",
      " Step 7 r-argmax [1] w-argmax [0] r-gamma [ 1.] w-gamma [ 1.55312967]\n",
      " Step 8 r-argmax [1] w-argmax [0] r-gamma [ 1.] w-gamma [ 1.]\n",
      " Step 9 r-argmax [1] w-argmax [0] r-gamma [ 1.] w-gamma [ 1.30927384]\n",
      " Step 10 r-argmax [1] w-argmax [127] r-gamma [ 1.] w-gamma [ 1.]\n",
      " Step 11 r-argmax [0] w-argmax [127] r-gamma [ 1.02680278] w-gamma [ 1.59572411]\n",
      " Step 12 r-argmax [0] w-argmax [0] r-gamma [ 1.49362504] w-gamma [ 1.]\n",
      " Step 13 r-argmax [0] w-argmax [0] r-gamma [ 1.17273879] w-gamma [ 1.]\n",
      " Step 14 r-argmax [0] w-argmax [0] r-gamma [ 1.21179879] w-gamma [ 1.]\n",
      " Step 15 r-argmax [0] w-argmax [0] r-gamma [ 1.19785964] w-gamma [ 1.20857739]\n",
      " Step 16 r-argmax [0] w-argmax [127] r-gamma [ 1.33896065] w-gamma [ 1.]\n",
      " Step 17 r-argmax [0] w-argmax [127] r-gamma [ 1.] w-gamma [ 1.04270792]\n",
      " Step 18 r-argmax [127] w-argmax [127] r-gamma [ 1.04426348] w-gamma [ 1.27375388]\n",
      " Step 19 r-argmax [127] w-argmax [127] r-gamma [ 1.10680127] w-gamma [ 1.08063269]\n",
      " Step 20 r-argmax [127] w-argmax [127] r-gamma [ 1.] w-gamma [ 1.00549877]\n",
      " Step 21 r-argmax [127] w-argmax [126] r-gamma [ 1.] w-gamma [ 1.12125599]\n",
      " Step 22 r-argmax [127] w-argmax [126] r-gamma [ 1.07154536] w-gamma [ 1.29444528]\n",
      " Step 23 r-argmax [127] w-argmax [126] r-gamma [ 1.] w-gamma [ 1.]\n",
      " Step 24 r-argmax [127] w-argmax [126] r-gamma [ 1.51301265] w-gamma [ 1.]\n",
      " Step 25 r-argmax [127] w-argmax [126] r-gamma [ 1.53120255] w-gamma [ 1.49606991]\n",
      " Step 26 r-argmax [127] w-argmax [126] r-gamma [ 1.] w-gamma [ 1.07750523]\n",
      " Step 27 r-argmax [127] w-argmax [126] r-gamma [ 1.] w-gamma [ 1.]\n",
      " Step 28 r-argmax [127] w-argmax [126] r-gamma [ 1.36549807] w-gamma [ 1.]\n",
      " Step 29 r-argmax [127] w-argmax [125] r-gamma [ 1.] w-gamma [ 1.]\n",
      " Step 30 r-argmax [127] w-argmax [125] r-gamma [ 1.38666737] w-gamma [ 1.]\n",
      " Step 31 r-argmax [126] w-argmax [125] r-gamma [ 1.] w-gamma [ 1.02285826]\n",
      " Step 32 r-argmax [127] w-argmax [125] r-gamma [ 1.29138184] w-gamma [ 1.]\n",
      " Step 33 r-argmax [127] w-argmax [125] r-gamma [ 1.28117204] w-gamma [ 1.]\n",
      " Step 34 r-argmax [127] w-argmax [124] r-gamma [ 1.35575688] w-gamma [ 1.]\n",
      " Step 35 r-argmax [127] w-argmax [124] r-gamma [ 1.] w-gamma [ 1.19701242]\n",
      " Step 36 r-argmax [127] w-argmax [124] r-gamma [ 1.05607903] w-gamma [ 1.]\n",
      " Step 37 r-argmax [127] w-argmax [124] r-gamma [ 1.02099466] w-gamma [ 1.]\n",
      " Step 38 r-argmax [127] w-argmax [123] r-gamma [ 1.22812796] w-gamma [ 1.]\n",
      " Step 39 r-argmax [127] w-argmax [123] r-gamma [ 1.] w-gamma [ 1.17528367]\n",
      " Step 40 r-argmax [127] w-argmax [123] r-gamma [ 1.04513633] w-gamma [ 1.]\n",
      " Step 41 r-argmax [127] w-argmax [123] r-gamma [ 1.] w-gamma [ 1.10331798]\n",
      " Step 42 r-argmax [127] w-argmax [122] r-gamma [ 1.02997041] w-gamma [ 1.]\n",
      " Step 43 r-argmax [127] w-argmax [122] r-gamma [ 1.] w-gamma [ 1.00114954]\n",
      " Step 44 r-argmax [127] w-argmax [122] r-gamma [ 1.06068027] w-gamma [ 1.]\n",
      " Step 45 r-argmax [127] w-argmax [121] r-gamma [ 1.] w-gamma [ 1.]\n",
      " Step 46 r-argmax [127] w-argmax [121] r-gamma [ 1.06116378] w-gamma [ 1.]\n",
      " Step 47 r-argmax [127] w-argmax [121] r-gamma [ 1.01075912] w-gamma [ 1.]\n",
      " Step 48 r-argmax [127] w-argmax [121] r-gamma [ 1.] w-gamma [ 1.]\n",
      " Step 49 r-argmax [127] w-argmax [120] r-gamma [ 1.] w-gamma [ 1.]\n",
      " Step 50 r-argmax [127] w-argmax [120] r-gamma [ 1.05974913] w-gamma [ 1.]\n",
      " Step 51 r-argmax [127] w-argmax [120] r-gamma [ 1.] w-gamma [ 1.]\n",
      " Step 52 r-argmax [127] w-argmax [120] r-gamma [ 1.] w-gamma [ 1.]\n",
      " Step 53 r-argmax [127] w-argmax [119] r-gamma [ 1.00364685] w-gamma [ 1.]\n",
      " Step 54 r-argmax [127] w-argmax [119] r-gamma [ 1.04679346] w-gamma [ 1.]\n",
      " Step 55 r-argmax [127] w-argmax [119] r-gamma [ 1.] w-gamma [ 1.]\n",
      " Step 56 r-argmax [127] w-argmax [119] r-gamma [ 1.0389992] w-gamma [ 1.]\n",
      " Step 57 r-argmax [127] w-argmax [118] r-gamma [ 1.02767134] w-gamma [ 1.]\n",
      "Epoch - 1, length - 1, error - 0.248\n",
      "Epoch - 2, length - 19, error - 0.947369\n",
      "Epoch - 3, length - 23, error - 0.956522\n",
      "Epoch - 4, length - 9, error - 0.888889\n",
      "Epoch - 5, length - 24, error - 0.946\n",
      "Epoch - 6, length - 23, error - 0.849739\n",
      "Epoch - 7, length - 9, error - 0.825778\n",
      "Epoch - 8, length - 10, error - 0.8184\n",
      "Epoch - 9, length - 15, error - 0.814933\n",
      "Epoch - 10, length - 19, error - 0.832\n",
      "Epoch - 11, length - 18, error - 0.819111\n",
      "Epoch - 12, length - 13, error - 0.810769\n",
      "Epoch - 13, length - 15, error - 0.8176\n",
      "Epoch - 14, length - 20, error - 0.8248\n",
      "Epoch - 15, length - 14, error - 0.818571\n",
      "Epoch - 16, length - 1, error - 0.0\n",
      "Epoch - 17, length - 9, error - 0.778667\n",
      "Epoch - 18, length - 20, error - 0.8306\n",
      "Epoch - 19, length - 18, error - 0.799111\n",
      "Epoch - 20, length - 25, error - 0.82128\n",
      "Epoch - 21, length - 16, error - 0.75375\n",
      "Epoch - 22, length - 2, error - 0.5\n",
      "Epoch - 23, length - 6, error - 0.641333\n",
      "Epoch - 24, length - 15, error - 0.714667\n",
      "Epoch - 25, length - 28, error - 0.664286\n",
      " Step 0 r-argmax [0] w-argmax [0] r-gamma [ 1.04583979] w-gamma [ 1.02173829]\n",
      " Step 1 r-argmax [0] w-argmax [1] r-gamma [ 1.33141685] w-gamma [ 1.34725618]\n",
      " Step 2 r-argmax [0] w-argmax [0] r-gamma [ 2.26738024] w-gamma [ 2.36433649]\n",
      " Step 3 r-argmax [1] w-argmax [1] r-gamma [ 2.19716358] w-gamma [ 2.38217688]\n",
      " Step 4 r-argmax [1] w-argmax [0] r-gamma [ 2.03834271] w-gamma [ 1.87997913]\n",
      " Step 5 r-argmax [1] w-argmax [127] r-gamma [ 2.39517307] w-gamma [ 1.86430359]\n",
      " Step 6 r-argmax [0] w-argmax [126] r-gamma [ 3.58246207] w-gamma [ 1.56081796]\n",
      " Step 7 r-argmax [127] w-argmax [126] r-gamma [ 4.07704878] w-gamma [ 2.30366898]\n",
      " Step 8 r-argmax [126] w-argmax [125] r-gamma [ 3.74142289] w-gamma [ 3.04079437]\n",
      " Step 9 r-argmax [126] w-argmax [125] r-gamma [ 3.33778143] w-gamma [ 3.89696908]\n",
      " Step 10 r-argmax [126] w-argmax [124] r-gamma [ 3.13993478] w-gamma [ 4.31687832]\n",
      " Step 11 r-argmax [125] w-argmax [123] r-gamma [ 2.79923415] w-gamma [ 4.09982872]\n",
      " Step 12 r-argmax [124] w-argmax [122] r-gamma [ 2.58437109] w-gamma [ 3.54166436]\n",
      " Step 13 r-argmax [124] w-argmax [121] r-gamma [ 2.71109676] w-gamma [ 3.49577427]\n",
      " Step 14 r-argmax [123] w-argmax [120] r-gamma [ 3.0744102] w-gamma [ 3.50159407]\n",
      " Step 15 r-argmax [122] w-argmax [120] r-gamma [ 3.21504283] w-gamma [ 3.30038738]\n",
      " Step 16 r-argmax [121] w-argmax [119] r-gamma [ 3.07932639] w-gamma [ 3.24751115]\n",
      " Step 17 r-argmax [120] w-argmax [118] r-gamma [ 2.93572235] w-gamma [ 3.50328279]\n",
      " Step 18 r-argmax [119] w-argmax [117] r-gamma [ 2.93058419] w-gamma [ 3.80075955]\n",
      " Step 19 r-argmax [118] w-argmax [117] r-gamma [ 3.05700302] w-gamma [ 3.99742341]\n",
      " Step 20 r-argmax [117] w-argmax [116] r-gamma [ 3.09663439] w-gamma [ 4.18345261]\n",
      " Step 21 r-argmax [116] w-argmax [115] r-gamma [ 3.01114082] w-gamma [ 4.36553097]\n",
      " Step 22 r-argmax [116] w-argmax [114] r-gamma [ 3.0966959] w-gamma [ 4.33351898]\n",
      " Step 23 r-argmax [116] w-argmax [113] r-gamma [ 3.0881319] w-gamma [ 3.84355092]\n",
      " Step 24 r-argmax [115] w-argmax [112] r-gamma [ 2.94160318] w-gamma [ 3.31249142]\n",
      " Step 25 r-argmax [114] w-argmax [111] r-gamma [ 2.91696405] w-gamma [ 3.03881907]\n",
      " Step 26 r-argmax [113] w-argmax [111] r-gamma [ 2.9463954] w-gamma [ 3.06529069]\n",
      " Step 27 r-argmax [112] w-argmax [110] r-gamma [ 2.98280692] w-gamma [ 3.21150064]\n",
      " Step 28 r-argmax [111] w-argmax [109] r-gamma [ 3.01765919] w-gamma [ 3.53163218]\n",
      " Step 29 r-argmax [110] w-argmax [109] r-gamma [ 2.95999861] w-gamma [ 3.87842727]\n",
      " Step 30 r-argmax [109] w-argmax [108] r-gamma [ 2.92924309] w-gamma [ 4.14807367]\n",
      " Step 31 r-argmax [108] w-argmax [107] r-gamma [ 3.0352509] w-gamma [ 4.33463621]\n",
      " Step 32 r-argmax [108] w-argmax [106] r-gamma [ 3.11505961] w-gamma [ 4.42451954]\n",
      " Step 33 r-argmax [108] w-argmax [105] r-gamma [ 3.11705303] w-gamma [ 4.11105061]\n",
      " Step 34 r-argmax [107] w-argmax [104] r-gamma [ 2.97394514] w-gamma [ 3.45517206]\n",
      " Step 35 r-argmax [106] w-argmax [103] r-gamma [ 2.88027334] w-gamma [ 3.0462904]\n",
      " Step 36 r-argmax [105] w-argmax [103] r-gamma [ 2.9411571] w-gamma [ 3.01756263]\n",
      " Step 37 r-argmax [104] w-argmax [102] r-gamma [ 2.99793291] w-gamma [ 3.18610501]\n",
      " Step 38 r-argmax [103] w-argmax [101] r-gamma [ 3.04022074] w-gamma [ 3.48779511]\n",
      " Step 39 r-argmax [102] w-argmax [101] r-gamma [ 2.99096012] w-gamma [ 3.83187079]\n",
      " Step 40 r-argmax [101] w-argmax [100] r-gamma [ 2.9073348] w-gamma [ 4.1227293]\n",
      " Step 41 r-argmax [100] w-argmax [99] r-gamma [ 3.01355052] w-gamma [ 4.33295727]\n",
      " Step 42 r-argmax [100] w-argmax [98] r-gamma [ 3.11989427] w-gamma [ 4.43890047]\n",
      " Step 43 r-argmax [100] w-argmax [97] r-gamma [ 3.1397872] w-gamma [ 4.15645981]\n",
      " Step 44 r-argmax [99] w-argmax [96] r-gamma [ 2.98324013] w-gamma [ 3.48418045]\n",
      " Step 45 r-argmax [98] w-argmax [95] r-gamma [ 2.86724091] w-gamma [ 3.04429626]\n",
      " Step 46 r-argmax [97] w-argmax [95] r-gamma [ 2.9324131] w-gamma [ 3.0081501]\n",
      " Step 47 r-argmax [96] w-argmax [94] r-gamma [ 3.00085115] w-gamma [ 3.18100929]\n",
      " Step 48 r-argmax [95] w-argmax [93] r-gamma [ 3.04917455] w-gamma [ 3.48298001]\n",
      " Step 49 r-argmax [94] w-argmax [93] r-gamma [ 2.99695635] w-gamma [ 3.83233285]\n",
      " Step 50 r-argmax [93] w-argmax [92] r-gamma [ 2.90201855] w-gamma [ 4.12720299]\n",
      " Step 51 r-argmax [92] w-argmax [91] r-gamma [ 3.01007295] w-gamma [ 4.33999443]\n",
      " Step 52 r-argmax [92] w-argmax [90] r-gamma [ 3.12705827] w-gamma [ 4.44302845]\n",
      " Step 53 r-argmax [92] w-argmax [89] r-gamma [ 3.1420331] w-gamma [ 4.14513063]\n",
      " Step 54 r-argmax [91] w-argmax [88] r-gamma [ 2.97701025] w-gamma [ 3.46724176]\n",
      " Step 55 r-argmax [90] w-argmax [87] r-gamma [ 2.86348343] w-gamma [ 3.0261023]\n",
      " Step 56 r-argmax [89] w-argmax [87] r-gamma [ 2.93095303] w-gamma [ 3.00947547]\n",
      " Step 57 r-argmax [88] w-argmax [86] r-gamma [ 3.0055697] w-gamma [ 3.19055104]\n",
      "Epoch - 26, length - 8, error - 0.4245\n",
      "Epoch - 27, length - 11, error - 0.201455\n",
      "Epoch - 28, length - 11, error - 0.185091\n",
      "Epoch - 29, length - 26, error - 0.0676923\n",
      "Epoch - 30, length - 1, error - 0.0\n",
      "Epoch - 31, length - 13, error - 0.796923\n",
      "Epoch - 32, length - 5, error - 0.5416\n",
      "Epoch - 33, length - 23, error - 0.0805217\n",
      "Epoch - 34, length - 21, error - 0.0828571\n",
      "Epoch - 35, length - 2, error - 0.19\n",
      "Epoch - 36, length - 15, error - 0.829067\n",
      "Epoch - 37, length - 8, error - 0.668\n",
      "Epoch - 38, length - 18, error - 0.104\n",
      "Epoch - 39, length - 15, error - 0.1112\n",
      "Epoch - 40, length - 13, error - 0.123077\n",
      "Epoch - 41, length - 2, error - 0.128\n",
      "Epoch - 42, length - 7, error - 0.748\n",
      "Epoch - 43, length - 13, error - 0.804\n",
      "Epoch - 44, length - 15, error - 0.805867\n",
      "Epoch - 45, length - 28, error - 0.829572\n",
      "Epoch - 46, length - 20, error - 0.0838\n",
      "Epoch - 47, length - 5, error - 0.344\n",
      "Epoch - 48, length - 2, error - 0.426\n",
      "Epoch - 49, length - 14, error - 0.117429\n",
      "Epoch - 50, length - 18, error - 0.0835555\n",
      " Step 0 r-argmax [0] w-argmax [0] r-gamma [ 1.07826757] w-gamma [ 1.]\n",
      " Step 1 r-argmax [0] w-argmax [0] r-gamma [ 1.71246946] w-gamma [ 1.]\n",
      " Step 2 r-argmax [0] w-argmax [0] r-gamma [ 3.11622453] w-gamma [ 1.]\n",
      " Step 3 r-argmax [1] w-argmax [1] r-gamma [ 1.] w-gamma [ 2.32473612]\n",
      " Step 4 r-argmax [1] w-argmax [1] r-gamma [ 1.] w-gamma [ 6.6756506]\n",
      " Step 5 r-argmax [1] w-argmax [0] r-gamma [ 2.25375986] w-gamma [ 3.69685674]\n",
      " Step 6 r-argmax [1] w-argmax [127] r-gamma [ 2.16410828] w-gamma [ 3.33797526]\n",
      " Step 7 r-argmax [1] w-argmax [126] r-gamma [ 2.43823242] w-gamma [ 3.14738607]\n",
      " Step 8 r-argmax [1] w-argmax [125] r-gamma [ 2.73276448] w-gamma [ 3.84119749]\n",
      " Step 9 r-argmax [1] w-argmax [124] r-gamma [ 2.30534315] w-gamma [ 3.6519351]\n",
      " Step 10 r-argmax [1] w-argmax [123] r-gamma [ 2.97066116] w-gamma [ 3.55143738]\n",
      " Step 11 r-argmax [1] w-argmax [122] r-gamma [ 2.41498947] w-gamma [ 3.41840124]\n",
      " Step 12 r-argmax [1] w-argmax [121] r-gamma [ 2.37227249] w-gamma [ 3.56835842]\n",
      " Step 13 r-argmax [1] w-argmax [120] r-gamma [ 2.72138977] w-gamma [ 3.77504921]\n",
      " Step 14 r-argmax [1] w-argmax [119] r-gamma [ 2.68381691] w-gamma [ 3.30394411]\n",
      " Step 15 r-argmax [1] w-argmax [118] r-gamma [ 3.04710412] w-gamma [ 3.27441359]\n",
      " Step 16 r-argmax [1] w-argmax [117] r-gamma [ 1.95654893] w-gamma [ 3.49089026]\n",
      " Step 17 r-argmax [1] w-argmax [116] r-gamma [ 2.3003583] w-gamma [ 3.59180856]\n",
      " Step 18 r-argmax [1] w-argmax [115] r-gamma [ 2.97123003] w-gamma [ 3.41496825]\n",
      " Step 19 r-argmax [1] w-argmax [114] r-gamma [ 1.99359322] w-gamma [ 3.36088157]\n",
      " Step 20 r-argmax [1] w-argmax [113] r-gamma [ 2.35014892] w-gamma [ 3.43895197]\n",
      " Step 21 r-argmax [1] w-argmax [112] r-gamma [ 2.87402964] w-gamma [ 3.16073704]\n",
      " Step 22 r-argmax [1] w-argmax [111] r-gamma [ 3.5462811] w-gamma [ 1.]\n",
      " Step 23 r-argmax [1] w-argmax [110] r-gamma [ 4.60599947] w-gamma [ 1.]\n",
      " Step 24 r-argmax [0] w-argmax [110] r-gamma [ 4.23403645] w-gamma [ 1.]\n",
      " Step 25 r-argmax [127] w-argmax [110] r-gamma [ 5.08475971] w-gamma [ 1.]\n",
      " Step 26 r-argmax [126] w-argmax [110] r-gamma [ 4.71129704] w-gamma [ 1.]\n",
      " Step 27 r-argmax [125] w-argmax [110] r-gamma [ 4.72918415] w-gamma [ 1.]\n",
      " Step 28 r-argmax [124] w-argmax [110] r-gamma [ 4.01181984] w-gamma [ 1.]\n",
      " Step 29 r-argmax [123] w-argmax [110] r-gamma [ 4.08272886] w-gamma [ 1.]\n",
      " Step 30 r-argmax [122] w-argmax [110] r-gamma [ 5.75668764] w-gamma [ 1.]\n",
      " Step 31 r-argmax [121] w-argmax [109] r-gamma [ 4.69281387] w-gamma [ 1.]\n",
      " Step 32 r-argmax [120] w-argmax [109] r-gamma [ 4.69927406] w-gamma [ 1.]\n",
      " Step 33 r-argmax [119] w-argmax [109] r-gamma [ 3.88936162] w-gamma [ 1.]\n",
      " Step 34 r-argmax [118] w-argmax [109] r-gamma [ 4.594491] w-gamma [ 1.]\n",
      " Step 35 r-argmax [117] w-argmax [109] r-gamma [ 6.34593296] w-gamma [ 1.]\n",
      " Step 36 r-argmax [116] w-argmax [109] r-gamma [ 3.19399142] w-gamma [ 1.]\n",
      " Step 37 r-argmax [115] w-argmax [109] r-gamma [ 4.07354546] w-gamma [ 1.]\n",
      " Step 38 r-argmax [114] w-argmax [109] r-gamma [ 5.73310947] w-gamma [ 1.]\n",
      " Step 39 r-argmax [113] w-argmax [109] r-gamma [ 3.2417779] w-gamma [ 1.]\n",
      " Step 40 r-argmax [112] w-argmax [109] r-gamma [ 3.53190136] w-gamma [ 1.]\n",
      " Step 41 r-argmax [111] w-argmax [109] r-gamma [ 4.36483192] w-gamma [ 1.]\n",
      " Step 42 r-argmax [110] w-argmax [109] r-gamma [ 1.86377919] w-gamma [ 2.75037885]\n",
      " Step 43 r-argmax [109] w-argmax [108] r-gamma [ 1.20217967] w-gamma [ 4.07793522]\n",
      " Step 44 r-argmax [108] w-argmax [107] r-gamma [ 1.28523839] w-gamma [ 3.9552598]\n",
      " Step 45 r-argmax [108] w-argmax [106] r-gamma [ 1.49651039] w-gamma [ 4.22975731]\n",
      " Step 46 r-argmax [107] w-argmax [105] r-gamma [ 1.72663736] w-gamma [ 4.27989388]\n",
      " Step 47 r-argmax [107] w-argmax [104] r-gamma [ 1.79407072] w-gamma [ 4.12590122]\n",
      " Step 48 r-argmax [106] w-argmax [103] r-gamma [ 2.01954126] w-gamma [ 3.95879316]\n",
      " Step 49 r-argmax [105] w-argmax [102] r-gamma [ 2.3149724] w-gamma [ 3.72891498]\n",
      " Step 50 r-argmax [104] w-argmax [101] r-gamma [ 2.48270369] w-gamma [ 3.42942429]\n",
      " Step 51 r-argmax [103] w-argmax [100] r-gamma [ 2.63328743] w-gamma [ 3.37827754]\n",
      " Step 52 r-argmax [102] w-argmax [99] r-gamma [ 2.76753283] w-gamma [ 3.47256684]\n",
      " Step 53 r-argmax [101] w-argmax [98] r-gamma [ 2.75975704] w-gamma [ 3.5447731]\n",
      " Step 54 r-argmax [100] w-argmax [97] r-gamma [ 2.68129396] w-gamma [ 3.50320625]\n",
      " Step 55 r-argmax [99] w-argmax [96] r-gamma [ 2.62470675] w-gamma [ 3.48063827]\n",
      " Step 56 r-argmax [98] w-argmax [95] r-gamma [ 2.58940578] w-gamma [ 3.48919988]\n",
      " Step 57 r-argmax [97] w-argmax [94] r-gamma [ 2.55099154] w-gamma [ 3.5264976]\n",
      "Epoch - 51, length - 27, error - 0.0491852\n",
      "Epoch - 52, length - 3, error - 0.398667\n",
      "Epoch - 53, length - 14, error - 0.0902857\n",
      "Epoch - 54, length - 6, error - 0.198667\n",
      "Epoch - 55, length - 3, error - 0.248\n",
      "Epoch - 56, length - 4, error - 0.418\n",
      "Epoch - 57, length - 22, error - 0.078\n",
      "Epoch - 58, length - 9, error - 0.174667\n",
      "Epoch - 59, length - 14, error - 0.814571\n",
      "Epoch - 60, length - 24, error - 0.836833\n",
      "Epoch - 61, length - 21, error - 0.829905\n",
      "Epoch - 62, length - 19, error - 0.833263\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "###################\n",
    "# Note on sequences\n",
    "#\n",
    "# Our sequences are of varying length, in the alphabet {0,...,num_classes - 3}.\n",
    "# Each input sequence begins with an initial symbol and ends with a terminal symbol\n",
    "# (the value of which are num_classes - 2 and num_classes - 1 by default). Output\n",
    "# sequences do not have either an initial nor a terminal symbol.\n",
    "#\n",
    "# Both input and output sequences are written on a \"tape\" of length N + N_out.\n",
    "# Input sequences are aligned at the BEGINNING of the tape, and all remaining space\n",
    "# is filled with terminal symbols. Output sequences are aligned at the END OF THE \n",
    "# MATCHING INPUT, with all remaining space filled with terminal symbols.\n",
    "#\n",
    "# Example: suppose N = N_out = 10, and num_classes = 10 so that init_symbol = 8\n",
    "# and term_symbol = 9. Then a sequence of length 8 (seq_length = 10 below) is\n",
    "#\n",
    "# a = [4, 4, 5, 6, 3, 3, 6, 7]\n",
    "#\n",
    "# which written on the tape is\n",
    "#\n",
    "# [8, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
    "#\n",
    "# If we are performing the copy task, so that the output sequence is also a, then\n",
    "# the output written on the tape will be (notice the alignment)\n",
    "#\n",
    "# [9, 9, 9, 9, 9, 9, 9, 9, 9, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9]\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences. Each\n",
    "        # batch has a fixed length of the input sequences\n",
    "        seq_length = random.randint(seq_length_min,N)\n",
    "        \n",
    "        for z in range(batch_size):\n",
    "            a = [random.randint(0,num_classes-3) for k in range(seq_length-2)]\n",
    "            fa = [term_symbol for k in range(seq_length-1)] + a + [term_symbol for k in range(N+N_out-2*seq_length+3)]\n",
    "            a = [init_symbol] + a + [term_symbol] + [term_symbol for k in range(N+N_out-seq_length)]\n",
    "            a_onehot = [one_hots[e] for e in a]\n",
    "            fa_onehot = [one_hots[e] for e in fa]\n",
    "            inp.append(np.array(a_onehot))\n",
    "            out.append(np.array(fa_onehot))        \n",
    "        \n",
    "        # An annoying thing here is that we cannot use a list as a key in a \n",
    "        # dictionary. The workaround we found on StackOverflow here:\n",
    "        # http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "\n",
    "        # Note we have to feed all the input nodes\n",
    "        feed_dict = {}\n",
    "        for d in range(N + N_out):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N + N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "        \n",
    "        # for the first batch in an epoch, we have some logging\n",
    "        if( j == 0 and i % 25 == 0 ):\n",
    "            ss_val, gamma_reads_val, gamma_writes_val, read_addresses_val, write_addresses_val = sess.run([ss, gamma_reads,gamma_writes,read_addresses,write_addresses],feed_dict)\n",
    "    \n",
    "            s = 0\n",
    "            for r in range(len(write_addresses_val)):\n",
    "                print(\" Step \" + str(s) + \" r-argmax [\" + str(read_addresses_val[r].argmax()) + \"]\" + \n",
    "                      \" w-argmax [\" + str(write_addresses_val[r].argmax()) + \"]\" +\n",
    "                      \" r-gamma \" + str(gamma_reads_val[r]) +\n",
    "                      \" w-gamma \" + str(gamma_writes_val[r]))\n",
    "\n",
    "                # \" w-rotations \" + str(ss_val[r])\n",
    "                #if( r == len(write_addresses_val) - 1 ):\n",
    "                #    print(\"Write address -\")\n",
    "                #    print(write_addresses_val[r])               \n",
    "                s = s + 1\n",
    "        \n",
    "        ##### Do gradient descent #####\n",
    "        summary,_ = sess.run([merged_summaries,minimize], feed_dict)\n",
    "        ########\n",
    "        \n",
    "        # Write out TensorBoard logs\n",
    "        file_writer.add_summary(summary)\n",
    "    # We only compute the error of the relevant portion of the output tape\n",
    "    # DEBUG: \"relevant\" here is task specific, i.e. this needs to be changed for\n",
    "    # a task other than Copy\n",
    "    current_mean = np.mean(sess.run(errors, feed_dict)[seq_length-2:2*seq_length-4])\n",
    "    \n",
    "    # Print the mean error of the final batch in the epoch\n",
    "    print(\"Epoch - \" + str(i+1) + \", length - \" + str(seq_length - 2) + \", error - \" + str(current_mean))\n",
    "    \n",
    "# Matplotlib\n",
    "f, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "f.add_subplot(111,frameon=False)\n",
    "plt.tick_params(labelcolor='none',top='off',bottom='off',left='off',right='off')\n",
    "plt.xlabel(\"Memory location\")\n",
    "plt.ylabel(\"Time\")\n",
    "\n",
    "ax1.imshow(np.stack(write_addresses_val), cmap='bone', interpolation='nearest')\n",
    "ax1.set_title('Write address')\n",
    "\n",
    "ax2.imshow(np.stack(read_addresses_val), cmap='bone', interpolation='nearest')\n",
    "ax2.set_title('Read address')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Write out variables to disk\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took\", time.time() - pre_train_time, \"seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "# Restore the weights from training\n",
    "sess = tf.Session()\n",
    "saver.restore(sess,save_path)\n",
    "\n",
    "inputs_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "targets_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size,\n",
    "                                  memory_address_size, memory_content_size)\n",
    "    \n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,memory_address_size,memory_content_size, powers_ring1) \n",
    "\n",
    "# Set up test graph\n",
    "rnn_outputs_test = []\n",
    "reuse = True\n",
    "for i in range(Ntest + Ntest_out):\n",
    "    output, state = cell(inputs_test[i],state,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "\n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size])\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.softmax(logit) for logit in logits_test] \n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets_test[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest + Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    # We sample each batch on the fly from the set of all sequences\n",
    "    seq_length = Ntest\n",
    "    \n",
    "    for z in range(batch_size):\n",
    "        a = [random.randint(0,num_classes-3) for k in range(seq_length-2)]        \n",
    "        fa = [term_symbol for k in range(seq_length-1)] + a + [term_symbol for k in range(Ntest+Ntest_out-2*seq_length+3)]\n",
    "        a = [init_symbol] + a + [term_symbol] + [term_symbol for k in range(Ntest+Ntest_out-seq_length)]            \n",
    "        a_onehot = [one_hots[e] for e in a]\n",
    "        fa_onehot = [one_hots[e] for e in fa]\n",
    "        inp.append(np.array(a_onehot))\n",
    "        out.append(np.array(fa_onehot))        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        in_node = inputs_test[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        out_node = targets_test[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = np.mean(sess.run(errors_test, feed_dict))\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"N_out         - \" + str(N_out))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "print(\"Ntest_out     - \" + str(Ntest_out))\n",
    "print(\"ring 1 powers - \" + str(powers_ring1))\n",
    "print(\"ring 2 powers - \" + str(powers_ring2))\n",
    "print(\"# epochs      - \" + str(epoch))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"(css,mas,mcs) - (\" + str(controller_state_size) + \",\" + str(memory_address_size) + \",\" + str(memory_content_size) + \")\")\n",
    "#print(\"train percent - \" + str(training_percent))\n",
    "print(\"num_training  - \" + str(num_training) + \"/\" + str(num_classes**N))\n",
    "print(\"num_test      - \" + str(num_test) + \"/\" + str(num_classes**N))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error         - \" + str(final_error))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
