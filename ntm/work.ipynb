{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of linear logic recurrent neural network\n",
    "#\n",
    "# The architecture is a modified RNN, see the paper \"Linear logic and recurrent neural networks\".\n",
    "# Our inputs are sequences of symbols taken from an alphabet of size num_classes. The length\n",
    "# of the sequences is N. Our outputs are also sequences of length N from the same alphabet.\n",
    "#\n",
    "# Here \"symbol\" means a one hot vector.\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total state size: 340\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "# GLOBAL FLAGS\n",
    "\n",
    "num_classes = 2\n",
    "batch_size = 1000 # take a smaller batch size (500 works) on Tesla\n",
    "input_size = num_classes # dimension of the input space I\n",
    "N = 20 # length of input sequences\n",
    "N_out = 40 # length of output sequences\n",
    "training_percent = 0.01 # percentage used for training\n",
    "epoch = 200\n",
    "\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller\n",
    "memory_address_size = 20 # number of memory locations\n",
    "memory_content_size = 5 # size of vector stored at a memory location\n",
    "pattern_ntm_powers1 = [-1,0,1]\n",
    "\n",
    "use_model = 'pattern_ntm'\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size = controller_state_size + 2*memory_address_size + memory_address_size * memory_content_size\n",
    "elif( use_model == 'pattern_ntm'):\n",
    "    state_size = controller_state_size + 4*memory_address_size + \\\n",
    "                memory_address_size * memory_content_size + \\\n",
    "                memory_address_size * len(pattern_ntm_powers1)\n",
    "\n",
    "print(\"Total state size: \" + str(state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 10485 out of 1048576 sequences.\n",
      "\n",
      "Under the chosen function, the sequence\n",
      "[1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0]\n",
      "which is encoded as\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]]\n",
      "is mapped to\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "which is encoded as\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# PREPARE TRAINING DATA\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "#task = 'copy'\n",
    "#func_to_learn = learnfuncs.f_identity\n",
    "###########\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "#task = 'repeat copy'\n",
    "#pattern = [0,1]\n",
    "#func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "##################\n",
    "\n",
    "##############\n",
    "# PATTERN TASK\n",
    "task = 'pattern'\n",
    "pattern = [1,0,0,2,0]\n",
    "func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "##############\n",
    "\n",
    "# Create a shuffled list of all binary sequences of length N\n",
    "seq_input = seqhelper.shuffled_binary_seqs(N)\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "\n",
    "seq_input_onehot = []\n",
    "for i in seq_input:\n",
    "    temp_list = []\n",
    "    for j in i:\n",
    "        temp_list.append(one_hots[j])\n",
    "    seq_input_onehot.append(np.array(temp_list))\n",
    "\n",
    "# Training output\n",
    "seq_output = []\n",
    "\n",
    "for i in seq_input:\n",
    "    seq_output.append(func_to_learn(i))\n",
    "\n",
    "seq_output_onehot = []\n",
    "for i in seq_output:\n",
    "    temp_list = []\n",
    "    for j in i:\n",
    "        temp_list.append(one_hots[j])\n",
    "    seq_output_onehot.append(np.array(temp_list))\n",
    "\n",
    "NUM_EXAMPLES = int(training_percent * len(seq_input))\n",
    "\n",
    "test_input = seq_input_onehot[NUM_EXAMPLES:3*NUM_EXAMPLES]\n",
    "test_output = seq_output_onehot[NUM_EXAMPLES:3*NUM_EXAMPLES]\n",
    "train_input = seq_input_onehot[:NUM_EXAMPLES]\n",
    "train_output = seq_output_onehot[:NUM_EXAMPLES]\n",
    "\n",
    "print(\"Number of training examples: \" + str(NUM_EXAMPLES) + \" out of \" + str(len(seq_input)) + \" sequences.\")\n",
    "print(\"\")\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(seq_input[0])\n",
    "print(\"which is encoded as\")\n",
    "print(seq_input_onehot[0])\n",
    "print(\"is mapped to\")\n",
    "print(seq_output[0])\n",
    "print(\"which is encoded as\")\n",
    "print(seq_output_onehot[0])\n",
    "\n",
    "#print(\"\")\n",
    "#print(\"The first one-hot encoded digit of the first three output sequences\")\n",
    "#print(test_output[0][0])\n",
    "#print(test_output[1][0])\n",
    "#print(test_output[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_59/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_58/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_57/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_56/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_55/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_54/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_53/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_52/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_51/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_50/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_49/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_48/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_47/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_46/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_45/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_44/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_43/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_42/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_41/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_40/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_39/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_38/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_37/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_36/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_35/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_34/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_33/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_32/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_31/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_30/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_29/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_28/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_27/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_26/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_25/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_24/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_23/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_22/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_21/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_20/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_19/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_18/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_17/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_15/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_13/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_11/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_9/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_7/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_5/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_3/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_1/split_grad/concat:0' shape=(1000, 340) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# Definition of the model\n",
    "\n",
    "# inputs, we create N of them, each of shape [None,input_size], one for\n",
    "# each position in the sequence\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N_out)]\n",
    "\n",
    "# state_size is the number of hidden neurons in each layer\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,\n",
    "                   memory_address_size,memory_content_size, [-1,0,1])\n",
    "elif( use_model == 'pattern_ntm' ):\n",
    "    cell = ntm.PatternNTM(state_size,input_size,controller_state_size,\n",
    "                          memory_address_size,memory_content_size, pattern_ntm_powers1, [-1,0,1])\n",
    "\n",
    "state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "reuse = False\n",
    "\n",
    "for i in range(N):\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    reuse = True\n",
    "\n",
    "# We only start recording the outputs of the controller once we have\n",
    "# finished feeding in the input. We feed zeros as input in the second phase.\n",
    "rnn_outputs = []\n",
    "for i in range(N_out):\n",
    "    output, state = cell(tf.zeros([batch_size,input_size]),state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "\n",
    "# Final fully connected layer\n",
    "E = tf.Variable(tf.truncated_normal([controller_state_size,input_size]))\n",
    "F = tf.Variable(tf.constant(0.1, shape=[input_size]))\n",
    "\n",
    "# prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * tf.log(prediction[i])) for i in range(N_out)]\n",
    "\n",
    "cross_entropy = -tf.add_n(ce)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The mean of the errors in each digit for the test set:\n",
      "[0.53100002, 0.48800001, 0.486, 0.50099999, 0.514, 0.551, 0.51200002, 0.52200001, 0.498, 0.54799998, 0.46900001, 0.45699999, 0.50300002, 0.48300001, 0.48300001, 0.48199999, 0.50700003, 0.55599999, 0.5, 0.54500002, 0.49000001, 0.514, 0.64700001, 0.44, 0.72100002, 0.426, 0.40000001, 0.52700001, 0.486, 0.50999999, 0.514, 0.47400001, 0.602, 0.41299999, 0.56800002, 0.44299999, 0.412, 0.60000002, 0.375, 0.60299999]\n",
      "Mean: 0.507525\n"
     ]
    }
   ],
   "source": [
    "# Display the errors before training\n",
    "feed_dict = {}\n",
    "test_input_batch = test_input[:batch_size]\n",
    "test_output_batch = test_output[:batch_size]\n",
    "for d in range(N):\n",
    "    in_node = inputs[d]\n",
    "    ti = []\n",
    "    for k in range(len(test_input_batch)):\n",
    "        ti.append(test_input_batch[k][d]) # A vector giving the one-hot encoding of the dth symbol in the kth sequence\n",
    "    feed_dict[in_node] = np.array(ti)\n",
    "    \n",
    "for d in range(N_out):\n",
    "    out_node = targets[d]\n",
    "    to = []\n",
    "    for k in range(len(test_output_batch)):\n",
    "        to.append(test_output_batch[k][d])\n",
    "    feed_dict[out_node] = np.array(to)\n",
    "\n",
    "# The first three digits of this should match the printout for the\n",
    "# first three test output sequences given earlier\n",
    "#print(sess.run(tf.argmax(targets[0],1),feed_dict))\n",
    "#print(sess.run(tf.argmax(prediction[0],1),feed_dict))\n",
    "#print(sess.run(tf.not_equal(tf.argmax(targets[0], 1), tf.argmax(prediction[0], 1)),feed_dict))\n",
    "\n",
    "print(\"\")\n",
    "print(\"The mean of the errors in each digit for the test set:\")\n",
    "incorrects = sess.run(errors, feed_dict)\n",
    "print(incorrects)\n",
    "print(\"Mean: \" + str(np.mean(incorrects)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 10\n",
      "Epoch - 1, Mean - 0.49345\n",
      "Epoch - 2, Mean - 0.490475\n",
      "Epoch - 3, Mean - 0.491475\n",
      "Epoch - 4, Mean - 0.48745\n",
      "Epoch - 5, Mean - 0.481525\n",
      "Epoch - 6, Mean - 0.474225\n",
      "Epoch - 7, Mean - 0.46815\n",
      "Epoch - 8, Mean - 0.45145\n",
      "Epoch - 9, Mean - 0.427\n",
      "Epoch - 10, Mean - 0.39505\n",
      "Epoch - 11, Mean - 0.36925\n",
      "Epoch - 12, Mean - 0.35955\n",
      "Epoch - 13, Mean - 0.347875\n",
      "Epoch - 14, Mean - 0.3427\n",
      "Epoch - 15, Mean - 0.33265\n",
      "Epoch - 16, Mean - 0.325375\n",
      "Epoch - 17, Mean - 0.32795\n",
      "Epoch - 18, Mean - 0.30485\n",
      "Epoch - 19, Mean - 0.301875\n",
      "Epoch - 20, Mean - 0.289275\n",
      "Epoch - 21, Mean - 0.27865\n",
      "Epoch - 22, Mean - 0.280425\n",
      "Epoch - 23, Mean - 0.2754\n",
      "Epoch - 24, Mean - 0.264\n",
      "Epoch - 25, Mean - 0.262475\n",
      "Epoch - 26, Mean - 0.255525\n",
      "Epoch - 27, Mean - 0.250725\n",
      "Epoch - 28, Mean - 0.258225\n",
      "Epoch - 29, Mean - 0.247\n",
      "Epoch - 30, Mean - 0.237725\n",
      "Epoch - 31, Mean - 0.247375\n",
      "Epoch - 32, Mean - 0.2423\n",
      "Epoch - 33, Mean - 0.226125\n",
      "Epoch - 34, Mean - 0.216625\n",
      "Epoch - 35, Mean - 0.2293\n",
      "Epoch - 36, Mean - 0.23295\n",
      "Epoch - 37, Mean - 0.218175\n",
      "Epoch - 38, Mean - 0.20835\n",
      "Epoch - 39, Mean - 0.196525\n",
      "Epoch - 40, Mean - 0.19405\n",
      "Epoch - 41, Mean - 0.180875\n",
      "Epoch - 42, Mean - 0.217075\n",
      "Epoch - 43, Mean - 0.196975\n",
      "Epoch - 44, Mean - 0.185375\n",
      "Epoch - 45, Mean - 0.176025\n",
      "Epoch - 46, Mean - 0.16795\n",
      "Epoch - 47, Mean - 0.16485\n",
      "Epoch - 48, Mean - 0.164875\n",
      "Epoch - 49, Mean - 0.154225\n",
      "Epoch - 50, Mean - 0.151975\n",
      "Epoch - 51, Mean - 0.144425\n",
      "Epoch - 52, Mean - 0.1472\n",
      "Epoch - 53, Mean - 0.144225\n",
      "Epoch - 54, Mean - 0.13575\n",
      "Epoch - 55, Mean - 0.145575\n",
      "Epoch - 56, Mean - 0.1392\n",
      "Epoch - 57, Mean - 0.142275\n",
      "Epoch - 58, Mean - 0.1308\n",
      "Epoch - 59, Mean - 0.124225\n",
      "Epoch - 60, Mean - 0.1235\n",
      "Epoch - 61, Mean - 0.1246\n",
      "Epoch - 62, Mean - 0.12825\n",
      "Epoch - 63, Mean - 0.118275\n",
      "Epoch - 64, Mean - 0.122975\n",
      "Epoch - 65, Mean - 0.1168\n",
      "Epoch - 66, Mean - 0.121675\n",
      "Epoch - 67, Mean - 0.11405\n",
      "Epoch - 68, Mean - 0.1082\n",
      "Epoch - 69, Mean - 0.107675\n",
      "Epoch - 70, Mean - 0.1089\n",
      "Epoch - 71, Mean - 0.11475\n",
      "Epoch - 72, Mean - 0.1027\n",
      "Epoch - 73, Mean - 0.095825\n",
      "Epoch - 74, Mean - 0.09165\n",
      "Epoch - 75, Mean - 0.1104\n",
      "Epoch - 76, Mean - 0.1025\n",
      "Epoch - 77, Mean - 0.091075\n",
      "Epoch - 78, Mean - 0.0889\n",
      "Epoch - 79, Mean - 0.0817\n",
      "Epoch - 80, Mean - 0.0891\n",
      "Epoch - 81, Mean - 0.07925\n",
      "Epoch - 82, Mean - 0.0763\n",
      "Epoch - 83, Mean - 0.0779\n",
      "Epoch - 84, Mean - 0.079575\n",
      "Epoch - 85, Mean - 0.077675\n",
      "Epoch - 86, Mean - 0.09045\n",
      "Epoch - 87, Mean - 0.079875\n",
      "Epoch - 88, Mean - 0.068425\n",
      "Epoch - 89, Mean - 0.068475\n",
      "Epoch - 90, Mean - 0.067675\n",
      "Epoch - 91, Mean - 0.0861\n",
      "Epoch - 92, Mean - 0.0662\n",
      "Epoch - 93, Mean - 0.0611\n",
      "Epoch - 94, Mean - 0.078175\n",
      "Epoch - 95, Mean - 0.06165\n",
      "Epoch - 96, Mean - 0.057775\n",
      "Epoch - 97, Mean - 0.058075\n",
      "Epoch - 98, Mean - 0.068775\n",
      "Epoch - 99, Mean - 0.067675\n",
      "Epoch - 100, Mean - 0.05815\n",
      "Epoch - 101, Mean - 0.0566\n",
      "Epoch - 102, Mean - 0.064125\n",
      "Epoch - 103, Mean - 0.076925\n",
      "Epoch - 104, Mean - 0.060475\n",
      "Epoch - 105, Mean - 0.05285\n",
      "Epoch - 106, Mean - 0.051475\n",
      "Epoch - 107, Mean - 0.04995\n",
      "Epoch - 108, Mean - 0.0464\n",
      "Epoch - 109, Mean - 0.0638\n",
      "Epoch - 110, Mean - 0.05015\n",
      "Epoch - 111, Mean - 0.046675\n",
      "Epoch - 112, Mean - 0.070825\n",
      "Epoch - 113, Mean - 0.050075\n",
      "Epoch - 114, Mean - 0.051075\n",
      "Epoch - 115, Mean - 0.0414\n",
      "Epoch - 116, Mean - 0.04245\n",
      "Epoch - 117, Mean - 0.049275\n",
      "Epoch - 118, Mean - 0.048675\n",
      "Epoch - 119, Mean - 0.045625\n",
      "Epoch - 120, Mean - 0.06045\n",
      "Epoch - 121, Mean - 0.0732\n",
      "Epoch - 122, Mean - 0.05625\n",
      "Epoch - 123, Mean - 0.046275\n",
      "Epoch - 124, Mean - 0.040875\n",
      "Epoch - 125, Mean - 0.0514\n",
      "Epoch - 126, Mean - 0.0372\n",
      "Epoch - 127, Mean - 0.0346\n",
      "Epoch - 128, Mean - 0.034575\n",
      "Epoch - 129, Mean - 0.035575\n",
      "Epoch - 130, Mean - 0.032425\n",
      "Epoch - 131, Mean - 0.03725\n",
      "Epoch - 132, Mean - 0.03555\n",
      "Epoch - 133, Mean - 0.035875\n",
      "Epoch - 134, Mean - 0.03365\n",
      "Epoch - 135, Mean - 0.03565\n",
      "Epoch - 136, Mean - 0.031725\n",
      "Epoch - 137, Mean - 0.031925\n",
      "Epoch - 138, Mean - 0.040375\n",
      "Epoch - 139, Mean - 0.130325\n",
      "Epoch - 140, Mean - 0.0785\n",
      "Epoch - 141, Mean - 0.0562\n",
      "Epoch - 142, Mean - 0.039525\n",
      "Epoch - 143, Mean - 0.033025\n",
      "Epoch - 144, Mean - 0.029225\n",
      "Epoch - 145, Mean - 0.02755\n",
      "Epoch - 146, Mean - 0.02645\n",
      "Epoch - 147, Mean - 0.029875\n",
      "Epoch - 148, Mean - 0.038475\n",
      "Epoch - 149, Mean - 0.029375\n",
      "Epoch - 150, Mean - 0.02755\n",
      "Epoch - 151, Mean - 0.026875\n",
      "Epoch - 152, Mean - 0.028525\n",
      "Epoch - 153, Mean - 0.03705\n",
      "Epoch - 154, Mean - 0.058375\n",
      "Epoch - 155, Mean - 0.04115\n",
      "Epoch - 156, Mean - 0.0294\n",
      "Epoch - 157, Mean - 0.02415\n",
      "Epoch - 158, Mean - 0.02225\n",
      "Epoch - 159, Mean - 0.02125\n",
      "Epoch - 160, Mean - 0.0209\n",
      "Epoch - 161, Mean - 0.02435\n",
      "Epoch - 162, Mean - 0.02745\n",
      "Epoch - 163, Mean - 0.021375\n",
      "Epoch - 164, Mean - 0.019675\n",
      "Epoch - 165, Mean - 0.02435\n",
      "Epoch - 166, Mean - 0.0476\n",
      "Epoch - 167, Mean - 0.0314\n",
      "Epoch - 168, Mean - 0.0231\n",
      "Epoch - 169, Mean - 0.0194\n",
      "Epoch - 170, Mean - 0.017625\n",
      "Epoch - 171, Mean - 0.017475\n",
      "Epoch - 172, Mean - 0.01895\n",
      "Epoch - 173, Mean - 0.018475\n",
      "Epoch - 174, Mean - 0.015525\n",
      "Epoch - 175, Mean - 0.015025\n",
      "Epoch - 176, Mean - 0.01715\n",
      "Epoch - 177, Mean - 0.020425\n",
      "Epoch - 178, Mean - 0.02385\n",
      "Epoch - 179, Mean - 0.016075\n",
      "Epoch - 180, Mean - 0.01545\n",
      "Epoch - 181, Mean - 0.01405\n",
      "Epoch - 182, Mean - 0.018525\n",
      "Epoch - 183, Mean - 0.162575\n",
      "Epoch - 184, Mean - 0.136375\n",
      "Epoch - 185, Mean - 0.08655\n",
      "Epoch - 186, Mean - 0.0655\n",
      "Epoch - 187, Mean - 0.050575\n",
      "Epoch - 188, Mean - 0.040925\n",
      "Epoch - 189, Mean - 0.0342\n",
      "Epoch - 190, Mean - 0.02605\n",
      "Epoch - 191, Mean - 0.020925\n",
      "Epoch - 192, Mean - 0.017725\n",
      "Epoch - 193, Mean - 0.0154\n",
      "Epoch - 194, Mean - 0.01375\n",
      "Epoch - 195, Mean - 0.012075\n",
      "Epoch - 196, Mean - 0.010475\n",
      "Epoch - 197, Mean - 0.009125\n",
      "Epoch - 198, Mean - 0.008175\n",
      "Epoch - 199, Mean - 0.007525\n",
      "Epoch - 200, Mean - 0.00675\n",
      "It took 1497.93355203 seconds to train.\n"
     ]
    }
   ],
   "source": [
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(len(train_input)/batch_size)\n",
    "print(\"Number of batches: \" + str(no_of_batches))\n",
    "\n",
    "# An annoying thing here is that we cannot use a list as a key in a \n",
    "# dictionary. The workaround we found on StackOverflow here:\n",
    "# http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "\n",
    "error_means = []\n",
    "\n",
    "# epoch is a global var\n",
    "for i in range(epoch):\n",
    "    ptr = 0\n",
    "    for j in range(no_of_batches):\n",
    "        inp = train_input[ptr:ptr+batch_size]\n",
    "        out = train_output[ptr:ptr+batch_size]\n",
    "        ptr += batch_size\n",
    "        \n",
    "        feed_dict = {}\n",
    "        for d in range(N):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "        sess.run(minimize, feed_dict)\n",
    "    current_mean = np.mean(sess.run(errors, feed_dict))\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Epoch - \" + str(i+1) + \", Mean - \" + str(current_mean))\n",
    "    \n",
    "print(\"It took\", time.time() - pre_train_time, \"seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################\n",
      "# Summary \n",
      "############################\n",
      "# model = pattern_ntm\n",
      "# task = pattern\n",
      "# training_percent = 0.01\n",
      "# epoch = 200\n",
      "# (css,mas,mcs) = (100,20,5)\n",
      "# number of weights = 13527\n",
      "# error = 0.00675\n"
     ]
    }
   ],
   "source": [
    "# Display the errors after training\n",
    "#feed_dict = {}\n",
    "#for d in range(N):\n",
    "#    in_node = inputs[d]\n",
    "    \n",
    "#    ti = []\n",
    "#    for k in range(len(test_input_batch)):\n",
    "#        ti.append(test_input_batch[k][d]) # A vector giving the one-hot encoding of the dth symbol in the kth sequence\n",
    "#    feed_dict[in_node] = np.array(ti)\n",
    "    \n",
    "#for d in range(N_out):\n",
    "#    out_node = targets[d]\n",
    "    \n",
    "#    to = []\n",
    "#    for k in range(len(test_output_batch)):\n",
    "#        to.append(test_output_batch[k][d])\n",
    "#    feed_dict[out_node] = np.array(to)\n",
    "\n",
    "# The first three digits of this should match the printout for the\n",
    "# first three test output sequences given earlier\n",
    "#data = sess.run([tf.argmax(targets[0],1), tf.argmax(prediction[0],1)],feed_dict)\n",
    "\n",
    "#print(\"First digits of test outputs (actual)\")\n",
    "#print(data[0])\n",
    "#print(\"First digits of test outputs (predicted)\")\n",
    "#print(data[1])\n",
    "\n",
    "# print the mean of the errors in each digit for the test set.\n",
    "#incorrects = sess.run(errors, feed_dict)\n",
    "# print(incorrects)\n",
    "\n",
    "final_error = np.min(error_means)\n",
    "\n",
    "print(\"############################\")\n",
    "print(\"# Summary \")\n",
    "print(\"############################\")\n",
    "print(\"# model = \" + use_model)\n",
    "print(\"# task = \" + task)\n",
    "print(\"# training_percent = \" + str(training_percent))\n",
    "print(\"# epoch = \" + str(epoch))\n",
    "print(\"# (css,mas,mcs) = (\" + str(controller_state_size) + \",\" + str(memory_address_size) + \",\" + str(memory_content_size) + \")\")\n",
    "#print(\"# powers1 = \" + str(pattern_ntm_powers1))\n",
    "print(\"# number of weights = \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"# error = \" + str(final_error))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
