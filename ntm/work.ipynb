{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of linear logic recurrent neural network\n",
    "#\n",
    "# The architecture is a modified RNN, see the paper \"Linear logic and recurrent neural networks\".\n",
    "# Our inputs are sequences of symbols taken from an alphabet of size num_classes. The length\n",
    "# of the sequences is N. Our outputs are also sequences of length N from the same alphabet.\n",
    "#\n",
    "# Here \"symbol\" means a one hot vector.\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# GLOBAL FLAGS\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, pattern_ntm_alt\n",
    "task                  = 'copy' # copy, repeat copy, pattern\n",
    "epoch                 = 200 # number of training epochs, default to 200\n",
    "num_classes           = 4 # number of symbols, INCLUDING initial and terminal symbols\n",
    "N                     = 22 # length of input sequences for training, default to 20, INCLUDING initial and terminal symbols\n",
    "Ntest                 = 22 # length of sequences for testing, default to N, INCLUDING initial and terminal symbols\n",
    "batch_size            = 500 # default to 500 (too large does not fit on GPUs)\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "memory_address_size   = 20 # number of memory locations, default 20\n",
    "memory_content_size   = 5 # size of vector stored at a memory location, default 5\n",
    "powers_ring1          = [0,-1,1] # powers of R used on ring 1, default [0,-1,1]\n",
    "powers_ring2          = [0,-1,1] # powers of R used on ring 2, default [0,-1,1]\n",
    "model_optimizer       = 'rmsprop' # adam, rmsprop, default to rmsprop\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "\n",
    "training_percent      = 0.01 # percentage used for training, default 0.01\n",
    "num_training          = int(training_percent * (num_classes-2)**N)\n",
    "num_test              = num_training\n",
    "\n",
    "init_symbol           = num_classes - 2\n",
    "term_symbol           = num_classes - 1\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there\n",
    "# 2. The initial and terminal symbols are always from the end of the list of symbols, so they\n",
    "# are respectively num_classes - 2 and num_classes - 1. So the number of symbols which are\n",
    "# not initial or terminal is num_classes - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n",
      "is mapped to\n",
      "[1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "# Below N_out and Ntest_out are the lengths of the outputs in both the training\n",
    "# and testing regimes respectively. Since outputs do not include the initial and terminal\n",
    "# symbols, these default to N - 2 and Ntest - 2 respectively.\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N - 2\n",
    "    Ntest_out = Ntest - 2\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "if( task == 'repeat copy' ):\n",
    "    pattern = [0,1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 2 * (N - 2)\n",
    "    Ntest_out = 2 * (Ntest - 2)\n",
    "\n",
    "##############\n",
    "# PATTERN TASK\n",
    "if( task == 'pattern' ):\n",
    "    pattern = [1,0,0,2,0]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 2 * (N - 2)\n",
    "    Ntest_out = 2 * (Ntest - 2)\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = [random.randint(0,num_classes-3) for i in range(N)]\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_63/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_62/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_61/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_60/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_59/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_58/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_57/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_56/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_55/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_54/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_53/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_52/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_51/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_50/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_49/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_48/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_47/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_46/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_45/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_44/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_42/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_40/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_38/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_36/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_34/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_32/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_30/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_28/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_26/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_24/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_22/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_20/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_18/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "\n",
    "input_size = num_classes # dimension of the input space I\n",
    "\n",
    "# inputs, we create N of them, each of shape [None,input_size], one for each position in the sequence\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N_out)]\n",
    "\n",
    "# state_size is the number of hidden neurons in each layer\n",
    "state_size = 0\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size = controller_state_size + 2*memory_address_size + memory_address_size * memory_content_size\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,memory_address_size,memory_content_size, powers_ring1)\n",
    "elif( use_model == 'pattern_ntm' ):\n",
    "    state_size = controller_state_size + 4*memory_address_size + \\\n",
    "                memory_address_size * memory_content_size + \\\n",
    "                memory_address_size * len(powers_ring1)\n",
    "\n",
    "    cell = ntm.PatternNTM(state_size,input_size,controller_state_size,\n",
    "                          memory_address_size,memory_content_size, powers_ring1, powers_ring2)\n",
    "elif( use_model == 'pattern_ntm_alt' ):\n",
    "    state_size = controller_state_size + 4*memory_address_size + \\\n",
    "                memory_address_size * memory_content_size + \\\n",
    "                memory_address_size * len(powers_ring1)\n",
    "\n",
    "    cell = ntm.PatternNTM_alt(state_size,input_size,controller_state_size,\n",
    "                          memory_address_size,memory_content_size, powers_ring1, powers_ring2)\n",
    "\n",
    "# Initialise the state\n",
    "if( use_model == 'ntm' ):\n",
    "    # DEBUG we have switched back to the old initialisation\n",
    "    ra = [0.0]*memory_address_size\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,memory_address_size]) + ra\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, controller_state_size], 0.0, 0.01, dtype=tf.float32)\n",
    "    # was tf.truncated_normal([batch_size, controller_state_size], 0.0, 1e-6, dtype=tf.float32)\n",
    "    init_read_address = tf.random_uniform([batch_size, memory_address_size], 0.0, 0.01)\n",
    "    # was was tf.truncated_normal([batch_size, memory_address_size], 0.0, 0.01, dtype=tf.float32)\n",
    "    # was tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,memory_address_size])\n",
    "    init_write_address = tf.random_uniform([batch_size, memory_address_size], 0.0, 0.01)\n",
    "    # was tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,memory_address_size])\n",
    "    init_memory = tf.truncated_normal([batch_size, memory_address_size*memory_content_size], 0.0, 0.01, dtype=tf.float32)\n",
    "    # was tf.constant(1e-6,dtype=tf.float32,shape=[batch_size,memory_address_size*memory_content_size])\n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "else:\n",
    "    state = tf.truncated_normal([batch_size, state_size], 0.0, 0.01, dtype=tf.float32)\n",
    "\n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "write_addresses = []\n",
    "gamma_writes = []\n",
    "gamma_reads = []\n",
    "\n",
    "for i in range(N):\n",
    "    # Store read and write addresses for later logging\n",
    "    h0, curr_read, curr_write, _ = tf.split(state, [controller_state_size,memory_address_size,memory_address_size,-1], 1)\n",
    "    read_addresses.append(curr_read[0,:])\n",
    "    write_addresses.append(curr_write[0,:])\n",
    "    \n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    \n",
    "    # DEBUG, getting gammas\n",
    "    with tf.variable_scope(\"NTM\",reuse=True):\n",
    "        W_gamma_write = tf.get_variable(\"W_gamma_write\", [controller_state_size,1])\n",
    "        B_gamma_write = tf.get_variable(\"B_gamma_write\", [])\n",
    "        gamma_write = 1.0 + tf.nn.relu(tf.matmul(h0,W_gamma_write) + B_gamma_write) # shape [batch_size,1]\n",
    "        \n",
    "        W_gamma_read = tf.get_variable(\"W_gamma_read\", [controller_state_size,1])\n",
    "        B_gamma_read = tf.get_variable(\"B_gamma_read\", [])\n",
    "        gamma_read = 1.0 + tf.nn.relu(tf.matmul(h0,W_gamma_read) + B_gamma_read) # shape [batch_size,1]\n",
    "\n",
    "    gamma_writes.append(gamma_write[0,:])\n",
    "    gamma_reads.append(gamma_read[0,:])\n",
    "    reuse = True\n",
    "\n",
    "# We only start recording the outputs of the controller once we have\n",
    "# finished feeding in the input. We feed terminal symbols as input in the second phase.\n",
    "\n",
    "term_symbol_tensor = tf.constant(np.zeros([batch_size,input_size]) + one_hots[term_symbol],\n",
    "                                 dtype=tf.float32,\n",
    "                                 shape=[batch_size,input_size])\n",
    "\n",
    "rnn_outputs = []\n",
    "for i in range(N_out):\n",
    "    output, state = cell(term_symbol_tensor,state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * tf.log(prediction[i])) for i in range(N_out)]\n",
    "\n",
    "if( model_optimizer == 'adam' ):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "elif( model_optimizer == 'rmsprop' ):\n",
    "    optimizer = tf.train.RMSPropOptimizer(1e-4,decay=0.9,momentum=0.9)\n",
    "\n",
    "cross_entropy = -tf.add_n(ce)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "mean_error = tf.scalar_mul(np.true_divide(1,N_out), tf.add_n(errors))\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.02017283]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[  7.35056633e-03   6.38146373e-03   9.34334006e-03   9.75135341e-03\n",
      "   4.43387034e-05   7.75114866e-03   2.90442351e-03   3.41982231e-03\n",
      "   3.15594068e-03   9.09621920e-03   3.03715584e-03   1.95013639e-03\n",
      "   9.21407901e-03   5.72184334e-04   9.93889291e-03   1.08471513e-03\n",
      "   9.18648113e-03   5.65909129e-03   1.10880611e-03   8.85619037e-03]\n",
      "\n",
      "Step 1 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.00146949]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.06856116  0.06998821  0.07737669  0.05825428  0.05299629  0.03267584\n",
      "  0.04264426  0.0287906   0.04747958  0.04659858  0.04267447  0.04298219\n",
      "  0.03587341  0.05960014  0.0354827   0.06108827  0.0485394   0.04844635\n",
      "  0.0472395   0.05270812]\n",
      "\n",
      "Step 2 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.22366285]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.06384015  0.07204755  0.06840532  0.0627387   0.0477706   0.04276427\n",
      "  0.03462803  0.03973812  0.04103688  0.04554528  0.04406923  0.04044851\n",
      "  0.04632843  0.04355074  0.05216971  0.04838013  0.05263113  0.04806395\n",
      "  0.04950738  0.05633589]\n",
      "\n",
      "Step 3 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.57611597]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.0646437   0.06852527  0.06742828  0.05907295  0.05013144  0.04130016\n",
      "  0.03864275  0.038789    0.04225297  0.04379345  0.0432114   0.04349276\n",
      "  0.04374629  0.047319    0.04848311  0.05090258  0.04987181  0.04982053\n",
      "  0.05149298  0.05707961]\n",
      "\n",
      "Step 4 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.37051725]\n",
      "Write address -\n",
      "[ 0.06409395  0.06698688  0.06433874  0.05778741  0.04905568  0.04270437\n",
      "  0.03944999  0.04015668  0.04190478  0.04312325  0.04348961  0.0435167\n",
      "  0.04512801  0.04678714  0.04913957  0.04980254  0.050141    0.05051516\n",
      "  0.05329452  0.05858401]\n",
      "\n",
      "Step 5 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.62961185]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.06917179  0.07126705  0.06740809  0.05830013  0.04844568  0.04078281\n",
      "  0.03751809  0.03744852  0.0389936   0.04031026  0.04093325  0.04194567\n",
      "  0.04345613  0.04601175  0.04795679  0.04936181  0.0499757   0.05183045\n",
      "  0.05613399  0.06274843]\n",
      "\n",
      "Step 6 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.73158693]\n",
      "Write address -\n",
      "[ 0.06835047  0.06931254  0.06498711  0.05690888  0.04805402  0.04147853\n",
      "  0.03830512  0.03803954  0.03909779  0.04021376  0.04115405  0.04225444\n",
      "  0.04402896  0.04609765  0.0479942   0.04923993  0.05051153  0.05296657\n",
      "  0.05751844  0.06348657]\n",
      "\n",
      "Step 7 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.71436715]\n",
      "Write gamma - [ 1.32298446]\n",
      "Write address -\n",
      "[ 0.08084927  0.0821442   0.07466783  0.06120588  0.0474617   0.03747781\n",
      "  0.03239766  0.03111267  0.03192384  0.03339976  0.03494576  0.03683654\n",
      "  0.03930557  0.04228256  0.04508766  0.04757273  0.05040321  0.05517808\n",
      "  0.06296375  0.07278352]\n",
      "\n",
      "Step 8 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.29197764]\n",
      "Write gamma - [ 1.45710468]\n",
      "Write address -\n",
      "[ 0.09082291  0.08877125  0.07640015  0.05913682  0.04352388  0.03316599\n",
      "  0.02813762  0.02688251  0.02763996  0.02919072  0.03113949  0.0335851\n",
      "  0.03667211  0.04007626  0.0434318   0.04688179  0.05164117  0.05934842\n",
      "  0.0705848   0.08296727]\n",
      "\n",
      "Step 9 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.58396626]\n",
      "Write gamma - [ 1.46450555]\n",
      "Write address -\n",
      "[ 0.10749115  0.10520315  0.08784496  0.0636465   0.04246209  0.02882511\n",
      "  0.02210678  0.01991051  0.02018613  0.02166078  0.02379164  0.02655922\n",
      "  0.03002544  0.03403914  0.03835825  0.0433217   0.05031052  0.06137335\n",
      "  0.07742625  0.09545721]\n",
      "\n",
      "Step 10 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.62491918]\n",
      "Write gamma - [ 1.45903516]\n",
      "Write address -\n",
      "[ 0.13281475  0.12660736  0.09843311  0.06371577  0.0367928   0.02144045\n",
      "  0.01453567  0.01227196  0.01232559  0.01356755  0.01557222  0.01831486\n",
      "  0.02186777  0.02618446  0.03127671  0.03784495  0.04786808  0.06427321\n",
      "  0.08851314  0.11577956]\n",
      "\n",
      "Step 11 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.87030768]\n",
      "Write gamma - [ 1.0707376]\n",
      "Write address -\n",
      "[ 0.16274112  0.15530574  0.11475831  0.06653647  0.03227026  0.01498926\n",
      "  0.00810631  0.00589261  0.0056344   0.0063314   0.00768439  0.00970157\n",
      "  0.01250796  0.01623512  0.02116768  0.02832292  0.04027554  0.06134291\n",
      "  0.09485371  0.13534237]\n",
      "\n",
      "Step 12 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.76219964]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.16257225  0.15505621  0.11629401  0.06911125  0.03405943  0.01548824\n",
      "  0.00781771  0.00526766  0.00483789  0.00539602  0.00661025  0.00847196\n",
      "  0.01110024  0.01467735  0.01962754  0.0271724   0.04007149  0.0625359\n",
      "  0.09700906  0.13682313]\n",
      "\n",
      "Step 13 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.75808597]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.15467998  0.14814666  0.11501564  0.07258982  0.03832375  0.01823546\n",
      "  0.00909146  0.00578514  0.00506695  0.00553475  0.00673661  0.00861483\n",
      "  0.01127192  0.0149294   0.02013257  0.02825698  0.04201831  0.0648804\n",
      "  0.0976628   0.13302657]\n",
      "\n",
      "Step 14 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.60295939]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.14673352  0.13750556  0.10770199  0.07067642  0.0395967   0.01995961\n",
      "  0.01019422  0.00636118  0.00542379  0.00586193  0.00712791  0.00912303\n",
      "  0.01195317  0.01592979  0.02182405  0.03129764  0.04700254  0.07122965\n",
      "  0.10244213  0.13205518]\n",
      "\n",
      "Step 15 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.28262544]\n",
      "Write gamma - [ 1.18692684]\n",
      "Write address -\n",
      "[ 0.13914675  0.12900835  0.10232876  0.06955975  0.0410633   0.02187066\n",
      "  0.01152584  0.00709517  0.00585197  0.00620982  0.00751353  0.00961372\n",
      "  0.01263507  0.01700221  0.02368821  0.03447209  0.0515914   0.07601283\n",
      "  0.10465518  0.1291554 ]\n",
      "\n",
      "Step 16 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.42679191]\n",
      "Write gamma - [ 1.37293017]\n",
      "Write address -\n",
      "[ 0.14581846  0.13099726  0.10006707  0.06514723  0.03660432  0.01839513\n",
      "  0.0090311   0.0051339   0.0040022   0.0042241   0.00527309  0.00707952\n",
      "  0.00987357  0.01428274  0.02161795  0.03406603  0.05406553  0.08209805\n",
      "  0.11364666  0.13857611]\n",
      "\n",
      "Step 17 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.64467406]\n",
      "Write gamma - [ 1.16668117]\n",
      "Write address -\n",
      "[ 0.16364487  0.13676582  0.09401666  0.05337628  0.02540225  0.01055225\n",
      "  0.00423522  0.00202314  0.0014534   0.00157621  0.00215636  0.00327203\n",
      "  0.00527573  0.00902209  0.01634252  0.03046615  0.05523681  0.09156206\n",
      "  0.13223617  0.16138399]\n",
      "\n",
      "Step 18 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.54965734]\n",
      "Write gamma - [ 1.65812242]\n",
      "Write address -\n",
      "[ 0.16176897  0.12560315  0.0800188   0.04205832  0.01850416  0.0070769\n",
      "  0.00259909  0.00115253  0.00082489  0.00095559  0.00142959  0.00240109\n",
      "  0.0043557   0.00847077  0.01724104  0.03477683  0.06504451  0.10673061\n",
      "  0.14821319  0.17077422]\n",
      "\n",
      "Step 19 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.72947645]\n",
      "Write gamma - [ 1.13341558]\n",
      "Write address -\n",
      "[  1.79095358e-01   1.16340950e-01   5.73098920e-02   2.15064324e-02\n",
      "   6.22793147e-03   1.44666538e-03   3.03265784e-04   7.96909226e-05\n",
      "   4.49456456e-05   5.92468532e-05   1.21816105e-04   3.06405273e-04\n",
      "   8.80283595e-04   2.76949094e-03   8.83092452e-03   2.58056633e-02\n",
      "   6.32975772e-02   1.23039752e-01   1.83888078e-01   2.08645612e-01]\n",
      "\n",
      "Step 20 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.58483672]\n",
      "Write gamma - [ 1.31455469]\n",
      "Write address -\n",
      "[  1.67817265e-01   1.05891794e-01   5.12364097e-02   1.90004203e-02\n",
      "   5.41665172e-03   1.20836496e-03   2.26123171e-04   4.61419550e-05\n",
      "   2.10893086e-05   3.03539491e-05   7.55123474e-05   2.34636653e-04\n",
      "   8.28843738e-04   3.05845938e-03   1.05915358e-02   3.12640630e-02\n",
      "   7.38591924e-02   1.35291159e-01   1.89927325e-01   2.03974620e-01]\n",
      "\n",
      "Step 21 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.63247657]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[  1.70054749e-01   9.88259986e-02   4.23887148e-02   1.33686773e-02\n",
      "   3.09125870e-03   5.26876771e-04   6.87899810e-05   8.32392379e-06\n",
      "   2.20338234e-06   3.41230998e-06   1.26081313e-05   6.15018653e-05\n",
      "   3.33476492e-04   1.74819992e-03   7.82096758e-03   2.74622161e-02\n",
      "   7.25022927e-02   1.41557649e-01   2.03724936e-01   2.16437161e-01]\n",
      "Epoch - 1, Mean error of final batch in epoch - 0.4186\n",
      "Epoch - 2, Mean error of final batch in epoch - 0.3121\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "# An annoying thing here is that we cannot use a list as a key in a \n",
    "# dictionary. The workaround we found on StackOverflow here:\n",
    "# http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "\n",
    "# epoch is a global var\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences\n",
    "        for z in range(batch_size):\n",
    "            # construct a sequence from 0,...,num_classes - 3 then append initial and terminal symbols\n",
    "            a = [random.randint(0,num_classes-3) for k in range(N-2)]\n",
    "            fa = func_to_learn(a)\n",
    "            a = [init_symbol] + a + [term_symbol]\n",
    "            a_onehot = [one_hots[e] for e in a]\n",
    "            fa_onehot = [one_hots[e] for e in fa]\n",
    "            inp.append(np.array(a_onehot))\n",
    "            out.append(np.array(fa_onehot))        \n",
    "        \n",
    "        feed_dict = {}\n",
    "        for d in range(N):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "        \n",
    "        # for the first batch in an epoch, we have some logging\n",
    "        if( j == 0 and i % 50 == 0 ):\n",
    "            gamma_reads_val, gamma_writes_val, read_addresses_val, write_addresses_val = sess.run([gamma_reads,gamma_writes,read_addresses,write_addresses],feed_dict)\n",
    "    \n",
    "            s = 0\n",
    "            for r in range(len(write_addresses_val)):\n",
    "                print(\"\")\n",
    "                print(\"Step \" + str(s) + \" of the RNN run on the first input of first batch of this epoch\")\n",
    "                print(\"Read gamma - \" + str(gamma_reads_val[r]))\n",
    "                print(\"Write gamma - \" + str(gamma_writes_val[r]))\n",
    "                print(\"Write address -\")\n",
    "                print(write_addresses_val[r])\n",
    "                s = s + 1\n",
    "        \n",
    "        # Do gradient descent\n",
    "        summary,_ = sess.run([merged_summaries,minimize], feed_dict)\n",
    "        \n",
    "        # Write out TensorBoard logs\n",
    "        file_writer.add_summary(summary)\n",
    "    current_mean = np.mean(sess.run(errors, feed_dict))\n",
    "    print(\"Epoch - \" + str(i+1) + \", Mean error of final batch in epoch - \" + str(current_mean))\n",
    "    \n",
    "    # DEBUG\n",
    "    #with tf.variable_scope(\"NTM\",reuse=True):\n",
    "    #    H = tf.get_variable(\"H\", [controller_state_size,controller_state_size])\n",
    "    #    print(sess.run(H))\n",
    "\n",
    "# Write out variables to disk\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took\", time.time() - pre_train_time, \"seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "# Note that all the weights will be loaded from the saved training session\n",
    "inputs_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest)]\n",
    "targets_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest_out)]\n",
    "state_test = tf.truncated_normal([batch_size, state_size], 0.0, 0.01, dtype=tf.float32)\n",
    "\n",
    "# Set up test graph\n",
    "reuse = True\n",
    "for i in range(Ntest):\n",
    "    output, state = cell(inputs_test[i],state_test,'NTM',reuse)\n",
    "\n",
    "rnn_outputs_test = []\n",
    "for i in range(Ntest_out):\n",
    "    output, state = cell(tf.zeros([batch_size,input_size]),state_test,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "    \n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.softmax(logit) for logit in logits_test] \n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets_test[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "\n",
    "# Restore the weights from training\n",
    "sess = tf.Session()\n",
    "saver.restore(sess,save_path)\n",
    "\n",
    "# DEBUG\n",
    "#with tf.variable_scope(\"NTM\",reuse=True):\n",
    "#    H = tf.get_variable(\"H\", [controller_state_size,controller_state_size])\n",
    "#    print(sess.run(H))\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "#print(\"Number of batches: \" + str(no_of_batches))\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    # We sample each batch on the fly from the set of all sequences\n",
    "    for z in range(batch_size):\n",
    "        a = [random.randint(0,num_classes-3) for k in range(Ntest-2)]\n",
    "        fa = func_to_learn(a)\n",
    "        a = [init_symbol] + a + [term_symbol]\n",
    "        a_onehot = [one_hots[e] for e in a]\n",
    "        fa_onehot = [one_hots[e] for e in fa]\n",
    "        inp.append(np.array(a_onehot))\n",
    "        out.append(np.array(fa_onehot))        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(Ntest):\n",
    "        in_node = inputs_test[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest_out):\n",
    "        out_node = targets_test[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = np.mean(sess.run(errors_test, feed_dict))\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "# The first three digits of this should match the printout for the\n",
    "# first three test output sequences given earlier\n",
    "#data = sess.run([tf.argmax(targets[0],1), tf.argmax(prediction[0],1)],feed_dict)\n",
    "\n",
    "#print(\"First digits of test outputs (actual)\")\n",
    "#print(data[0])\n",
    "#print(\"First digits of test outputs (predicted)\")\n",
    "#print(data[1])\n",
    "\n",
    "# print the mean of the errors in each digit for the test set.\n",
    "#incorrects = sess.run(errors, feed_dict)\n",
    "# print(incorrects)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"N_out         - \" + str(N_out))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "print(\"Ntest_out     - \" + str(Ntest_out))\n",
    "print(\"ring 1 powers - \" + str(powers_ring1))\n",
    "print(\"ring 2 powers - \" + str(powers_ring2))\n",
    "print(\"# epochs      - \" + str(epoch))\n",
    "print(\"optimizer     - \" + str(model_optimizer))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"(css,mas,mcs) - (\" + str(controller_state_size) + \",\" + str(memory_address_size) + \",\" + str(memory_content_size) + \")\")\n",
    "print(\"train percent - \" + str(training_percent))\n",
    "print(\"num_training  - \" + str(num_training) + \"/\" + str(num_classes**N))\n",
    "print(\"num_test      - \" + str(num_test) + \"/\" + str(num_classes**N))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error         - \" + str(final_error))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
