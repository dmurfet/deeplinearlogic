{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of the Linear Logic Recurrent Neural Network (LLRNN)\n",
    "#\n",
    "# Version 10.0\n",
    "\n",
    "###################\n",
    "# HYPERPARAMETERS #\n",
    "###################\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, mult_pattern_ntm\n",
    "task                  = 'copy' # copy, repeat copy, pattern i, mult pattern i, variable pattern i\n",
    "epoch                 = 100 # number of training epochs, default to 100\n",
    "num_classes           = 10 # number of symbols, INCLUDING initial and terminal symbols, default 10\n",
    "N                     = 30 # length of input sequences for training, default to 30\n",
    "Ntest                 = 35 # length of sequences for testing, default to 35\n",
    "batch_size            = 250 # default 250\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "num_training          = 10000 # default 10000\n",
    "num_test              = num_training\n",
    "term_symbol           = num_classes - 1\n",
    "init_symbol           = num_classes - 2\n",
    "div_symbol            = num_classes - 3\n",
    "learning_rate         = 1e-4 # default 1e-4\n",
    "memory_init_bias      = 1.0 # default 1.0\n",
    "use_curriculum        = True # default True\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "\n",
    "##################\n",
    "# MODEL SPECIFIC #\n",
    "##################\n",
    "\n",
    "ntm_memory_address_size   = 128 # number of memory locations, default 128\n",
    "ntm_memory_content_size   = 20 # size of vector stored at a memory location, default 20\n",
    "ntm_powers                = [0,-1,1] # powers of R used by controller, default [0,-1,1]\n",
    "\n",
    "pattern_ntm_powers               = [[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "pattern_ntm_powers_2_on_1        = [0,1,2] # allowed powers used by ring 2 to manipulate ring 1\n",
    "pattern_ntm_memory_address_sizes = [128, 1] # number of memory locations for the three rings\n",
    "pattern_ntm_memory_content_sizes = [20, 3] # size of content vector for each ring\n",
    "pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "mult_pattern_ntm_powers               = [[0,-1,1],[0,-1,1],[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "mult_pattern_ntm_powers_2_on_1        = [0,1,2] # allowed powers used by rings 2,3 to manipulate ring 1\n",
    "mult_pattern_ntm_memory_address_sizes = [128, 20, 20, 10] # number of memory locations for the rings\n",
    "mult_pattern_ntm_memory_content_sizes = [20, 3, 3, 2] # size of content vector for each ring\n",
    "mult_pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there\n",
    "# 2. The initial and terminal symbols are always from the end of the list of symbols, so they\n",
    "# are respectively num_classes - 2 and num_classes - 1. So the number of symbols which are\n",
    "# not initial or terminal is num_classes - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import pandas\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs\n",
    "\n",
    "assert use_model == 'ntm' or use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[1, 5, 0, 5, 7, 3, 7, 3]\n",
      "is mapped to\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "# Below N_out and Ntest_out are the lengths of the outputs in both the training\n",
    "# and testing regimes respectively. Since outputs do not include the initial and terminal\n",
    "# symbols, these default to N - 2 and Ntest - 2 respectively.\n",
    "\n",
    "# Default sampling from space of inputs\n",
    "def generate_input_seq_default(max_symbol,input_length):\n",
    "    return [random.randint(0,max_symbol) for k in range(input_length)]\n",
    "\n",
    "generate_input_seq = generate_input_seq_default\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "#\n",
    "# In this task the input is simply copied to the output (although we\n",
    "# require the RNN to output the first output symbol after the last\n",
    "# input symbol has been read, so this effectively requires the system\n",
    "# to store the input and later retrieve it)\n",
    "\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N - 2\n",
    "    Ntest_out = Ntest - 2\n",
    "    seq_length_min = 7\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "#\n",
    "# In this task every digit of the input is repeated.\n",
    "#\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "\n",
    "if( task == 'repeat copy' ):\n",
    "    no_of_copies = 2\n",
    "    pattern = [0]*(no_of_copies - 1) + [1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = no_of_copies * (N - 2)\n",
    "    Ntest_out = no_of_copies * (Ntest - 2)\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 1\n",
    "if( task == 'pattern 1' ):\n",
    "    pattern = [0,1,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,c,c,d,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = (N - 2) + divmod(N - 2, 2)[0] # N - 2 plus the number of times 2 divides N - 2\n",
    "    Ntest_out = (Ntest - 2) + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 2\n",
    "if( task == 'pattern 2' ):\n",
    "    pattern = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = N - 2 + divmod(N - 2, 2)[0]\n",
    "    Ntest_out = Ntest - 2 + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 3\n",
    "if( task == 'pattern 3' ):\n",
    "    pattern = [0,2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,b,b,d,c,c,e,d,d,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 4 + (N - 2 - 2) * 3\n",
    "    Ntest_out = 4 + (Ntest - 2 - 2) * 3\n",
    "    seq_length_min = 7\n",
    "\n",
    "################\n",
    "# PATTERN TASK 4\n",
    "if( task == 'pattern 4' ):\n",
    "    pattern = [0,2,1,2,-2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,d,f,d,c,c,e,f,h,f,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 7\n",
    "\n",
    "################\n",
    "# PATTERN TASK 5\n",
    "if( task == 'pattern 5' ):\n",
    "    pattern = [4,1,1,-4] # so (a,b,c,d,e,f,...) goes to (a,e,f,g,k,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 7\n",
    "\n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 1\n",
    "if( task == 'mult pattern 1' or task == 'mult pattern 2'):\n",
    "    pattern1 = [1] # so (a,b,c,d,e,f,...) goes to (a,b,c,d,e,f,...)\n",
    "    pattern2 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 7\n",
    "    \n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 2\n",
    "if( task == 'mult pattern 2' ):\n",
    "    # Almost everything is the same as mult pattern 1, but in pattern 2 we \n",
    "    # make sure there is a div symbol somewhere in the sequence\n",
    "    def generate_input_seq_forcediv(max_symbol,input_length):\n",
    "        t = [random.randint(0,max_symbol) for k in range(input_length)]\n",
    "        div_pos = random.randint(0,len(t)-1)\n",
    "        t[div_pos] = div_symbol\n",
    "        return t\n",
    "    \n",
    "    generate_input_seq = generate_input_seq_forcediv\n",
    "\n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 3\n",
    "if( task == 'mult pattern 3'):\n",
    "    pattern1 = [1] # so (a,b,c,d,e,f,...) goes to (a,b,c,d,e,f,...)\n",
    "    pattern2 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    pattern3 = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2,pattern3],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 7\n",
    "    \n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 4\n",
    "if( task == 'mult pattern 4'):\n",
    "    pattern1 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    pattern2 = [2,-1] # so (a,b,c,d,e,f,...) goes to (a,c,b,d,c,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 7\n",
    "\n",
    "#########################\n",
    "# VARIABLE PATTERN TASK 1\n",
    "#\n",
    "# The input is a pattern together with a string to which we are supposed to apply the\n",
    "# pattern, separated by an initial symbol. There is no division symbol.\n",
    "\n",
    "varpatterns = [[1],[2],[0,1],[0,2],[1,2]]\n",
    "\n",
    "def generate_input_seq_varpattern(max_symbol,input_length):\n",
    "    vp = varpatterns[random.randint(0,len(varpatterns)-1)]\n",
    "    t = vp + [init_symbol] + [random.randint(0,max_symbol) for k in range(input_length-len(vp)-1)]\n",
    "    return t\n",
    "\n",
    "if( task == 'variable pattern 1'):\n",
    "    generate_input_seq = generate_input_seq_varpattern\n",
    "    func_to_learn = lambda s: learnfuncs.f_varpattern(s,init_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 10\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = [random.randint(0,num_classes-3) for i in range(N - 2)]\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# INITIALISE STATE #\n",
    "####################\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "input_size = num_classes # dimension of the input space I\n",
    "\n",
    "#####\n",
    "# NTM\n",
    "\n",
    "def init_state_ntm(batch_size, css, mas, mcs):\n",
    "    state_size = css + 2*mas + mas * mcs\n",
    "    \n",
    "    ra = [0.0]*mas\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,mas]) + ra\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    init_read_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_write_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_memory = tf.truncated_normal([batch_size, mas*mcs], 0.0, 1e-6, dtype=tf.float32)\n",
    "    \n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "#############\n",
    "# PATTERN NTM\n",
    "\n",
    "def init_state_pattern_ntm(batch_size, css, mas, mcs):\n",
    "    # mas and mcs are arrays of address sizes and content sizes for rings\n",
    "    state_size = css\n",
    "    \n",
    "    init_address = []\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        state_size = state_size + mas[i] * mcs[i] # for memory vector\n",
    "        state_size = state_size + 2 * mas[i] # for addresses (read and write)\n",
    "    \n",
    "        ra = [0.0]*mas[i]\n",
    "        ra[0] = 1.0\n",
    "        init_address.append(np.zeros([batch_size,mas[i]]) + ra)\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    \n",
    "    tensor_list = [init_controller_state]\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        init_read_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        init_write_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        tensor_list = tensor_list + [init_read_address,init_write_address]\n",
    "        \n",
    "    for i in range(len(mas)):\n",
    "        # The first ring is initialised to zero, the rest differently\n",
    "        if( i == 0 ):\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "        else:\n",
    "            # This initialisation has the result of biasing the output of rings 2 and 3 to be\n",
    "            # \"no rotation\" and biasing ring 4 to say \"use ring 2\"\n",
    "            ra = [0.0]*mcs[i] \n",
    "            ra[0] = memory_init_bias\n",
    "            ra = np.zeros([batch_size,mas[i],mcs[i]]) + ra\n",
    "            ra = tf.constant(ra,dtype=tf.float32,shape=[batch_size,mas[i],mcs[i]])\n",
    "            ra = tf.reshape(ra,[batch_size,mas[i]*mcs[i]])\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32) + ra\n",
    "            #init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "            \n",
    "        tensor_list = tensor_list + [init_memory]\n",
    "    \n",
    "    state = tf.concat(tensor_list,1)\n",
    "\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "######################\n",
    "# MULTIPLE PATTERN NTM\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_25/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_24/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_23/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_22/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_21/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_20/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_19/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_18/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_17/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_15/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_13/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_11/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_9/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_7/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_5/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_3/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_1/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "\n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "read_addresses2 = []\n",
    "read_addresses3 = []\n",
    "read_addresses4 = []\n",
    "write_addresses = []\n",
    "write_addresses2 = []\n",
    "write_addresses3 = []\n",
    "write_addresses4 = []\n",
    "interps = []\n",
    "rnn_outputs = []\n",
    "m2 = []\n",
    "m3 = []\n",
    "m4 = []\n",
    "    \n",
    "for i in range(N + N_out):\n",
    "    \n",
    "    old_state = state\n",
    "\n",
    "    #### RUN MODEL ####\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "    ###################\n",
    "\n",
    "    reuse = True\n",
    "    \n",
    "    #### SET UP NODES FOR LOGGING #####\n",
    "    if( use_model == 'ntm' ):\n",
    "        h0, curr_read, curr_write, _ = tf.split(old_state, [controller_state_size,ntm_memory_address_size,\n",
    "                                                        ntm_memory_address_size,-1], 1)\n",
    "\n",
    "    if( use_model == 'pattern_ntm' ):\n",
    "        mas = pattern_ntm_memory_address_sizes\n",
    "        mcs = pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(old_state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],mas[0] * mcs[0],mas[1] * mcs[1]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        m1_state = ret[5]\n",
    "        m2_state = ret[6]\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm' ):\n",
    "        mas = mult_pattern_ntm_memory_address_sizes\n",
    "        mcs = mult_pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(old_state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],                        \n",
    "                            mas[2],mas[2],mas[3],mas[3],mas[0] * mcs[0],mas[1] * mcs[1],\n",
    "                            mas[2] * mcs[2],mas[3] * mcs[3]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        curr_read3 = ret[5]\n",
    "        curr_write3 = ret[6]\n",
    "        curr_read4 = ret[7]\n",
    "        curr_write4 = ret[8]\n",
    "        m1_state = ret[9]\n",
    "        m2_state = ret[10]\n",
    "        m3_state = ret[11]\n",
    "        m4_state = ret[12]\n",
    "        \n",
    "    read_addresses.append(curr_read[0,:])\n",
    "    write_addresses.append(curr_write[0,:])\n",
    "    \n",
    "    if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses2.append(curr_read2[0,:])\n",
    "        write_addresses2.append(curr_write2[0,:])\n",
    "        m2_state = tf.reshape(m2_state, [-1,mas[1],mcs[1]])\n",
    "        m2.append(tf.nn.softmax(m2_state[0,:]))\n",
    "        \n",
    "        with tf.variable_scope(\"NTM\",reuse=True):\n",
    "            W_interp = tf.get_variable(\"W_interp\", [controller_state_size,1])\n",
    "            B_interp = tf.get_variable(\"B_interp\", [1])\n",
    "            interp = tf.sigmoid(tf.matmul(h0,W_interp) + B_interp)\n",
    "            interp_matrix = tf.concat([interp,tf.ones_like(interp,dtype=tf.float32) - interp],axis=1) # shape [-1,2]\n",
    "            interps.append(interp_matrix[0,:])\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses3.append(curr_read3[0,:])\n",
    "        write_addresses3.append(curr_write3[0,:])\n",
    "        read_addresses4.append(curr_read4[0,:])\n",
    "        write_addresses4.append(curr_write4[0,:])\n",
    "        m3_state = tf.reshape(m3_state, [-1,mult_pattern_ntm_memory_address_sizes[2],mult_pattern_ntm_memory_content_sizes[2]])\n",
    "        m3.append(tf.nn.softmax(m3_state[0,:]))\n",
    "        m4_state = tf.reshape(m4_state, [-1,mult_pattern_ntm_memory_address_sizes[3],mult_pattern_ntm_memory_content_sizes[3]])\n",
    "        m4_state = m4_state[0,:]\n",
    "        m4_state = tf.concat([tf.nn.softmax(m4_state),tf.zeros([mult_pattern_ntm_memory_address_sizes[3],1])],1)\n",
    "        m4.append(m4_state)\n",
    "    ### END LOGGING ###\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# Note: prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "# Note: we use log_softmax to avoid precision issues with floats causing log(0) to create NaNs\n",
    "\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.log_softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * prediction[i]) for i in range(N + N_out)] # an array of numbers\n",
    "\n",
    "# Note: We allow the length of input sequences to vary between batches, which means\n",
    "# that the cross entropy needs to be masked to the relevant part of the output. The\n",
    "# relevant part consists of those positions that are not terminal symbols in the output\n",
    "# of _every_ input sequence in the batch. We detect such positions as follows. First,\n",
    "# we create a tensor term_detector which detects all the positions which are terminal symbols.\n",
    "# term_detector[i] is a boolean tensor which has False for those elements of the batch with\n",
    "# a terminal symbol in the output position i, and True otherwise.\n",
    "\n",
    "term_detector = [tf.not_equal(tf.argmax(targets[i],1),term_symbol) for i in range(N + N_out)]\n",
    "\n",
    "# We then convert False to 0.0 and True to 1.0, and compute the reduce_max, with the result\n",
    "# that mask is 1.0 in position i if and only if there was SOME element of the batch which\n",
    "# did NOT have a terminal symbol in position i\n",
    "\n",
    "mask = [tf.reduce_max(tf.cast(m, tf.float32)) for m in term_detector]\n",
    "ce_mask = [ce[i] * mask[i] for i in range(N + N_out)]\n",
    "cross_entropy = -tf.add_n(ce_mask)\n",
    "cross_entropy /= tf.add_n(mask)\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate,decay=0.9,momentum=0.9)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N + N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "errors_mask = [errors[i] * mask[i] for i in range(N + N_out)]\n",
    "mean_error = tf.add_n(errors_mask)\n",
    "mean_error /= tf.add_n(mask)\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1, mean error - 0.778\n",
      "Epoch - 2, mean error - 0.5533\n",
      "\n",
      "It took 32 seconds to train.\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "###################\n",
    "# Note on sequences\n",
    "#\n",
    "# Our sequences are of varying length, in the alphabet {0,...,num_classes - 3}.\n",
    "# Each input sequence begins with an initial symbol and ends with a terminal symbol\n",
    "# (the value of which are num_classes - 2 and num_classes - 1 by default).\n",
    "#\n",
    "# Both input and output sequences are written on a \"tape\" of length N + N_out.\n",
    "# Input sequences are aligned at the BEGINNING of the tape, and all remaining space\n",
    "# is filled with terminal symbols. Output sequences are aligned at the END OF THE \n",
    "# MATCHING INPUT, with all remaining space filled with terminal symbols.\n",
    "#\n",
    "# Example: suppose N = N_out = 10, and num_classes = 10 so that init_symbol = 8\n",
    "# and term_symbol = 9. Then a sequence of length 8 (seq_length = 10 below) is\n",
    "#\n",
    "# a = [4, 4, 5, 6, 3, 3, 6, 7]\n",
    "#\n",
    "# which written on the tape is\n",
    "#\n",
    "# [8, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
    "#\n",
    "# If we are performing the copy task, so that the output sequence is also a, then\n",
    "# the output written on the tape will be (notice the alignment)\n",
    "#\n",
    "# [9, 9, 9, 9, 9, 9, 9, 9, 9, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9]\n",
    "#\n",
    "\n",
    "def io_generator(max_symbol, input_length, total_length):\n",
    "    \"\"\"\n",
    "    Returns a one-hot encoded pair of input and output sequence, with terminal and initial symbols.\n",
    "    \n",
    "    max_symbol - generate sequences in 0,...,max_symbol\n",
    "    input_length - length of input sequences, without initial and terminal symbols\n",
    "    total_length - length of the buffer, so that the sequences are padded to this length\n",
    "    \"\"\"\n",
    "    a = generate_input_seq(max_symbol,input_length)\n",
    "    fa = func_to_learn(a)\n",
    "    a = [init_symbol] + a + [term_symbol]\n",
    "    a = a + [term_symbol for k in range(total_length-len(a))]\n",
    "    a_onehot = [one_hots[e] for e in a]\n",
    "    \n",
    "    # If the output is too long to fit in the buffer, truncate it\n",
    "    if( len(fa) + input_length + 1 > total_length ):\n",
    "        fa = fa[:total_length-input_length-1]\n",
    "        \n",
    "    fa = [term_symbol for k in range(input_length+1)] + fa + \\\n",
    "                [term_symbol for k in range(total_length-(input_length+1)-len(fa))]\n",
    "    fa_onehot = [one_hots[e] for e in fa]\n",
    "    \n",
    "    return a, fa, np.array(a_onehot), np.array(fa_onehot)\n",
    "\n",
    "error_means = []\n",
    "epoch_error_means = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences. Each\n",
    "        # batch has a fixed length of the sequences. Recall that all input seqs\n",
    "        # have an initial and terminal symbol, so if seq_length = 10 then there\n",
    "        # are eight positions for the \"content\" symbols\n",
    "        \n",
    "        # Our version of curriculum training says: spend the first half\n",
    "        # of the epochs ramping up to the full training set. Assuming that\n",
    "        # epoch > N we divide allocate each integer in [seq_length_min,N]\n",
    "        # an equal portion of the first half of the epochs.\n",
    "        if( use_curriculum == True ):\n",
    "            if( 2 * i > epoch ):\n",
    "                seq_length_max = N\n",
    "            else:\n",
    "                curriculum_band = max(1,int(epoch/(2*(N - seq_length_min))))\n",
    "                seq_length_max = min(seq_length_min + int(i/curriculum_band),N)\n",
    "        else:\n",
    "            seq_length_max = N\n",
    "            \n",
    "        seq_length = random.randint(seq_length_min,seq_length_max)\n",
    "        \n",
    "        # Hack: if we are on the final batch of the final epoch, force\n",
    "        # it to use the full sequence length, so we get a good visualisation\n",
    "        if( i + 1 == epoch and j + 1 == no_of_batches ):\n",
    "            seq_length = N\n",
    "        \n",
    "        for z in range(batch_size):\n",
    "            a, fa, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=N+N_out)\n",
    "            \n",
    "            inp.append(a_onehot)\n",
    "            out.append(fa_onehot)\n",
    "            \n",
    "            # Record the first sequence in the last batch of the last epoch\n",
    "            if( i == epoch - 1 and j == no_of_batches - 1 and z == 0):\n",
    "                final_seq = a\n",
    "                final_seq_output = fa\n",
    "        \n",
    "        # An annoying thing here is that we cannot use a list as a key in a \n",
    "        # dictionary. The workaround we found on StackOverflow here:\n",
    "        # http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "        feed_dict = {}\n",
    "        \n",
    "        for d in range(N + N_out):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N + N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "\n",
    "        ##### Do gradient descent #####\n",
    "        mean_error_val,_ = sess.run([mean_error,minimize], feed_dict)\n",
    "        ###############################\n",
    "        \n",
    "        error_means.append(mean_error_val)\n",
    "    \n",
    "    epoch_error = np.mean(error_means[-no_of_batches:])\n",
    "    epoch_error_means.append(epoch_error)\n",
    "    \n",
    "    # Print the mean error of the final batch in the epoch\n",
    "    print_str = \"Epoch - \" + str(i+1) + \", mean error - \" + str(epoch_error)\n",
    "    \n",
    "    if( use_curriculum == True ):\n",
    "        print_str = print_str + \", training at max length - \" + str(seq_length_max)\n",
    "        \n",
    "    print(print_str)\n",
    "\n",
    "# For the final batch of the final epoch, we record the memory states as well\n",
    "seq_length_for_vis = seq_length - 2\n",
    "interps_val = sess.run(interps,feed_dict)\n",
    "m2_val, m3_val, m4_val = sess.run([m2,m3,m4],feed_dict)            \n",
    "r1_val, w1_val = sess.run([read_addresses,write_addresses],feed_dict)\n",
    "r2_val, w2_val = sess.run([read_addresses2,write_addresses2],feed_dict)\n",
    "r3_val, w3_val = sess.run([read_addresses3,write_addresses3],feed_dict)\n",
    "r4_val, w4_val = sess.run([read_addresses4,write_addresses4],feed_dict)\n",
    "errors_mask_val = sess.run(errors_mask,feed_dict)\n",
    "\n",
    "mask_val = sess.run(tf.cast(mask,tf.int64),feed_dict)\n",
    "predicted_seq = [tf.argmax(prediction[i], 1) for i in range(N + N_out)]\n",
    "predicted_seq_val = sess.run(predicted_seq,feed_dict)\n",
    "final_seq_pred_0 = [a[0] for a in predicted_seq_val]\n",
    "final_seq_pred = []\n",
    "\n",
    "for i in range(len(mask_val)):\n",
    "    if( mask_val[i] == 1.0 ):\n",
    "        final_seq_pred.append(final_seq_pred_0[i])\n",
    "    else:\n",
    "        final_seq_pred.append(9)\n",
    "\n",
    "# Write out variables to disk\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took \" + str(int(time.time() - pre_train_time)) + \" seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length used for visualisations - 8\n",
      "\n",
      "Sequence used for visualisations is (Note: initial symbol is 8, terminal symbol is 9)\n",
      "[8, 0, 1, 8, 2, 7, 0, 2, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "\n",
      "Correct output for this sequence:\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 2, 2, 7, 7, 0, 0, 2, 2, 6, 6, 9, 9, 9, 9, 9, 9, 9]\n",
      "\n",
      "Predicted output for this sequence\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 5, 2, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "\n",
      "Mask for output\n",
      "[0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0]\n",
      "\n",
      "Error probabilities for final batch\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.82800001, 0.91600001, 0.96399999, 0.80400002, 0.62400001, 0.62400001, 0.18799999, 0.18799999, 0.18799999, 0.18799999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAAL8CAYAAAC2zccJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neyWxvdK+9QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a698cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 1 #\n",
    "###########################\n",
    "\n",
    "print(\"Sequence length used for visualisations - \" + str(seq_length_for_vis))\n",
    "print(\"\")\n",
    "print(\"Sequence used for visualisations is (Note: initial symbol is \" + str(init_symbol) + \", terminal symbol is \" + str(term_symbol) + \")\")\n",
    "print(final_seq)\n",
    "print(\"\")\n",
    "print(\"Correct output for this sequence:\")\n",
    "print(final_seq_output)\n",
    "print(\"\")\n",
    "print(\"Predicted output for this sequence\")\n",
    "print(final_seq_pred)\n",
    "print(\"\")\n",
    "print(\"Mask for output\")\n",
    "print(mask_val)\n",
    "print(\"\")\n",
    "print(\"Error probabilities for final batch\")\n",
    "print(errors_mask_val)\n",
    "print(\"\")\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 9, 13\n",
    "fig_num = 0\n",
    "\n",
    "# RING 1\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "plt.figure(fig_num)\n",
    "ax1 = plt.subplot(2,1,1)\n",
    "ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "ax1.imshow(np.stack(w1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax1.set_title('Write address (ring 1)')\n",
    "ax1.set_xlabel('position')\n",
    "ax1.set_ylabel('time')\n",
    "\n",
    "ax2.imshow(np.stack(r1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax2.set_title('Read address (ring 1)')\n",
    "ax2.set_xlabel('position')\n",
    "ax2.set_ylabel('time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 2 #\n",
    "###########################\n",
    "\n",
    "if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 2)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 2)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Assume that powers2_on_1 has three entries we can use as colour channels\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 2)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    max_xticks = 2\n",
    "    xloc = plt.MaxNLocator(max_xticks)\n",
    "\n",
    "    ax.imshow(np.stack(interps_val), cmap='bone', interpolation='nearest', aspect='auto')\n",
    "    ax.set_title('Interpolation')\n",
    "    ax.set_xlabel('direct vs indirect')\n",
    "    ax.set_ylabel('time')\n",
    "    ax.xaxis.set_major_locator(xloc)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# VISUALISATIONS - OTHER RINGS #\n",
    "################################\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 3)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 3)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 3)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 4)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 4)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax6 = plt.subplot(1,1,1)    \n",
    "    ax6.imshow(np.stack(m4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax6.set_title('Memory contents (ring 4)')\n",
    "    ax6.set_xlabel('position')\n",
    "    ax6.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3kAAAUKCAYAAABblriAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKzkE92sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1521fd650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########################\n",
    "# VISUALISATIONS - ERROR #\n",
    "##########################\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "\n",
    "plt.figure(fig_num)\n",
    "ax = plt.subplot(1,1,1)\n",
    "sc = pandas.Series(error_means)\n",
    "ma = sc.rolling(window=500).mean()\n",
    "ax.plot(sc.index, sc, color='lightgray')\n",
    "ax.plot(ma.index, ma, color='red')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(sc.index.min(), sc.index.max())\n",
    "ax.set_title('Error')\n",
    "ax.set_xlabel('batch')\n",
    "ax.set_ylabel('error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on sequences of length 13\n",
      "\n",
      "Batch - 1, Mean error - 0.5234\n",
      "Batch - 2, Mean error - 0.535\n",
      "Batch - 3, Mean error - 0.549\n",
      "Batch - 4, Mean error - 0.5348\n",
      "\n",
      "###########\n",
      "# Summary #\n",
      "###########\n",
      "\n",
      "model         - ntm\n",
      "task name     - variable pattern 1\n",
      "epochs        - 2\n",
      "num_classes   - 10\n",
      "N             - 10\n",
      "Ntest         - 15\n",
      "# weights     - 18958\n",
      "\n",
      "\n",
      "error train(test) - 0.5533 (0.53555)\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "# Restore the weights from training\n",
    "sess = tf.Session()\n",
    "saver.restore(sess,save_path)\n",
    "\n",
    "inputs_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "targets_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "# Set up test graph\n",
    "rnn_outputs_test = []\n",
    "reuse = True\n",
    "for i in range(Ntest + Ntest_out):\n",
    "    output, state = cell(inputs_test[i],state,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "\n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size])\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.log_softmax(logit) for logit in logits_test] \n",
    "term_detector = [tf.not_equal(tf.argmax(targets_test[i],1),term_symbol) for i in range(Ntest + Ntest_out)]\n",
    "mask = [tf.reduce_max(tf.cast(m, tf.float32)) for m in term_detector]\n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets_test[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest + Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "errors_test_mask = [errors_test[i] * mask[i] for i in range(Ntest + Ntest_out)]\n",
    "mean_error_test = tf.add_n(errors_test_mask)\n",
    "mean_error_test /= tf.add_n(mask)\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "\n",
    "seq_length = Ntest\n",
    "print(\"Testing on sequences of length \" + str(seq_length-2))\n",
    "print(\"\")\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    for z in range(batch_size):\n",
    "        a, fa, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=Ntest+Ntest_out)\n",
    "            \n",
    "        inp.append(a_onehot)\n",
    "        out.append(fa_onehot)        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        in_node = inputs_test[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        out_node = targets_test[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = sess.run(mean_error_test, feed_dict)\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"epochs        - \" + str(epoch))\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error train(test) - \" + str(epoch_error_means[-1]) + \" (\" + str(final_error) + \")\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
