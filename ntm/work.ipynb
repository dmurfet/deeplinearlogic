{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of linear logic recurrent neural network\n",
    "#\n",
    "# The architecture is a modified RNN, see the paper \"Linear logic and recurrent neural networks\".\n",
    "# Our inputs are sequences of symbols taken from an alphabet of size num_classes. The length\n",
    "# of the sequences is N. Our outputs are also sequences of length N from the same alphabet.\n",
    "#\n",
    "# Here \"symbol\" means a one hot vector.\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# GLOBAL FLAGS\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, pattern_ntm_alt\n",
    "task                  = 'copy' # copy, repeat copy, pattern\n",
    "epoch                 = 200 # number of training epochs, default to 200\n",
    "num_classes           = 4 # number of symbols, INCLUDING initial and terminal symbols\n",
    "N                     = 22 # length of input sequences for training, default to 20, INCLUDING initial and terminal symbols\n",
    "Ntest                 = 22 # length of sequences for testing, default to N, INCLUDING initial and terminal symbols\n",
    "batch_size            = 500 # default to 500 (too large does not fit on GPUs)\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "memory_address_size   = 20 # number of memory locations, default 20\n",
    "memory_content_size   = 5 # size of vector stored at a memory location, default 5\n",
    "powers_ring1          = [0,-1,1] # powers of R used on ring 1, default [0,-1,1]\n",
    "powers_ring2          = [0,-1,1] # powers of R used on ring 2, default [0,-1,1]\n",
    "model_optimizer       = 'rmsprop' # adam, rmsprop, default to rmsprop\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "\n",
    "training_percent      = 0.01 # percentage used for training, default 0.01\n",
    "num_training          = int(training_percent * (num_classes-2)**N)\n",
    "num_test              = num_training\n",
    "\n",
    "init_symbol           = num_classes - 2\n",
    "term_symbol           = num_classes - 1\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there\n",
    "# 2. The initial and terminal symbols are always from the end of the list of symbols, so they\n",
    "# are respectively num_classes - 2 and num_classes - 1. So the number of symbols which are\n",
    "# not initial or terminal is num_classes - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "is mapped to\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "# Below N_out and Ntest_out are the lengths of the outputs in both the training\n",
    "# and testing regimes respectively. Since outputs do not include the initial and terminal\n",
    "# symbols, these default to N - 2 and Ntest - 2 respectively.\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N - 2\n",
    "    Ntest_out = Ntest - 2\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "if( task == 'repeat copy' ):\n",
    "    pattern = [0,1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 2 * (N - 2)\n",
    "    Ntest_out = 2 * (Ntest - 2)\n",
    "\n",
    "##############\n",
    "# PATTERN TASK\n",
    "if( task == 'pattern' ):\n",
    "    pattern = [1,0,0,2,0]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 2 * (N - 2)\n",
    "    Ntest_out = 2 * (Ntest - 2)\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = [random.randint(0,num_classes-3) for i in range(N)]\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# INITIALISE STATE #\n",
    "####################\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "input_size = num_classes # dimension of the input space I\n",
    "state_size = 0\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size = controller_state_size + 2*memory_address_size + memory_address_size * memory_content_size\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,memory_address_size,memory_content_size, powers_ring1)\n",
    "elif( use_model == 'pattern_ntm' ):\n",
    "    state_size = controller_state_size + 4*memory_address_size + \\\n",
    "                memory_address_size * memory_content_size + \\\n",
    "                memory_address_size * len(powers_ring1)\n",
    "\n",
    "    cell = ntm.PatternNTM(state_size,input_size,controller_state_size,\n",
    "                          memory_address_size,memory_content_size, powers_ring1, powers_ring2)\n",
    "elif( use_model == 'pattern_ntm_alt' ):\n",
    "    state_size = controller_state_size + 4*memory_address_size + \\\n",
    "                memory_address_size * memory_content_size + \\\n",
    "                memory_address_size * len(powers_ring1)\n",
    "\n",
    "    cell = ntm.PatternNTM_alt(state_size,input_size,controller_state_size,\n",
    "                          memory_address_size,memory_content_size, powers_ring1, powers_ring2)\n",
    "\n",
    "# Initialise the state\n",
    "if( use_model == 'ntm' ):\n",
    "    # DEBUG we have switched back to the old initialisation\n",
    "    ra = [0.0]*memory_address_size\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,memory_address_size]) + ra\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, controller_state_size], 0.0, 0.001, dtype=tf.float32)\n",
    "    # was tf.truncated_normal([batch_size, controller_state_size], 0.0, 1e-6, dtype=tf.float32)\n",
    "    init_read_address = tf.random_uniform([batch_size, memory_address_size], 0.0, 0.001)\n",
    "    # was was tf.truncated_normal([batch_size, memory_address_size], 0.0, 0.01, dtype=tf.float32)\n",
    "    # was tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,memory_address_size])\n",
    "    init_write_address = tf.random_uniform([batch_size, memory_address_size], 0.0, 0.001)\n",
    "    # was tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,memory_address_size])\n",
    "    init_memory = tf.truncated_normal([batch_size, memory_address_size*memory_content_size], 0.0, 0.001, dtype=tf.float32)\n",
    "    # was tf.constant(1e-6,dtype=tf.float32,shape=[batch_size,memory_address_size*memory_content_size])\n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "    # DEBUG we got convergence with 0.01 and e,a both on softmax\n",
    "else:\n",
    "    state = tf.truncated_normal([batch_size, state_size], 0.0, 0.01, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_63/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_62/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_61/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_60/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_59/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_58/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_57/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_56/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_55/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_54/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_53/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_52/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_51/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_50/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_49/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_48/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_47/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_46/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_45/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_44/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_42/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_40/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_38/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_36/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_34/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_32/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_30/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_28/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_26/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_24/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_22/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_20/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_18/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "# inputs, we create N of them, each of shape [None,input_size], one for each position in the sequence\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N_out)]\n",
    "\n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "write_addresses = []\n",
    "gamma_writes = []\n",
    "gamma_reads = []\n",
    "\n",
    "for i in range(N):\n",
    "    # Store read and write addresses for later logging\n",
    "    h0, curr_read, curr_write, _ = tf.split(state, [controller_state_size,memory_address_size,memory_address_size,-1], 1)\n",
    "    read_addresses.append(curr_read[0,:])\n",
    "    write_addresses.append(curr_write[0,:])\n",
    "    \n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    \n",
    "    # DEBUG, getting gammas\n",
    "    with tf.variable_scope(\"NTM\",reuse=True):\n",
    "        W_gamma_write = tf.get_variable(\"W_gamma_write\", [controller_state_size,1])\n",
    "        B_gamma_write = tf.get_variable(\"B_gamma_write\", [])\n",
    "        gamma_write = 1.0 + tf.nn.relu(tf.matmul(h0,W_gamma_write) + B_gamma_write) # shape [batch_size,1]\n",
    "        \n",
    "        W_gamma_read = tf.get_variable(\"W_gamma_read\", [controller_state_size,1])\n",
    "        B_gamma_read = tf.get_variable(\"B_gamma_read\", [])\n",
    "        gamma_read = 1.0 + tf.nn.relu(tf.matmul(h0,W_gamma_read) + B_gamma_read) # shape [batch_size,1]\n",
    "\n",
    "    gamma_writes.append(gamma_write[0,:])\n",
    "    gamma_reads.append(gamma_read[0,:])\n",
    "    reuse = True\n",
    "\n",
    "# We only start recording the outputs of the controller once we have\n",
    "# finished feeding in the input. We feed terminal symbols as input in the second phase.\n",
    "\n",
    "term_symbol_tensor = tf.constant(np.zeros([batch_size,input_size]) + one_hots[term_symbol],\n",
    "                                 dtype=tf.float32,\n",
    "                                 shape=[batch_size,input_size])\n",
    "\n",
    "rnn_outputs = []\n",
    "for i in range(N_out):\n",
    "    output, state = cell(term_symbol_tensor,state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * tf.log(prediction[i])) for i in range(N_out)]\n",
    "\n",
    "if( model_optimizer == 'adam' ):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "elif( model_optimizer == 'rmsprop' ):\n",
    "    optimizer = tf.train.RMSPropOptimizer(1e-4,decay=0.9,momentum=0.9)\n",
    "\n",
    "cross_entropy = -tf.add_n(ce)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "mean_error = tf.scalar_mul(np.true_divide(1,N_out), tf.add_n(errors))\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.00110817]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[  5.68949385e-04   4.79092269e-04   3.06021597e-04   9.28897643e-04\n",
      "   3.08985727e-05   1.02051978e-04   3.08915507e-04   1.64057739e-04\n",
      "   9.53398994e-05   2.40324152e-04   5.13800420e-04   5.35509025e-04\n",
      "   4.90074512e-04   5.04704134e-04   5.71017299e-05   9.52802133e-04\n",
      "   9.74891227e-05   8.64363101e-04   1.09583976e-04   5.42593130e-04]\n",
      "\n",
      "Step 1 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.06717617  0.05718842  0.07240545  0.0534216   0.04488385  0.01866178\n",
      "  0.02427489  0.02400698  0.0211101   0.03587415  0.05445522  0.0650125\n",
      "  0.06463196  0.04441778  0.06399915  0.04672013  0.0809095   0.0452083\n",
      "  0.06408662  0.05155541]\n",
      "\n",
      "Step 2 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.05894428  0.06436623  0.06238014  0.05894359  0.04251515  0.0300626\n",
      "  0.02203682  0.02348303  0.02543579  0.03416826  0.04952747  0.06082983\n",
      "  0.06039763  0.0565134   0.05264843  0.06084259  0.05989176  0.06316626\n",
      "  0.05403819  0.05980856]\n",
      "\n",
      "Step 3 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.18951309]\n",
      "Write address -\n",
      "[ 0.06115005  0.0619643   0.06177318  0.05415067  0.04319805  0.03108478\n",
      "  0.02505893  0.02372875  0.02794409  0.03692598  0.04876773  0.0571468\n",
      "  0.05914568  0.05634609  0.05678087  0.05794478  0.0613578   0.0588847\n",
      "  0.05894841  0.0576984 ]\n",
      "\n",
      "Step 4 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.2393173]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.06178756  0.06358144  0.06009656  0.05286534  0.04106056  0.03081762\n",
      "  0.02379294  0.02281103  0.02698338  0.03602851  0.04664722  0.05511832\n",
      "  0.05824723  0.05881555  0.05812579  0.06030874  0.06040583  0.06156072\n",
      "  0.05973062  0.06121502]\n",
      "\n",
      "Step 5 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.0622264   0.0619244   0.05865337  0.05093632  0.04091773  0.03131407\n",
      "  0.02541065  0.02447221  0.02884027  0.03706044  0.04652839  0.05381512\n",
      "  0.05757029  0.05842853  0.05904364  0.05973676  0.06076352  0.0606316\n",
      "  0.06073155  0.06099474]\n",
      "\n",
      "Step 6 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.10888135]\n",
      "Write address -\n",
      "[ 0.06165557  0.0610351   0.0575481   0.05084803  0.04185016  0.03322405\n",
      "  0.02741255  0.02618038  0.02967467  0.03678408  0.04509671  0.05214316\n",
      "  0.05637728  0.05828478  0.05901824  0.05978374  0.06032453  0.06071354\n",
      "  0.06077375  0.06127159]\n",
      "\n",
      "Step 7 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.3765204]\n",
      "Write address -\n",
      "[ 0.06244285  0.06076909  0.05633251  0.04889369  0.03995263  0.03182226\n",
      "  0.02680794  0.0261308   0.02987349  0.03682996  0.04490811  0.05182655\n",
      "  0.05640399  0.05876464  0.05998426  0.06073396  0.06134775  0.06168677\n",
      "  0.06206042  0.06242835]\n",
      "\n",
      "Step 8 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.21020854]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.06595716  0.06273007  0.05585211  0.0458941   0.03517143  0.02649769\n",
      "  0.02175114  0.02158116  0.02585174  0.03368083  0.04311504  0.05169114\n",
      "  0.05775545  0.06126121  0.06314943  0.06431901  0.06512627  0.06572396\n",
      "  0.06625866  0.06663234]\n",
      "\n",
      "Step 9 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.45636153]\n",
      "Write address -\n",
      "[ 0.06529635  0.06196076  0.05552665  0.04645574  0.03657916  0.02827094\n",
      "  0.02340012  0.02283575  0.02651398  0.03351991  0.04214054  0.05032118\n",
      "  0.056568    0.06053605  0.06280151  0.06412666  0.06500483  0.06565982\n",
      "  0.06617202  0.06631004]\n",
      "\n",
      "Step 10 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.22153628]\n",
      "Write gamma - [ 1.25464737]\n",
      "Write address -\n",
      "[ 0.0701765   0.06452876  0.05511717  0.04324918  0.03156219  0.02260489\n",
      "  0.01770492  0.01707205  0.02054418  0.02773865  0.03751322  0.04785721\n",
      "  0.05672174  0.06305514  0.06701943  0.06940033  0.07092958  0.07202441\n",
      "  0.0727087   0.07247169]\n",
      "\n",
      "Step 11 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.21547031]\n",
      "Write address -\n",
      "[ 0.07323238  0.06581331  0.05454108  0.0412543   0.02884232  0.01968904\n",
      "  0.01474773  0.01392979  0.01701144  0.02384375  0.03374154  0.04503993\n",
      "  0.05559837  0.06386451  0.06948655  0.07302901  0.0752912   0.07679023\n",
      "  0.07750846  0.07674506]\n",
      "\n",
      "Step 12 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.13221741]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.0762157   0.06726136  0.05446957  0.04005724  0.02703378  0.01762925\n",
      "  0.0125218   0.01138569  0.01390526  0.02009103  0.02968007  0.04146532\n",
      "  0.05341921  0.06362268  0.07115031  0.07617988  0.07942039  0.0814341\n",
      "  0.08218338  0.08087398]\n",
      "\n",
      "Step 13 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.07285642  0.06309307  0.05051446  0.0372657   0.02573531  0.01756348\n",
      "  0.01333409  0.01300191  0.01641741  0.02336009  0.0331531   0.04442343\n",
      "  0.05542468  0.06472256  0.07169044  0.07647864  0.07957449  0.08126988\n",
      "  0.08129988  0.07882092]\n",
      "\n",
      "Step 14 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.26872897]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.06969007  0.05948369  0.04723163  0.03492897  0.02456317  0.01745513\n",
      "  0.01413661  0.01465639  0.01885569  0.02631903  0.03615081  0.04696274\n",
      "  0.05723716  0.06584269  0.07232938  0.07682776  0.0796586   0.08090049\n",
      "  0.08014679  0.07662325]\n",
      "\n",
      "Step 15 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.27683592]\n",
      "Write address -\n",
      "[ 0.0676406   0.05756306  0.04587706  0.03435476  0.02472224  0.01817932\n",
      "  0.01529286  0.01616795  0.02060375  0.02806849  0.0376093   0.04792788\n",
      "  0.05769639  0.06594222  0.07224973  0.07665837  0.07933865  0.08024666\n",
      "  0.07896945  0.07489122]\n",
      "\n",
      "Step 16 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.40483308]\n",
      "Write gamma - [ 1.01010311]\n",
      "Write address -\n",
      "[ 0.06945049  0.05685799  0.04315503  0.03051985  0.02067924  0.01444058\n",
      "  0.01181847  0.01257263  0.0166206   0.02389016  0.03388181  0.04548452\n",
      "  0.05722263  0.06774626  0.07621495  0.08232593  0.08601111  0.08705629\n",
      "  0.08494575  0.07910572]\n",
      "\n",
      "Step 17 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.17793238]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.06879539  0.0567746   0.04373689  0.03160038  0.02194651  0.0156318\n",
      "  0.01284871  0.01348776  0.01741655  0.02443299  0.03400503  0.04512885\n",
      "  0.05649777  0.06687223  0.07538987  0.0816166   0.08533878  0.08628518\n",
      "  0.08402446  0.0781696 ]\n",
      "\n",
      "Step 18 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.32215667]\n",
      "Write address -\n",
      "[ 0.06914149  0.0578673   0.04546916  0.03366165  0.0239586   0.01731579\n",
      "  0.01410022  0.01431302  0.01781162  0.02433083  0.0333375   0.0439277\n",
      "  0.05493028  0.06518269  0.07379349  0.08022304  0.08415668  0.08530175\n",
      "  0.08330096  0.07787624]\n",
      "\n",
      "Step 19 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.12581897]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.06820299  0.05342925  0.03882858  0.02641624  0.01738495  0.0119757\n",
      "  0.00984266  0.01063542  0.01435277  0.02117575  0.03101481  0.0431954\n",
      "  0.05648014  0.06937908  0.08052134  0.08884751  0.09356568  0.09403449\n",
      "  0.08980614  0.08091108]\n",
      "\n",
      "Step 20 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.0025804]\n",
      "Write gamma - [ 1.34716225]\n",
      "Write address -\n",
      "[ 0.06489491  0.05052575  0.03669745  0.02516112  0.01689702  0.01208857\n",
      "  0.0104852   0.01186507  0.016236    0.02364987  0.03384772  0.04604563\n",
      "  0.05900676  0.07132646  0.08172819  0.08919898  0.09295785  0.09240939\n",
      "  0.08724891  0.07772915]\n",
      "\n",
      "Step 21 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.08339214]\n",
      "Write gamma - [ 1.10990739]\n",
      "Write address -\n",
      "[ 0.06555396  0.04779534  0.03201033  0.01997133  0.01212235  0.00791956\n",
      "  0.00654111  0.0075367   0.01108374  0.01774147  0.02792453  0.04140823\n",
      "  0.05710986  0.07324518  0.08772914  0.09857041  0.1041254   0.10329371\n",
      "  0.09579967  0.082518  ]\n",
      "Epoch - 1, Mean error of final batch in epoch - 0.0397\n",
      "Epoch - 2, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 3, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 4, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 5, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 6, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 7, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 8, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 9, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 10, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 11, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 12, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 13, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 14, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 15, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 16, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 17, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 18, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 19, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 20, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 21, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 22, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 23, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 24, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 25, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 26, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 27, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 28, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 29, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 30, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 31, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 32, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 33, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 34, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 35, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 36, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 37, Mean error of final batch in epoch - 0.0001\n",
      "Epoch - 38, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 39, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 40, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 41, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 42, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 43, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 44, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 45, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 46, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 47, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 48, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 49, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 50, Mean error of final batch in epoch - 0.0\n",
      "\n",
      "Step 0 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[  4.49216022e-04   8.87263450e-04   6.63795625e-04   2.37366810e-04\n",
      "   3.25754663e-04   2.85229209e-04   8.87189643e-04   4.18756506e-04\n",
      "   5.63035253e-04   1.77367576e-04   1.84552555e-04   3.01220076e-04\n",
      "   8.13310180e-05   9.87967011e-04   9.30858878e-05   4.56701178e-04\n",
      "   9.09807102e-04   4.46189777e-04   1.49257306e-04   8.54085723e-04]\n",
      "\n",
      "Step 1 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.07989167  0.06988145  0.06295061  0.04467724  0.02995239  0.05503212\n",
      "  0.05437266  0.06776696  0.04000545  0.03363529  0.02391632  0.01940861\n",
      "  0.05160997  0.03743356  0.05729854  0.05255624  0.06253502  0.0536293\n",
      "  0.05402125  0.04942543]\n",
      "\n",
      "Step 2 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.06183095  0.07349881  0.06240052  0.05081072  0.04327015  0.04188315\n",
      "  0.05775091  0.0545309   0.05295086  0.03473384  0.02793203  0.02904501\n",
      "  0.03170543  0.04928488  0.04592694  0.05727645  0.05534386  0.05833421\n",
      "  0.05277623  0.05871416]\n",
      "\n",
      "Step 3 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.06444822  0.06333277  0.06399898  0.0541428   0.04672061  0.04839388\n",
      "  0.04825277  0.05564434  0.04714051  0.04181196  0.03190605  0.02943072\n",
      "  0.03671662  0.03883848  0.05182497  0.0506179   0.05744759  0.05473978\n",
      "  0.05785618  0.05673492]\n",
      "\n",
      "Step 4 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.05953427  0.06422403  0.06008295  0.0572567   0.05166373  0.04736397\n",
      "  0.05097972  0.0482755   0.05021183  0.0413871   0.03681881  0.03348618\n",
      "  0.033211    0.04224242  0.04379164  0.05376779  0.05248118  0.05743967\n",
      "  0.0556309   0.06015069]\n",
      "\n",
      "Step 5 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.06214903  0.05997745  0.0606165   0.05580307  0.05214493  0.05116762\n",
      "  0.04794801  0.05050017  0.04492871  0.04324572  0.03730084  0.03490745\n",
      "  0.03782018  0.0387894   0.04801672  0.0484663   0.05554252  0.05422455\n",
      "  0.05871868  0.05773216]\n",
      "\n",
      "Step 6 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.0591772   0.06120051  0.05768762  0.05569557  0.05306453  0.04977013\n",
      "  0.05065259  0.04638548  0.04622126  0.04061671  0.0383481   0.037482\n",
      "  0.03719407  0.04352684  0.04457763  0.05221675  0.05198167  0.0572475\n",
      "  0.05637416  0.06057969]\n",
      "\n",
      "Step 7 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.06096466  0.05823207  0.05753178  0.05462193  0.051765    0.05142903\n",
      "  0.04756314  0.04767632  0.0425972   0.04096495  0.03852331  0.03757693\n",
      "  0.04144422  0.04214074  0.04924626  0.04955864  0.05551433  0.05494843\n",
      "  0.05942022  0.0582809 ]\n",
      "\n",
      "Step 8 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.05830619  0.05897808  0.05618564  0.05422763  0.05276801  0.04939257\n",
      "  0.04924008  0.04476934  0.04379767  0.04027134  0.03900935  0.04014929\n",
      "  0.04022259  0.04585078  0.04645704  0.05278158  0.05271043  0.05770238\n",
      "  0.05691268  0.0602674 ]\n",
      "\n",
      "Step 9 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.05940292  0.05697013  0.05589047  0.05396544  0.05111545  0.05044635\n",
      "  0.04644409  0.04567474  0.04188261  0.04066979  0.04016582  0.0398073\n",
      "  0.0437829   0.04431795  0.05027923  0.05057979  0.0559147   0.05549722\n",
      "  0.05931899  0.0578742 ]\n",
      "\n",
      "Step 10 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.05726655  0.05691808  0.05485933  0.05253292  0.05146979  0.04784878\n",
      "  0.0470604   0.04325254  0.04212923  0.04066774  0.04006101  0.04268225\n",
      "  0.04301092  0.04831991  0.04877187  0.05421259  0.05408714  0.0582819\n",
      "  0.05721101  0.05935609]\n",
      "\n",
      "Step 11 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.057671    0.05563027  0.05391447  0.05252586  0.04934276  0.04842468\n",
      "  0.04472258  0.04365883  0.04148353  0.04070473  0.04202264  0.04210139\n",
      "  0.04650654  0.04699869  0.05231893  0.05245985  0.05696717  0.05627063\n",
      "  0.05899237  0.05728307]\n",
      "\n",
      "Step 12 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.05614312  0.05504     0.05345561  0.05072755  0.04964037  0.04612871\n",
      "  0.04507151  0.04246356  0.04158201  0.04184641  0.0416906   0.04513073\n",
      "  0.04555529  0.05054086  0.05085586  0.05554176  0.05516228  0.05836093\n",
      "  0.05700952  0.05805331]\n",
      "\n",
      "Step 13 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.05538917  0.05378214  0.05125619  0.05007467  0.04670762  0.04558543\n",
      "  0.04292013  0.04197639  0.04190558  0.04168294  0.04469383  0.04513241\n",
      "  0.04983832  0.05028258  0.05489041  0.05471252  0.05797973  0.05684652\n",
      "  0.05805985  0.05628362]\n",
      "\n",
      "Step 14 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.05423076  0.05199571  0.05072382  0.04752921  0.04636741  0.04360244\n",
      "  0.0426056   0.04207991  0.04173601  0.04417776  0.0445371   0.04889729\n",
      "  0.04939653  0.05396646  0.05395773  0.05740884  0.05649667  0.05803022\n",
      "  0.05640314  0.05585735]\n",
      "\n",
      "Step 15 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.05355952  0.05209814  0.04937483  0.04804933  0.04522239  0.04405757\n",
      "  0.04267496  0.04207851  0.04325396  0.04347912  0.04688757  0.04756861\n",
      "  0.0518147   0.05227087  0.05592658  0.05561259  0.05770757  0.05653617\n",
      "  0.05669421  0.05513282]\n",
      "\n",
      "Step 16 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.05258023  0.05005304  0.04868069  0.04589774  0.04467735  0.04308261\n",
      "  0.04238615  0.04314498  0.04326513  0.04627889  0.04694336  0.05099645\n",
      "  0.05155813  0.05524159  0.05511816  0.05740118  0.05642007  0.05684295\n",
      "  0.05537254  0.05405871]\n",
      "\n",
      "Step 17 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.05078762  0.04934471  0.04666733  0.04535019  0.04359034  0.04277113\n",
      "  0.04309975  0.04312435  0.04565549  0.04634663  0.05008075  0.05082224\n",
      "  0.05441589  0.05457661  0.05696191  0.05626599  0.05690891  0.05560267\n",
      "  0.05454849  0.05307899]\n",
      "\n",
      "Step 18 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.05067145  0.04817851  0.04675952  0.04470843  0.04367971  0.04325147\n",
      "  0.04300373  0.0446802   0.04522594  0.0484203   0.04927517  0.05279206\n",
      "  0.05329896  0.05599627  0.05572456  0.05689883  0.0558841   0.05539331\n",
      "  0.05399717  0.05216033]\n",
      "\n",
      "Step 19 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.05005573  0.04855385  0.04633851  0.04508808  0.04391489  0.04331392\n",
      "  0.04396872  0.04421928  0.0466157   0.04744764  0.0506932   0.05149951\n",
      "  0.05445214  0.05466782  0.05644379  0.05586293  0.05608118  0.05489717\n",
      "  0.05367544  0.05221059]\n",
      "\n",
      "Step 20 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.04978282  0.04767541  0.04623085  0.04475205  0.04389394  0.04387014\n",
      "  0.04393043  0.04555953  0.04642548  0.04912297  0.05024688  0.0530096\n",
      "  0.0537411   0.05565256  0.05559761  0.05615633  0.05532191  0.0545137\n",
      "  0.05315949  0.05135726]\n",
      "\n",
      "Step 21 of the RNN run on the first input of first batch of this epoch\n",
      "Read gamma - [ 1.]\n",
      "Write gamma - [ 1.]\n",
      "Write address -\n",
      "[ 0.04874682  0.04724353  0.04559582  0.04455691  0.04410965  0.04391841\n",
      "  0.04504436  0.04571943  0.04806261  0.0491727   0.05185945  0.05276942\n",
      "  0.05486927  0.05509713  0.05600011  0.05542621  0.054987    0.05379289\n",
      "  0.05227562  0.05075267]\n",
      "Epoch - 51, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 52, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 53, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 54, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 55, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 56, Mean error of final batch in epoch - 0.0\n",
      "Epoch - 57, Mean error of final batch in epoch - 0.0\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "# An annoying thing here is that we cannot use a list as a key in a \n",
    "# dictionary. The workaround we found on StackOverflow here:\n",
    "# http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "\n",
    "# epoch is a global var\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences\n",
    "        for z in range(batch_size):\n",
    "            # construct a sequence from 0,...,num_classes - 3 then append initial and terminal symbols\n",
    "            a = [random.randint(0,num_classes-3) for k in range(N-2)]\n",
    "            fa = func_to_learn(a)\n",
    "            a = [init_symbol] + a + [term_symbol]\n",
    "            a_onehot = [one_hots[e] for e in a]\n",
    "            fa_onehot = [one_hots[e] for e in fa]\n",
    "            inp.append(np.array(a_onehot))\n",
    "            out.append(np.array(fa_onehot))        \n",
    "        \n",
    "        feed_dict = {}\n",
    "        for d in range(N):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "        \n",
    "        # for the first batch in an epoch, we have some logging\n",
    "        if( j == 0 and i % 50 == 0 ):\n",
    "            gamma_reads_val, gamma_writes_val, read_addresses_val, write_addresses_val = sess.run([gamma_reads,gamma_writes,read_addresses,write_addresses],feed_dict)\n",
    "    \n",
    "            s = 0\n",
    "            for r in range(len(write_addresses_val)):\n",
    "                print(\"\")\n",
    "                print(\"Step \" + str(s) + \" of the RNN run on the first input of first batch of this epoch\")\n",
    "                print(\"Read gamma - \" + str(gamma_reads_val[r]))\n",
    "                print(\"Write gamma - \" + str(gamma_writes_val[r]))\n",
    "                print(\"Write address -\")\n",
    "                print(write_addresses_val[r])\n",
    "                s = s + 1\n",
    "        \n",
    "        # Do gradient descent\n",
    "        summary,_ = sess.run([merged_summaries,minimize], feed_dict)\n",
    "        \n",
    "        # Write out TensorBoard logs\n",
    "        file_writer.add_summary(summary)\n",
    "    current_mean = np.mean(sess.run(errors, feed_dict))\n",
    "    print(\"Epoch - \" + str(i+1) + \", Mean error of final batch in epoch - \" + str(current_mean))\n",
    "    \n",
    "    # DEBUG\n",
    "    #with tf.variable_scope(\"NTM\",reuse=True):\n",
    "    #    H = tf.get_variable(\"H\", [controller_state_size,controller_state_size])\n",
    "    #    print(sess.run(H))\n",
    "\n",
    "# Write out variables to disk\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took\", time.time() - pre_train_time, \"seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "# Note that all the weights will be loaded from the saved training session\n",
    "inputs_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest)]\n",
    "targets_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest_out)]\n",
    "state_test = tf.truncated_normal([batch_size, state_size], 0.0, 0.01, dtype=tf.float32)\n",
    "\n",
    "# Set up test graph\n",
    "reuse = True\n",
    "for i in range(Ntest):\n",
    "    output, state = cell(inputs_test[i],state_test,'NTM',reuse)\n",
    "\n",
    "rnn_outputs_test = []\n",
    "for i in range(Ntest_out):\n",
    "    output, state = cell(tf.zeros([batch_size,input_size]),state_test,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "    \n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.softmax(logit) for logit in logits_test] \n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets_test[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "\n",
    "# Restore the weights from training\n",
    "sess = tf.Session()\n",
    "saver.restore(sess,save_path)\n",
    "\n",
    "# DEBUG\n",
    "#with tf.variable_scope(\"NTM\",reuse=True):\n",
    "#    H = tf.get_variable(\"H\", [controller_state_size,controller_state_size])\n",
    "#    print(sess.run(H))\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "#print(\"Number of batches: \" + str(no_of_batches))\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    # We sample each batch on the fly from the set of all sequences\n",
    "    for z in range(batch_size):\n",
    "        a = [random.randint(0,num_classes-3) for k in range(Ntest-2)]\n",
    "        fa = func_to_learn(a)\n",
    "        a = [init_symbol] + a + [term_symbol]\n",
    "        a_onehot = [one_hots[e] for e in a]\n",
    "        fa_onehot = [one_hots[e] for e in fa]\n",
    "        inp.append(np.array(a_onehot))\n",
    "        out.append(np.array(fa_onehot))        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(Ntest):\n",
    "        in_node = inputs_test[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest_out):\n",
    "        out_node = targets_test[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = np.mean(sess.run(errors_test, feed_dict))\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "# The first three digits of this should match the printout for the\n",
    "# first three test output sequences given earlier\n",
    "#data = sess.run([tf.argmax(targets[0],1), tf.argmax(prediction[0],1)],feed_dict)\n",
    "\n",
    "#print(\"First digits of test outputs (actual)\")\n",
    "#print(data[0])\n",
    "#print(\"First digits of test outputs (predicted)\")\n",
    "#print(data[1])\n",
    "\n",
    "# print the mean of the errors in each digit for the test set.\n",
    "#incorrects = sess.run(errors, feed_dict)\n",
    "# print(incorrects)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"N_out         - \" + str(N_out))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "print(\"Ntest_out     - \" + str(Ntest_out))\n",
    "print(\"ring 1 powers - \" + str(powers_ring1))\n",
    "print(\"ring 2 powers - \" + str(powers_ring2))\n",
    "print(\"# epochs      - \" + str(epoch))\n",
    "print(\"optimizer     - \" + str(model_optimizer))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"(css,mas,mcs) - (\" + str(controller_state_size) + \",\" + str(memory_address_size) + \",\" + str(memory_content_size) + \")\")\n",
    "print(\"train percent - \" + str(training_percent))\n",
    "print(\"num_training  - \" + str(num_training) + \"/\" + str(num_classes**N))\n",
    "print(\"num_test      - \" + str(num_test) + \"/\" + str(num_classes**N))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error         - \" + str(final_error))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
