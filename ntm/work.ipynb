{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of linear logic recurrent neural network\n",
    "#\n",
    "# The architecture is a modified RNN, see the paper \"Linear logic and recurrent neural networks\".\n",
    "# Our inputs are sequences of symbols taken from an alphabet of size num_classes. The length\n",
    "# of the sequences is N. Our outputs are also sequences of length N from the same alphabet.\n",
    "#\n",
    "# Here \"symbol\" means a one hot vector.\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs\n",
    "\n",
    "###########\n",
    "# TODO/BUGS\n",
    "#\n",
    "# - Running on N = 10 crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# GLOBAL FLAGS\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, pattern_ntm_alt\n",
    "task                  = 'pattern' # copy, repeat copy, pattern\n",
    "epoch                 = 20 # number of training epochs\n",
    "num_classes           = 5 # number of symbols in the alphabet\n",
    "N                     = 20 # length of input sequences\n",
    "batch_size            = 500 # take a smaller batch size (500 works) on Tesla\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller\n",
    "memory_address_size   = 20 # number of memory locations\n",
    "memory_content_size   = 5 # size of vector stored at a memory location\n",
    "powers_ring1          = [0,1,2] # powers available to pattern NTM\n",
    "model_optimizer       = 'rmsprop' # adam, rmsprop\n",
    "LOG_DIR               = '/Users/murfetd/Coding/deeplinearlogic/log'\n",
    "\n",
    "training_percent      = 0.01 # percentage used for training\n",
    "num_training          = 20000 # int(training_percent * num_classes**N)\n",
    "num_test              = 2 * num_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[2, 0, 2, 1, 3, 3, 2, 1, 0, 0, 2, 2, 1, 1, 1, 4, 3, 2, 2, 3]\n",
      "is mapped to\n",
      "[2, 0, 0, 0, 1, 1, 3, 3, 3, 2, 2, 1, 1, 1, 0, 0, 2, 2, 2, 1, 1, 1, 1, 1, 4, 4, 3, 3, 3, 2, 2, 3, 3, 3, 0, 0, 2, 2, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "if( task == 'repeat copy' ):\n",
    "    pattern = [0,1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 2 * N\n",
    "\n",
    "##############\n",
    "# PATTERN TASK\n",
    "if( task == 'pattern' ):\n",
    "    pattern = [1,0,0,2,0]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 2 * N\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = [random.randint(0,num_classes-1) for i in range(N)]\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_59/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_58/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_57/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_56/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_55/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_54/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_53/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_52/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_51/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_50/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_49/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_48/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_47/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_46/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_45/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_44/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_43/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_42/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_41/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_40/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_39/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_38/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_37/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_36/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_35/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_34/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_33/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_32/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_31/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_30/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_29/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_28/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_27/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_26/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_25/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_24/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_23/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_22/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_21/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_20/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_19/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_18/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_17/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_15/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_13/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_11/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_9/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_7/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_5/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_3/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_1/split_grad/concat:0' shape=(500, 240) dtype=float32>, None, None]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'error:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "input_size = num_classes # dimension of the input space I\n",
    "\n",
    "# inputs, we create N of them, each of shape [None,input_size], one for\n",
    "# each position in the sequence\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N_out)]\n",
    "\n",
    "# state_size is the number of hidden neurons in each layer\n",
    "state_size = 0\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size = controller_state_size + 2*memory_address_size + memory_address_size * memory_content_size\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,memory_address_size,memory_content_size, [-1,0,1])\n",
    "elif( use_model == 'pattern_ntm' ):\n",
    "    state_size = controller_state_size + 4*memory_address_size + \\\n",
    "                memory_address_size * memory_content_size + \\\n",
    "                memory_address_size * len(powers_ring1)\n",
    "\n",
    "    cell = ntm.PatternNTM(state_size,input_size,controller_state_size,\n",
    "                          memory_address_size,memory_content_size, powers_ring1, [-1,0,1])\n",
    "elif( use_model == 'pattern_ntm_alt' ):\n",
    "    state_size = controller_state_size + 4*memory_address_size + \\\n",
    "                memory_address_size * memory_content_size + \\\n",
    "                memory_address_size * len(powers_ring1)\n",
    "\n",
    "    cell = ntm.PatternNTM_alt(state_size,input_size,controller_state_size,\n",
    "                          memory_address_size,memory_content_size, powers_ring1, [-1,0,1])\n",
    "\n",
    "# Initialise the state\n",
    "state = tf.truncated_normal([batch_size, state_size], 0.0, 0.01, dtype=tf.float32)\n",
    "#state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "reuse = False\n",
    "\n",
    "for i in range(N):\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    reuse = True\n",
    "\n",
    "# We only start recording the outputs of the controller once we have\n",
    "# finished feeding in the input. We feed zeros as input in the second phase.\n",
    "rnn_outputs = []\n",
    "for i in range(N_out):\n",
    "    output, state = cell(tf.zeros([batch_size,input_size]),state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "\n",
    "# Final fully connected layer\n",
    "E = tf.Variable(tf.truncated_normal([controller_state_size,input_size]))\n",
    "F = tf.Variable(tf.constant(0.1, shape=[input_size]))\n",
    "\n",
    "# prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * tf.log(prediction[i])) for i in range(N_out)]\n",
    "\n",
    "if( model_optimizer == 'adam' ):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "elif( model_optimizer == 'rmsprop' ):\n",
    "    optimizer = tf.train.RMSPropOptimizer(1e-4,decay=0.9,momentum=0.9)\n",
    "\n",
    "cross_entropy = -tf.add_n(ce)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "mean_error = tf.scalar_mul(np.true_divide(1,N_out), tf.add_n(errors))\n",
    "tf.summary.scalar('error', mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# INITIALISE MODEL #\n",
    "####################\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1, Mean error of final batch in epoch - 0.77555\n",
      "Epoch - 2, Mean error of final batch in epoch - 0.71845\n",
      "Epoch - 3, Mean error of final batch in epoch - 0.714\n",
      "Epoch - 4, Mean error of final batch in epoch - 0.6966\n",
      "Epoch - 5, Mean error of final batch in epoch - 0.68945\n",
      "Epoch - 6, Mean error of final batch in epoch - 0.679\n",
      "Epoch - 7, Mean error of final batch in epoch - 0.6697\n",
      "Epoch - 8, Mean error of final batch in epoch - 0.66875\n",
      "Epoch - 9, Mean error of final batch in epoch - 0.67085\n",
      "Epoch - 10, Mean error of final batch in epoch - 0.6507\n",
      "Epoch - 11, Mean error of final batch in epoch - 0.6405\n",
      "Epoch - 12, Mean error of final batch in epoch - 0.64365\n",
      "Epoch - 13, Mean error of final batch in epoch - 0.63695\n",
      "Epoch - 14, Mean error of final batch in epoch - 0.6237\n",
      "Epoch - 15, Mean error of final batch in epoch - 0.62785\n",
      "Epoch - 16, Mean error of final batch in epoch - 0.6246\n",
      "Epoch - 17, Mean error of final batch in epoch - 0.6154\n",
      "Epoch - 18, Mean error of final batch in epoch - 0.62125\n",
      "Epoch - 19, Mean error of final batch in epoch - 0.6135\n",
      "Epoch - 20, Mean error of final batch in epoch - 0.61085\n",
      "\n",
      "It took 227.423444986 seconds to train.\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "# An annoying thing here is that we cannot use a list as a key in a \n",
    "# dictionary. The workaround we found on StackOverflow here:\n",
    "# http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "\n",
    "# epoch is a global var\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences\n",
    "        for z in range(batch_size):\n",
    "            a = [random.randint(0,num_classes-1) for k in range(N)]\n",
    "            fa = func_to_learn(a)\n",
    "            a_onehot = [one_hots[e] for e in a]\n",
    "            fa_onehot = [one_hots[e] for e in fa]\n",
    "            inp.append(np.array(a_onehot))\n",
    "            out.append(np.array(fa_onehot))        \n",
    "        \n",
    "        feed_dict = {}\n",
    "        for d in range(N):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "        summary,_ = sess.run([merged_summaries,minimize], feed_dict)\n",
    "        file_writer.add_summary(summary)\n",
    "    current_mean = np.mean(sess.run(errors, feed_dict))\n",
    "    print(\"Epoch - \" + str(i+1) + \", Mean error of final batch in epoch - \" + str(current_mean))\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took\", time.time() - pre_train_time, \"seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch - 1, Mean error - 0.60605\n",
      "Batch - 2, Mean error - 0.60385\n",
      "Batch - 3, Mean error - 0.6078\n",
      "Batch - 4, Mean error - 0.60905\n",
      "Batch - 5, Mean error - 0.59985\n",
      "Batch - 6, Mean error - 0.6045\n",
      "Batch - 7, Mean error - 0.60605\n",
      "Batch - 8, Mean error - 0.60725\n",
      "Batch - 9, Mean error - 0.60735\n",
      "Batch - 10, Mean error - 0.60595\n",
      "Batch - 11, Mean error - 0.6105\n",
      "Batch - 12, Mean error - 0.59895\n",
      "Batch - 13, Mean error - 0.60205\n",
      "Batch - 14, Mean error - 0.61105\n",
      "Batch - 15, Mean error - 0.60795\n",
      "Batch - 16, Mean error - 0.6069\n",
      "Batch - 17, Mean error - 0.616\n",
      "Batch - 18, Mean error - 0.60825\n",
      "Batch - 19, Mean error - 0.6093\n",
      "Batch - 20, Mean error - 0.60225\n",
      "Batch - 21, Mean error - 0.5956\n",
      "Batch - 22, Mean error - 0.611\n",
      "Batch - 23, Mean error - 0.6069\n",
      "Batch - 24, Mean error - 0.60235\n",
      "Batch - 25, Mean error - 0.60495\n",
      "Batch - 26, Mean error - 0.6133\n",
      "Batch - 27, Mean error - 0.6094\n",
      "Batch - 28, Mean error - 0.6043\n",
      "Batch - 29, Mean error - 0.6042\n",
      "Batch - 30, Mean error - 0.60625\n",
      "Batch - 31, Mean error - 0.6012\n",
      "Batch - 32, Mean error - 0.60805\n",
      "Batch - 33, Mean error - 0.6009\n",
      "Batch - 34, Mean error - 0.6073\n",
      "Batch - 35, Mean error - 0.60685\n",
      "Batch - 36, Mean error - 0.60585\n",
      "Batch - 37, Mean error - 0.5987\n",
      "Batch - 38, Mean error - 0.6054\n",
      "Batch - 39, Mean error - 0.60475\n",
      "Batch - 40, Mean error - 0.6069\n",
      "Batch - 41, Mean error - 0.61\n",
      "Batch - 42, Mean error - 0.6018\n",
      "Batch - 43, Mean error - 0.60565\n",
      "Batch - 44, Mean error - 0.61305\n",
      "Batch - 45, Mean error - 0.59805\n",
      "Batch - 46, Mean error - 0.59685\n",
      "Batch - 47, Mean error - 0.60905\n",
      "Batch - 48, Mean error - 0.6051\n",
      "Batch - 49, Mean error - 0.60625\n",
      "Batch - 50, Mean error - 0.60865\n",
      "Batch - 51, Mean error - 0.601\n",
      "Batch - 52, Mean error - 0.6118\n",
      "Batch - 53, Mean error - 0.6003\n",
      "Batch - 54, Mean error - 0.6059\n",
      "Batch - 55, Mean error - 0.6077\n",
      "Batch - 56, Mean error - 0.6045\n",
      "Batch - 57, Mean error - 0.60165\n",
      "Batch - 58, Mean error - 0.60925\n",
      "Batch - 59, Mean error - 0.6147\n",
      "Batch - 60, Mean error - 0.60375\n",
      "Batch - 61, Mean error - 0.6117\n",
      "Batch - 62, Mean error - 0.6023\n",
      "Batch - 63, Mean error - 0.6122\n",
      "Batch - 64, Mean error - 0.6136\n",
      "Batch - 65, Mean error - 0.60385\n",
      "Batch - 66, Mean error - 0.61135\n",
      "Batch - 67, Mean error - 0.6033\n",
      "Batch - 68, Mean error - 0.6093\n",
      "Batch - 69, Mean error - 0.61015\n",
      "Batch - 70, Mean error - 0.6059\n",
      "Batch - 71, Mean error - 0.60935\n",
      "Batch - 72, Mean error - 0.6032\n",
      "Batch - 73, Mean error - 0.59695\n",
      "Batch - 74, Mean error - 0.6037\n",
      "Batch - 75, Mean error - 0.60875\n",
      "Batch - 76, Mean error - 0.6106\n",
      "Batch - 77, Mean error - 0.6034\n",
      "Batch - 78, Mean error - 0.6023\n",
      "Batch - 79, Mean error - 0.6082\n",
      "Batch - 80, Mean error - 0.60305\n",
      "\n",
      "###########\n",
      "# Summary #\n",
      "###########\n",
      "\n",
      "model         - ntm\n",
      "task name     - pattern\n",
      "num_classes   - 5\n",
      "N             - 20\n",
      "N_out         - 40\n",
      "ring powers   - [0, 1, 2]\n",
      "# epochs      - 20\n",
      "optimizer     - rmsprop\n",
      "# weights     - 13221\n",
      "(css,mas,mcs) - (100,20,5)\n",
      "train percent - 0.01\n",
      "num_training  - 20000/95367431640625\n",
      "num_test      - 40000/95367431640625\n",
      "\n",
      "\n",
      "error         - 0.60604\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "#print(\"Number of batches: \" + str(no_of_batches))\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    # We sample each batch on the fly from the set of all sequences\n",
    "    for z in range(batch_size):\n",
    "        a = [random.randint(0,num_classes-1) for k in range(N)]\n",
    "        fa = func_to_learn(a)\n",
    "        a_onehot = [one_hots[e] for e in a]\n",
    "        fa_onehot = [one_hots[e] for e in fa]\n",
    "        inp.append(np.array(a_onehot))\n",
    "        out.append(np.array(fa_onehot))        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(N):\n",
    "        in_node = inputs[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(N_out):\n",
    "        out_node = targets[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = np.mean(sess.run(errors, feed_dict))\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "# The first three digits of this should match the printout for the\n",
    "# first three test output sequences given earlier\n",
    "#data = sess.run([tf.argmax(targets[0],1), tf.argmax(prediction[0],1)],feed_dict)\n",
    "\n",
    "#print(\"First digits of test outputs (actual)\")\n",
    "#print(data[0])\n",
    "#print(\"First digits of test outputs (predicted)\")\n",
    "#print(data[1])\n",
    "\n",
    "# print the mean of the errors in each digit for the test set.\n",
    "#incorrects = sess.run(errors, feed_dict)\n",
    "# print(incorrects)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"N_out         - \" + str(N_out))\n",
    "print(\"ring powers   - \" + str(powers_ring1))\n",
    "print(\"# epochs      - \" + str(epoch))\n",
    "print(\"optimizer     - \" + str(model_optimizer))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"(css,mas,mcs) - (\" + str(controller_state_size) + \",\" + str(memory_address_size) + \",\" + str(memory_content_size) + \")\")\n",
    "print(\"train percent - \" + str(training_percent))\n",
    "print(\"num_training  - \" + str(num_training) + \"/\" + str(num_classes**N))\n",
    "print(\"num_test      - \" + str(num_test) + \"/\" + str(num_classes**N))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error         - \" + str(final_error))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
