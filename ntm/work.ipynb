{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of the Linear Logic Recurrent Neural Network (LLRNN)\n",
    "#\n",
    "# Version 9.0\n",
    "\n",
    "###################\n",
    "# HYPERPARAMETERS #\n",
    "###################\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, mult_pattern_ntm\n",
    "task                  = 'copy' # copy, repeat copy, pattern i, mult pattern i\n",
    "epoch                 = 100 # number of training epochs, default to 100\n",
    "num_classes           = 10 # number of symbols, INCLUDING initial and terminal symbols, default 10\n",
    "N                     = 30 # length of input sequences for training, default to 30, INCLUDING initial and terminal symbols\n",
    "Ntest                 = 35 # length of sequences for testing, default to 35, INCLUDING initial and terminal symbols\n",
    "batch_size            = 250 # default 250\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "num_training          = 10000 # default 10000\n",
    "num_test              = num_training\n",
    "term_symbol           = num_classes - 1\n",
    "init_symbol           = num_classes - 2\n",
    "div_symbol            = num_classes - 3\n",
    "learning_rate         = 1e-4 # default 1e-4\n",
    "memory_init_bias      = 1.0 # default 1.0\n",
    "use_curriculum        = False # default False\n",
    "\n",
    "##################\n",
    "# MODEL SPECIFIC #\n",
    "##################\n",
    "\n",
    "ntm_memory_address_size   = 128 # number of memory locations, default 128\n",
    "ntm_memory_content_size   = 20 # size of vector stored at a memory location, default 20\n",
    "ntm_powers                = [0,-1,1] # powers of R used by controller, default [0,-1,1]\n",
    "\n",
    "pattern_ntm_powers               = [[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "pattern_ntm_powers_2_on_1        = [0,1,2] # allowed powers used by ring 2 to manipulate ring 1\n",
    "pattern_ntm_memory_address_sizes = [128, 1] # number of memory locations for the three rings\n",
    "pattern_ntm_memory_content_sizes = [20, 3] # size of content vector for each ring\n",
    "pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "mult_pattern_ntm_powers               = [[0,-1,1],[0,-1,1],[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "mult_pattern_ntm_powers_2_on_1        = [0,1,2] # allowed powers used by rings 2,3 to manipulate ring 1\n",
    "mult_pattern_ntm_memory_address_sizes = [128, 20, 20, 10] # number of memory locations for the rings\n",
    "mult_pattern_ntm_memory_content_sizes = [20, 3, 3, 2] # size of content vector for each ring\n",
    "mult_pattern_ntm_direct_bias          = 1.0\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there\n",
    "# 2. The initial and terminal symbols are always from the end of the list of symbols, so they\n",
    "# are respectively num_classes - 2 and num_classes - 1. So the number of symbols which are\n",
    "# not initial or terminal is num_classes - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import pandas\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs\n",
    "\n",
    "assert use_model == 'ntm' or use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[3, 2, 1, 1, 7, 4, 6, 4]\n",
      "is mapped to\n",
      "[3, 2, 1, 1, 4, 4, 6, 6, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "# Below N_out and Ntest_out are the lengths of the outputs in both the training\n",
    "# and testing regimes respectively. Since outputs do not include the initial and terminal\n",
    "# symbols, these default to N - 2 and Ntest - 2 respectively.\n",
    "\n",
    "# Default sampling from space of inputs\n",
    "def generate_input_seq_default(max_symbol,input_length):\n",
    "    return [random.randint(0,max_symbol) for k in range(input_length)]\n",
    "\n",
    "generate_input_seq = generate_input_seq_default\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N - 2\n",
    "    Ntest_out = Ntest - 2\n",
    "    seq_length_min = 7\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "if( task == 'repeat copy' ):\n",
    "    no_of_copies = 2\n",
    "    pattern = [0]*(no_of_copies - 1) + [1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = no_of_copies * (N - 2)\n",
    "    Ntest_out = no_of_copies * (Ntest - 2)\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 1\n",
    "if( task == 'pattern 1' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,1,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,c,c,d,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = (N - 2) + divmod(N - 2, 2)[0] # N - 2 plus the number of times 2 divides N - 2\n",
    "    Ntest_out = (Ntest - 2) + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 2\n",
    "if( task == 'pattern 2' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = N - 2 + divmod(N - 2, 2)[0]\n",
    "    Ntest_out = Ntest - 2 + divmod(Ntest - 2, 2)[0]\n",
    "    seq_length_min = 7\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 3\n",
    "if( task == 'pattern 3' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,b,b,d,c,c,e,d,d,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 4 + (N - 2 - 2) * 3\n",
    "    Ntest_out = 4 + (Ntest - 2 - 2) * 3\n",
    "    seq_length_min = 7\n",
    "\n",
    "################\n",
    "# PATTERN TASK 4\n",
    "if( task == 'pattern 4' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2,1,2,-2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,d,f,d,c,c,e,f,h,f,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 7\n",
    "\n",
    "################\n",
    "# PATTERN TASK 5\n",
    "if( task == 'pattern 5' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [4,1,1,-4] # so (a,b,c,d,e,f,...) goes to (a,e,f,g,k,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "    seq_length_min = 7\n",
    "\n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 1\n",
    "if( task == 'mult pattern 1' or task == 'mult pattern 2'):\n",
    "    pattern1 = [1] # so (a,b,c,d,e,f,...) goes to (a,b,c,d,e,f,...)\n",
    "    pattern2 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 7\n",
    "    \n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 2\n",
    "if( task == 'mult pattern 2' ):\n",
    "    # Almost everything is the same as mult pattern 1, but in pattern 2 we \n",
    "    # make sure there is a div symbol somewhere in the sequence\n",
    "    def generate_input_seq_forcediv(max_symbol,input_length):\n",
    "        t = [random.randint(0,max_symbol) for k in range(input_length)]\n",
    "        div_pos = random.randint(0,len(t)-1)\n",
    "        t[div_pos] = div_symbol\n",
    "        return t\n",
    "    \n",
    "    generate_input_seq = generate_input_seq_forcediv\n",
    "\n",
    "#########################\n",
    "# MULTIPLE PATTERN TASK 3\n",
    "if( task == 'mult pattern 3'):\n",
    "    pattern1 = [1] # so (a,b,c,d,e,f,...) goes to (a,b,c,d,e,f,...)\n",
    "    pattern2 = [0,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,b,...)\n",
    "    pattern3 = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_multpattern(s,[pattern1,pattern2,pattern3],div_symbol)\n",
    "    N_out = 2*(N-2)\n",
    "    Ntest_out = 2*(Ntest-2)\n",
    "    seq_length_min = 7\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = [random.randint(0,num_classes-3) for i in range(N - 2)]\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# INITIALISE STATE #\n",
    "####################\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "input_size = num_classes # dimension of the input space I\n",
    "\n",
    "#####\n",
    "# NTM\n",
    "\n",
    "def init_state_ntm(batch_size, css, mas, mcs):\n",
    "    state_size = css + 2*mas + mas * mcs\n",
    "    \n",
    "    ra = [0.0]*mas\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,mas]) + ra\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    init_read_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_write_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_memory = tf.truncated_normal([batch_size, mas*mcs], 0.0, 1e-6, dtype=tf.float32)\n",
    "    \n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "#############\n",
    "# PATTERN NTM\n",
    "\n",
    "def init_state_pattern_ntm(batch_size, css, mas, mcs):\n",
    "    # mas and mcs are arrays of address sizes and content sizes for rings\n",
    "    state_size = css\n",
    "    \n",
    "    init_address = []\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        state_size = state_size + mas[i] * mcs[i] # for memory vector\n",
    "        state_size = state_size + 2 * mas[i] # for addresses (read and write)\n",
    "    \n",
    "        ra = [0.0]*mas[i]\n",
    "        ra[0] = 1.0\n",
    "        init_address.append(np.zeros([batch_size,mas[i]]) + ra)\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    \n",
    "    tensor_list = [init_controller_state]\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        init_read_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        init_write_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        tensor_list = tensor_list + [init_read_address,init_write_address]\n",
    "        \n",
    "    for i in range(len(mas)):\n",
    "        # The first ring is initialised to zero, the rest differently\n",
    "        if( i == 0 ):\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "        else:\n",
    "            # This initialisation has the result of biasing the output of rings 2 and 3 to be\n",
    "            # \"no rotation\" and biasing ring 4 to say \"use ring 2\"\n",
    "            ra = [0.0]*mcs[i] \n",
    "            ra[0] = memory_init_bias\n",
    "            ra = np.zeros([batch_size,mas[i],mcs[i]]) + ra\n",
    "            ra = tf.constant(ra,dtype=tf.float32,shape=[batch_size,mas[i],mcs[i]])\n",
    "            ra = tf.reshape(ra,[batch_size,mas[i]*mcs[i]])\n",
    "            init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32) + ra\n",
    "            #init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "            \n",
    "        tensor_list = tensor_list + [init_memory]\n",
    "    \n",
    "    state = tf.concat(tensor_list,1)\n",
    "\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "######################\n",
    "# MULTIPLE PATTERN NTM\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_25/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_24/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_23/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_22/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_21/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_20/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_19/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_18/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_17/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_15/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_13/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_11/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_9/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_7/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_5/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_3/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_1/split_grad/concat:0' shape=(250, 2916) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "\n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "read_addresses2 = []\n",
    "read_addresses3 = []\n",
    "read_addresses4 = []\n",
    "write_addresses = []\n",
    "write_addresses2 = []\n",
    "write_addresses3 = []\n",
    "write_addresses4 = []\n",
    "interps = []\n",
    "rnn_outputs = []\n",
    "m2 = []\n",
    "m3 = []\n",
    "m4 = []\n",
    "    \n",
    "for i in range(N + N_out):\n",
    "    \n",
    "    old_state = state\n",
    "\n",
    "    #### RUN MODEL ####\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "    ###################\n",
    "\n",
    "    reuse = True\n",
    "    \n",
    "    #### SET UP NODES FOR LOGGING #####\n",
    "    if( use_model == 'ntm' ):\n",
    "        h0, curr_read, curr_write, _ = tf.split(old_state, [controller_state_size,ntm_memory_address_size,\n",
    "                                                        ntm_memory_address_size,-1], 1)\n",
    "\n",
    "    if( use_model == 'pattern_ntm' ):\n",
    "        mas = pattern_ntm_memory_address_sizes\n",
    "        mcs = pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(old_state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],mas[0] * mcs[0],mas[1] * mcs[1]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        m1_state = ret[5]\n",
    "        m2_state = ret[6]\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm' ):\n",
    "        mas = mult_pattern_ntm_memory_address_sizes\n",
    "        mcs = mult_pattern_ntm_memory_content_sizes\n",
    "        \n",
    "        ret = tf.split(old_state, [controller_state_size,mas[0],mas[0],mas[1],mas[1],                        \n",
    "                            mas[2],mas[2],mas[3],mas[3],mas[0] * mcs[0],mas[1] * mcs[1],\n",
    "                            mas[2] * mcs[2],mas[3] * mcs[3]], 1)\n",
    "        \n",
    "        h0 = ret[0]\n",
    "        curr_read = ret[1]\n",
    "        curr_write = ret[2]\n",
    "        curr_read2 = ret[3]\n",
    "        curr_write2 = ret[4]\n",
    "        curr_read3 = ret[5]\n",
    "        curr_write3 = ret[6]\n",
    "        curr_read4 = ret[7]\n",
    "        curr_write4 = ret[8]\n",
    "        m1_state = ret[9]\n",
    "        m2_state = ret[10]\n",
    "        m3_state = ret[11]\n",
    "        m4_state = ret[12]\n",
    "        \n",
    "    read_addresses.append(curr_read[0,:])\n",
    "    write_addresses.append(curr_write[0,:])\n",
    "    \n",
    "    if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses2.append(curr_read2[0,:])\n",
    "        write_addresses2.append(curr_write2[0,:])\n",
    "        m2_state = tf.reshape(m2_state, [-1,mas[1],mcs[1]])\n",
    "        m2.append(tf.nn.softmax(m2_state[0,:]))\n",
    "        \n",
    "        with tf.variable_scope(\"NTM\",reuse=True):\n",
    "            W_interp = tf.get_variable(\"W_interp\", [controller_state_size,1])\n",
    "            B_interp = tf.get_variable(\"B_interp\", [1])\n",
    "            interp = tf.sigmoid(tf.matmul(h0,W_interp) + B_interp)\n",
    "            interp_matrix = tf.concat([interp,tf.ones_like(interp,dtype=tf.float32) - interp],axis=1) # shape [-1,2]\n",
    "            interps.append(interp_matrix[0,:])\n",
    "        \n",
    "    if( use_model == 'mult_pattern_ntm'):\n",
    "        read_addresses3.append(curr_read3[0,:])\n",
    "        write_addresses3.append(curr_write3[0,:])\n",
    "        read_addresses4.append(curr_read4[0,:])\n",
    "        write_addresses4.append(curr_write4[0,:])\n",
    "        m3_state = tf.reshape(m3_state, [-1,mult_pattern_ntm_memory_address_sizes[2],mult_pattern_ntm_memory_content_sizes[2]])\n",
    "        m3.append(tf.nn.softmax(m3_state[0,:]))\n",
    "        m4_state = tf.reshape(m4_state, [-1,mult_pattern_ntm_memory_address_sizes[3],mult_pattern_ntm_memory_content_sizes[3]])\n",
    "        m4_state = m4_state[0,:]\n",
    "        m4_state = tf.concat([tf.nn.softmax(m4_state),tf.zeros([mult_pattern_ntm_memory_address_sizes[3],1])],1)\n",
    "        m4.append(m4_state)\n",
    "    ### END LOGGING ###\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# Note: prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "\n",
    "# Note: We allow the length of input sequences to vary between batches, which means\n",
    "# that the cross entropy needs to be masked to the relevant part of the output\n",
    "\n",
    "# Note: we use log_softmax to avoid precision issues with floats causing log(0) to create NaNs\n",
    "\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.log_softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * prediction[i]) for i in range(N + N_out)] # an array of numbers\n",
    "mask = [tf.sign(tf.reduce_max(tf.abs(targets[i]))) for i in range(N + N_out)]\n",
    "ce_mask = [ce[i] * mask[i] for i in range(N + N_out)]\n",
    "cross_entropy = -tf.add_n(ce_mask)\n",
    "cross_entropy /= tf.add_n(mask) # DEBUG do we really need this?\n",
    "# NOTE: here in creating the mask we are assuming that batches have the same sequence length\n",
    "                    \n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate,decay=0.9,momentum=0.9)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N + N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "errors_mask = [errors[i] * mask[i] for i in range(N + N_out)]\n",
    "mean_error = tf.add_n(errors_mask)\n",
    "mean_error /= tf.add_n(mask)\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1, mean error - 0.920432\n",
      "Epoch - 2, mean error - 0.898946\n",
      "\n",
      "It took 31 seconds to train.\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "###################\n",
    "# Note on sequences\n",
    "#\n",
    "# Our sequences are of varying length, in the alphabet {0,...,num_classes - 3}.\n",
    "# Each input sequence begins with an initial symbol and ends with a terminal symbol\n",
    "# (the value of which are num_classes - 2 and num_classes - 1 by default). Output\n",
    "# sequences do not have either an initial nor a terminal symbol.\n",
    "#\n",
    "# Both input and output sequences are written on a \"tape\" of length N + N_out.\n",
    "# Input sequences are aligned at the BEGINNING of the tape, and all remaining space\n",
    "# is filled with terminal symbols. Output sequences are aligned at the END OF THE \n",
    "# MATCHING INPUT, with all remaining space filled with zero vectors.\n",
    "#\n",
    "# Example: suppose N = N_out = 10, and num_classes = 10 so that init_symbol = 8\n",
    "# and term_symbol = 9. Then a sequence of length 8 (seq_length = 10 below) is\n",
    "#\n",
    "# a = [4, 4, 5, 6, 3, 3, 6, 7]\n",
    "#\n",
    "# which written on the tape is\n",
    "#\n",
    "# [8, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
    "#\n",
    "# If we are performing the copy task, so that the output sequence is also a, then\n",
    "# the output written on the tape will be (notice the alignment)\n",
    "#\n",
    "# [-, -, -, -, -, -, -, -, -, 4, 4, 5, 6, 3, 3, 6, 7, -, -, -]\n",
    "#\n",
    "# where - is a symbol whose encoding is the zero vector.\n",
    "\n",
    "def io_generator(max_symbol, input_length, total_length):\n",
    "    \"\"\"\n",
    "    Returns a one-hot encoded pair of input and output sequence, with terminal and initial symbols.\n",
    "    \n",
    "    max_symbol - generate sequences in 0,...,max_symbol\n",
    "    input_length - length of input sequences, without initial and terminal symbols\n",
    "    total_length - length of the buffer, so that the sequences are padded to this length\n",
    "    \"\"\"\n",
    "    a = generate_input_seq(max_symbol,input_length)\n",
    "    fa = func_to_learn(a)\n",
    "    a = [init_symbol] + a + [term_symbol]\n",
    "    a = a + [term_symbol for k in range(total_length-len(a))]\n",
    "    a_onehot = [one_hots[e] for e in a]\n",
    "    fa_onehot = [[0.0]*num_classes for k in range(input_length+1)] + \\\n",
    "                [one_hots[e] for e in fa] + \\\n",
    "                [[0.0]*num_classes for k in range(total_length-(input_length+1)-len(fa))]\n",
    "    return a, fa, np.array(a_onehot), np.array(fa_onehot)\n",
    "\n",
    "error_means = []\n",
    "epoch_error_means = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences. Each\n",
    "        # batch has a fixed length of the sequences. Recall that all input seqs\n",
    "        # have an initial and terminal symbol, so if seq_length = 10 then there\n",
    "        # are eight positions for the \"content\" symbols\n",
    "        \n",
    "        # Our version of curriculum training says: spend the first half\n",
    "        # of the epochs ramping up to the full training set. Assuming that\n",
    "        # epoch > N we divide allocate each integer in [seq_length_min,N]\n",
    "        # an equal portion of the first half of the epochs.\n",
    "        if( use_curriculum == True ):\n",
    "            if( 2 * i > epoch ):\n",
    "                seq_length_max = N\n",
    "            else:\n",
    "                curriculum_band = max(1,int(epoch/(2*(N - seq_length_min))))\n",
    "                seq_length_max = min(seq_length_min + int(i/curriculum_band),N)\n",
    "        else:\n",
    "            seq_length_max = N\n",
    "            \n",
    "        seq_length = random.randint(seq_length_min,seq_length_max)\n",
    "        \n",
    "        # Hack: if we are on the final batch of the final epoch, force\n",
    "        # it to use the full sequence length, so we get a good visualisation\n",
    "        if( i + 1 == epoch and j + 1 == no_of_batches ):\n",
    "            seq_length = N\n",
    "        \n",
    "        for z in range(batch_size):\n",
    "            a, fa, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=N+N_out)\n",
    "            \n",
    "            inp.append(a_onehot)\n",
    "            out.append(fa_onehot)\n",
    "            \n",
    "            # Record the first sequence in the last batch of the last epoch\n",
    "            if( i == epoch - 1 and j == no_of_batches - 1 and z == 0):\n",
    "                final_seq = a\n",
    "                final_seq_output = fa\n",
    "        \n",
    "        # An annoying thing here is that we cannot use a list as a key in a \n",
    "        # dictionary. The workaround we found on StackOverflow here:\n",
    "        # http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "        feed_dict = {}\n",
    "        \n",
    "        for d in range(N + N_out):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N + N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "\n",
    "        ##### Do gradient descent #####\n",
    "        mean_error_val,_ = sess.run([mean_error,minimize], feed_dict)\n",
    "        ###############################\n",
    "        \n",
    "        error_means.append(mean_error_val)\n",
    "    \n",
    "    epoch_error = np.mean(error_means[-no_of_batches:])\n",
    "    epoch_error_means.append(epoch_error)\n",
    "    \n",
    "    # Print the mean error of the final batch in the epoch\n",
    "    print_str = \"Epoch - \" + str(i+1) + \", mean error - \" + str(epoch_error)\n",
    "    \n",
    "    if( use_curriculum == True ):\n",
    "        print_str = print_str + \", training at max length - \" + str(seq_length_max)\n",
    "        \n",
    "    print(print_str)\n",
    "\n",
    "# For the final batch of the final epoch, we record the memory states as well\n",
    "seq_length_for_vis = seq_length - 2\n",
    "interps_val = sess.run(interps,feed_dict)\n",
    "m2_val, m3_val, m4_val = sess.run([m2,m3,m4],feed_dict)            \n",
    "r1_val, w1_val = sess.run([read_addresses,write_addresses],feed_dict)\n",
    "r2_val, w2_val = sess.run([read_addresses2,write_addresses2],feed_dict)\n",
    "r3_val, w3_val = sess.run([read_addresses3,write_addresses3],feed_dict)\n",
    "r4_val, w4_val = sess.run([read_addresses4,write_addresses4],feed_dict)\n",
    "errors_mask_val = sess.run(errors_mask,feed_dict)\n",
    "\n",
    "mask_val = sess.run(mask,feed_dict)\n",
    "predicted_seq = [tf.argmax(prediction[i], 1) for i in range(N + N_out)]\n",
    "predicted_seq_val = sess.run(predicted_seq,feed_dict)\n",
    "final_seq_pred_0 = [a[0] for a in predicted_seq_val]\n",
    "final_seq_pred = []\n",
    "\n",
    "for i in range(len(mask_val)):\n",
    "    if( mask_val[i] == 1.0 ):\n",
    "        final_seq_pred.append(final_seq_pred_0[i])\n",
    "\n",
    "# Write out variables to disk\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took \" + str(int(time.time() - pre_train_time)) + \" seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length used for visualisations - 8\n",
      "\n",
      "Sequence used for visualisations is (Note: initial symbol is 8, terminal symbol is 9)\n",
      "[8, 7, 7, 4, 6, 7, 4, 5, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "\n",
      "Correct output for this sequence:\n",
      "[4, 6, 4, 4, 5, 5]\n",
      "\n",
      "Predicted output for this sequence\n",
      "[6, 6, 4, 3, 6, 3, 6, 2, 2, 6, 2, 2, 2, 2]\n",
      "\n",
      "Mask for output\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Error probabilities for final batch\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.84799999, 0.81599998, 0.77600002, 0.83600003, 0.86799997, 0.86799997, 0.90399998, 0.90799999, 0.91600001, 0.94800001, 0.98000002, 0.98000002, 0.95999998, 0.96399999, 0.0, 0.0, 0.0]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAAL8CAYAAAC2zccJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nklQzJnqSJEmSVDP/H7LJgB4mxlDSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119d91d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 1 #\n",
    "###########################\n",
    "\n",
    "print(\"Sequence length used for visualisations - \" + str(seq_length_for_vis))\n",
    "print(\"\")\n",
    "print(\"Sequence used for visualisations is (Note: initial symbol is \" + str(init_symbol) + \", terminal symbol is \" + str(term_symbol) + \")\")\n",
    "print(final_seq)\n",
    "print(\"\")\n",
    "print(\"Correct output for this sequence:\")\n",
    "print(final_seq_output)\n",
    "print(\"\")\n",
    "print(\"Predicted output for this sequence\")\n",
    "print(final_seq_pred)\n",
    "print(\"\")\n",
    "#print(\"Mask for output\")\n",
    "#print(mask_val)\n",
    "#print(\"\")\n",
    "print(\"Error probabilities for final batch\")\n",
    "print(errors_mask_val)\n",
    "print(\"\")\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 9, 13\n",
    "fig_num = 0\n",
    "\n",
    "# RING 1\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "plt.figure(fig_num)\n",
    "ax1 = plt.subplot(2,1,1)\n",
    "ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "ax1.imshow(np.stack(w1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax1.set_title('Write address (ring 1)')\n",
    "ax1.set_xlabel('position')\n",
    "ax1.set_ylabel('time')\n",
    "\n",
    "ax2.imshow(np.stack(r1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax2.set_title('Read address (ring 1)')\n",
    "ax2.set_xlabel('position')\n",
    "ax2.set_ylabel('time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# VISUALISATIONS - RING 2 #\n",
    "###########################\n",
    "\n",
    "if( use_model == 'pattern_ntm' or use_model == 'mult_pattern_ntm'):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 2)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 2)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Assume that powers2_on_1 has three entries we can use as colour channels\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 2)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    max_xticks = 2\n",
    "    xloc = plt.MaxNLocator(max_xticks)\n",
    "\n",
    "    ax.imshow(np.stack(interps_val), cmap='bone', interpolation='nearest', aspect='auto')\n",
    "    ax.set_title('Interpolation')\n",
    "    ax.set_xlabel('direct vs indirect')\n",
    "    ax.set_ylabel('time')\n",
    "    ax.xaxis.set_major_locator(xloc)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# VISUALISATIONS - OTHER RINGS #\n",
    "################################\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 3)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 3)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax = plt.subplot(1,1,1)    \n",
    "    ax.imshow(np.stack(m3_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax.set_title('Memory contents (ring 3)')\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_num = fig_num + 1\n",
    "    \n",
    "    plt.figure(fig_num)\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "    ax1.imshow(np.stack(w4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax1.set_title('Write address (ring 4)')\n",
    "    ax1.set_xlabel('position')\n",
    "    ax1.set_ylabel('time')\n",
    "\n",
    "    ax2.imshow(np.stack(r4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax2.set_title('Read address (ring 4)')\n",
    "    ax2.set_xlabel('position')\n",
    "    ax2.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig_num = fig_num + 1\n",
    "    plt.figure(fig_num)\n",
    "    ax6 = plt.subplot(1,1,1)    \n",
    "    ax6.imshow(np.stack(m4_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax6.set_title('Memory contents (ring 4)')\n",
    "    ax6.set_xlabel('position')\n",
    "    ax6.set_ylabel('time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3kAAAUKCAYAAABblriAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x138013350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########################\n",
    "# VISUALISATIONS - ERROR #\n",
    "##########################\n",
    "\n",
    "fig_num = fig_num + 1\n",
    "\n",
    "plt.figure(fig_num)\n",
    "ax = plt.subplot(1,1,1)\n",
    "sc = pandas.Series(error_means)\n",
    "ma = sc.rolling(window=500).mean()\n",
    "ax.plot(sc.index, sc, color='lightgray')\n",
    "ax.plot(ma.index, ma, color='red')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(sc.index.min(), sc.index.max())\n",
    "ax.set_title('Error')\n",
    "ax.set_xlabel('batch')\n",
    "ax.set_ylabel('error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on sequences of length 13\n",
      "\n",
      "Batch - 1, Mean error - 0.880167\n",
      "Batch - 2, Mean error - 0.885333\n",
      "Batch - 3, Mean error - 0.8925\n",
      "Batch - 4, Mean error - 0.887833\n",
      "\n",
      "###########\n",
      "# Summary #\n",
      "###########\n",
      "\n",
      "model         - ntm\n",
      "task name     - mult pattern 1\n",
      "epochs        - 2\n",
      "num_classes   - 10\n",
      "N             - 10\n",
      "Ntest         - 15\n",
      "# weights     - 18958\n",
      "\n",
      "\n",
      "error train(test) - 0.898946 (0.886458)\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "# Restore the weights from training\n",
    "sess = tf.Session()\n",
    "saver.restore(sess,save_path)\n",
    "\n",
    "inputs_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "targets_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1, pattern_ntm_direct_bias)\n",
    "\n",
    "if( use_model == 'mult_pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, mult_pattern_ntm_memory_address_sizes, \n",
    "                                               mult_pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.MultPatternNTM(state_size, input_size, controller_state_size, mult_pattern_ntm_memory_address_sizes,\n",
    "                          mult_pattern_ntm_memory_content_sizes, mult_pattern_ntm_powers, mult_pattern_ntm_powers_2_on_1, \n",
    "                              mult_pattern_ntm_direct_bias)\n",
    "# Set up test graph\n",
    "rnn_outputs_test = []\n",
    "reuse = True\n",
    "for i in range(Ntest + Ntest_out):\n",
    "    output, state = cell(inputs_test[i],state,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "\n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size])\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.log_softmax(logit) for logit in logits_test] \n",
    "mask = [tf.sign(tf.reduce_max(tf.abs(targets_test[i]))) for i in range(Ntest + Ntest_out)]\n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets_test[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest + Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "errors_test_mask = [errors_test[i] * mask[i] for i in range(Ntest + Ntest_out)]\n",
    "mean_error_test = tf.add_n(errors_test_mask)\n",
    "mean_error_test /= tf.add_n(mask)\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "\n",
    "seq_length = Ntest\n",
    "print(\"Testing on sequences of length \" + str(seq_length-2))\n",
    "print(\"\")\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    for z in range(batch_size):\n",
    "        a, fa, a_onehot, fa_onehot = io_generator(max_symbol=num_classes-3,\n",
    "                                                      input_length=seq_length-2,\n",
    "                                                      total_length=Ntest+Ntest_out)\n",
    "            \n",
    "        inp.append(a_onehot)\n",
    "        out.append(fa_onehot)        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        in_node = inputs_test[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        out_node = targets_test[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = sess.run(mean_error_test, feed_dict)\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"epochs        - \" + str(epoch))\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error train(test) - \" + str(epoch_error_means[-1]) + \" (\" + str(final_error) + \")\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
