{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of the Linear Logic Recurrent Neural Network (LLRNN)\n",
    "#\n",
    "\n",
    "###################\n",
    "# HYPERPARAMETERS #\n",
    "###################\n",
    "\n",
    "use_model             = 'pattern_ntm' # ntm, pattern_ntm\n",
    "task                  = 'copy' # copy, repeat copy, pattern i \n",
    "epoch                 = 2 # number of training epochs, default to 200\n",
    "num_classes           = 10 # number of symbols, INCLUDING initial and terminal symbols\n",
    "N                     = 30 # length of input sequences for training, default to 20, INCLUDING initial and terminal symbols\n",
    "Ntest                 = 35 # length of sequences for testing, default to N, INCLUDING initial and terminal symbols\n",
    "batch_size            = 250 # default to 500 (too large does not fit on GPUs)\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "num_training          = 10000 # default to int(training_percent * (num_classes-2)**N)\n",
    "num_test              = num_training\n",
    "init_symbol           = num_classes - 2\n",
    "term_symbol           = num_classes - 1\n",
    "seq_length_min        = 4\n",
    "\n",
    "##################\n",
    "# MODEL SPECIFIC #\n",
    "##################\n",
    "\n",
    "ntm_memory_address_size   = 128 # number of memory locations, default 128\n",
    "ntm_memory_content_size   = 20 # size of vector stored at a memory location, default 20\n",
    "ntm_powers                = [0,-1,1] # powers of R used by controller, default [0,-1,1]\n",
    "\n",
    "pattern_ntm_powers               = [[0,-1,1],[0,-1,1]] # powers used by controller on each ring resp.\n",
    "pattern_ntm_powers_2_on_1        = [0,2,4] # allowed powers used by ring 2 to manipulate ring 1\n",
    "pattern_ntm_memory_address_sizes = [128, 128] # number of memory locations for the three rings\n",
    "pattern_ntm_memory_content_sizes = [20, len(pattern_ntm_powers_2_on_1)] # size of content vector for each ring\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there\n",
    "# 2. The initial and terminal symbols are always from the end of the list of symbols, so they\n",
    "# are respectively num_classes - 2 and num_classes - 1. So the number of symbols which are\n",
    "# not initial or terminal is num_classes - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import pandas\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[0, 3, 5, 1, 1, 5, 1, 5, 3, 0, 4, 4, 2, 7, 7, 3, 6, 2, 1, 5, 3, 1, 5, 5, 6, 6, 1, 5]\n",
      "is mapped to\n",
      "[0, 3, 5, 1, 1, 5, 1, 5, 3, 0, 4, 4, 2, 7, 7, 3, 6, 2, 1, 5, 3, 1, 5, 5, 6, 6, 1, 5]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "# Below N_out and Ntest_out are the lengths of the outputs in both the training\n",
    "# and testing regimes respectively. Since outputs do not include the initial and terminal\n",
    "# symbols, these default to N - 2 and Ntest - 2 respectively.\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N - 2\n",
    "    Ntest_out = Ntest - 2\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "if( task == 'repeat copy' ):\n",
    "    no_of_copies = 2\n",
    "    pattern = [0]*(no_of_copies - 1) + [1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = no_of_copies * (N - 2)\n",
    "    Ntest_out = no_of_copies * (Ntest - 2)\n",
    "\n",
    "################\n",
    "# PATTERN TASK 1\n",
    "if( task == 'pattern 1' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,1,1] # so (a,b,c,d,e,f,...) goes to (a,a,b,c,c,d,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = (N - 2) + divmod(N - 2, 2)[0] # N - 2 plus the number of times 2 divides N - 2\n",
    "    Ntest_out = (Ntest - 2) + divmod(Ntest - 2, 2)[0]\n",
    "\n",
    "################\n",
    "# PATTERN TASK 2\n",
    "if( task == 'pattern 2' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2] # so (a,b,c,d,e,f,...) goes to (a,a,c,c,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = N - 2 + divmod(N - 2, 2)[0]\n",
    "    Ntest_out = Ntest - 2 + divmod(Ntest - 2, 2)[0]\n",
    "    \n",
    "################\n",
    "# PATTERN TASK 3\n",
    "if( task == 'pattern 3' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,b,b,d,c,c,e,d,d,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 4 + (N - 2 - 2) * 3\n",
    "    Ntest_out = 4 + (Ntest - 2 - 2) * 3\n",
    "\n",
    "################\n",
    "# PATTERN TASK 4\n",
    "if( task == 'pattern 4' ):\n",
    "    # WARNING: for this task make sure seq_lengh_min is at least 4\n",
    "    pattern = [0,2,1,2,-2,-1] # so (a,b,c,d,e,f,...) goes to (a,a,c,d,f,d,c,c,e,f,h,f,e,e,...)\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = len(func_to_learn([0]*(N-2)))\n",
    "    Ntest_out = len(func_to_learn([0]*(Ntest-2)))\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = [random.randint(0,num_classes-3) for i in range(N - 2)]\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# INITIALISE STATE #\n",
    "####################\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "input_size = num_classes # dimension of the input space I\n",
    "state_size = 0\n",
    "\n",
    "#####\n",
    "# NTM\n",
    "\n",
    "def init_state_ntm(batch_size, css, mas, mcs):\n",
    "    state_size = css + 2*mas + mas * mcs\n",
    "    \n",
    "    ra = [0.0]*mas\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,mas]) + ra\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    init_read_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_write_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    init_memory = tf.truncated_normal([batch_size, mas*mcs], 0.0, 1e-6, dtype=tf.float32)\n",
    "    \n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "#############\n",
    "# PATTERN NTM\n",
    "\n",
    "def init_state_pattern_ntm(batch_size, css, mas, mcs):\n",
    "    # mas and mcs are arrays of address sizes and content sizes for rings\n",
    "    state_size = css\n",
    "    \n",
    "    init_address = []\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        state_size = state_size + mas[i] * mcs[i] # for memory vector\n",
    "        state_size = state_size + 2 * mas[i] # for addresses (read and write)\n",
    "    \n",
    "        ra = [0.0]*mas[i]\n",
    "        ra[0] = 1.0\n",
    "        init_address.append(np.zeros([batch_size,mas[i]]) + ra)\n",
    "    \n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)    \n",
    "    \n",
    "    tensor_list = [init_controller_state]\n",
    "    \n",
    "    for i in range(len(mas)):\n",
    "        init_read_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        init_write_address = tf.constant(init_address[i],dtype=tf.float32,shape=[batch_size,mas[i]])\n",
    "        tensor_list = tensor_list + [init_read_address,init_write_address]\n",
    "        \n",
    "    for i in range(len(mas)):\n",
    "        init_memory = tf.truncated_normal([batch_size, mas[i]*mcs[i]], 0.0, 1e-6, dtype=tf.float32)\n",
    "        tensor_list = tensor_list + [init_memory]\n",
    "    \n",
    "    state = tf.concat(tensor_list,1)\n",
    "    \n",
    "    return state_size, state\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_114/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_112/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_110/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_108/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_106/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_104/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_102/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_100/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_98/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_96/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_94/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_92/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_90/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_88/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_86/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_84/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_82/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_80/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_78/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_76/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_74/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_72/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_70/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_68/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_66/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_64/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_62/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_60/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_58/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_56/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_54/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_52/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_50/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_48/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_46/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_44/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_42/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_40/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_38/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_36/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_34/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_32/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_30/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_28/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_26/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_24/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_22/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_20/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_18/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(250, 3556) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N + N_out)]\n",
    "\n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "write_addresses = []\n",
    "read_addresses2 = []\n",
    "write_addresses2 = []\n",
    "interps = []\n",
    "rnn_outputs = []\n",
    "    \n",
    "for i in range(N + N_out):\n",
    "    # Logging\n",
    "    if( use_model == 'ntm' ):\n",
    "        h0, curr_read, curr_write, _ = tf.split(state, [controller_state_size,ntm_memory_address_size,\n",
    "                                                        ntm_memory_address_size,-1], 1)\n",
    "\n",
    "    if( use_model == 'pattern_ntm' ):\n",
    "        h0, curr_read, curr_write, curr_read2, curr_write2, _ = tf.split(state, [controller_state_size,\n",
    "                            pattern_ntm_memory_address_sizes[0],pattern_ntm_memory_address_sizes[0],\n",
    "                            pattern_ntm_memory_address_sizes[1],pattern_ntm_memory_address_sizes[1],-1], 1)\n",
    "\n",
    "    #### RUN MODEL ####\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "    ###################\n",
    "    \n",
    "    # More logging\n",
    "    read_addresses.append(curr_read[0,:])\n",
    "    write_addresses.append(curr_write[0,:])\n",
    "    \n",
    "    if( use_model == 'pattern_ntm' ):\n",
    "        read_addresses2.append(curr_read2[0,:])\n",
    "        write_addresses2.append(curr_write2[0,:])\n",
    "\n",
    "        with tf.variable_scope(\"NTM\",reuse=True):\n",
    "            W_interp = tf.get_variable(\"W_interp\", [controller_state_size,1])\n",
    "            B_interp = tf.get_variable(\"B_interp\", [1])\n",
    "            interp = tf.sigmoid(tf.matmul(h0,W_interp) + B_interp)\n",
    "            interp_matrix = tf.concat([interp,tf.ones_like(interp,dtype=tf.float32) - interp],axis=1) # shape [-1,2]\n",
    "            interps.append(interp_matrix[0,:])\n",
    "\n",
    "    reuse = True\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# Note: prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "\n",
    "# Note: We allow the length of input sequences to vary between batches, which means\n",
    "# that the cross entropy needs to be masked to the relevant part of the output\n",
    "\n",
    "# Note: we use log_softmax to avoid precision issues with floats causing log(0) to create NaNs\n",
    "\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.log_softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * prediction[i]) for i in range(N + N_out)] # an array of numbers\n",
    "mask = [tf.sign(tf.reduce_max(tf.abs(targets[i]))) for i in range(N + N_out)]\n",
    "ce_mask = [ce[i] * mask[i] for i in range(N + N_out)]\n",
    "cross_entropy = -tf.add_n(ce_mask)\n",
    "cross_entropy /= tf.add_n(mask) # DEBUG do we really need this?\n",
    "# NOTE: here in creating the mask we are assuming that batches have the same sequence length\n",
    "                    \n",
    "optimizer = tf.train.RMSPropOptimizer(1e-4,decay=0.9,momentum=0.9)\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N + N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "errors_mask = [errors[i] * mask[i] for i in range(N + N_out)]\n",
    "mean_error = tf.add_n(errors_mask)\n",
    "mean_error /= tf.add_n(mask)\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 0 r-argmax [0] w-argmax [0] r2-argmax [0] w2-argmax [0] interp [ 0.11920285  0.88079715]\n",
      " Step 1 r-argmax [127] w-argmax [0] r2-argmax [0] w2-argmax [0] interp [ 0.10694468  0.89305532]\n",
      " Step 2 r-argmax [0] w-argmax [0] r2-argmax [0] w2-argmax [0] interp [ 0.14798725  0.85201275]\n",
      " Step 3 r-argmax [0] w-argmax [0] r2-argmax [0] w2-argmax [0] interp [ 0.23244441  0.76755559]\n",
      " Step 4 r-argmax [0] w-argmax [0] r2-argmax [0] w2-argmax [0] interp [ 0.13137609  0.86862391]\n",
      " Step 5 r-argmax [0] w-argmax [0] r2-argmax [0] w2-argmax [0] interp [ 0.09156442  0.90843558]\n",
      " Step 6 r-argmax [0] w-argmax [0] r2-argmax [0] w2-argmax [0] interp [ 0.12023532  0.87976468]\n",
      " Step 7 r-argmax [1] w-argmax [0] r2-argmax [0] w2-argmax [0] interp [ 0.10023803  0.89976197]\n",
      " Step 8 r-argmax [1] w-argmax [127] r2-argmax [0] w2-argmax [0] interp [ 0.08135569  0.91864431]\n",
      " Step 9 r-argmax [1] w-argmax [0] r2-argmax [0] w2-argmax [0] interp [ 0.07824403  0.92175597]\n",
      " Step 10 r-argmax [1] w-argmax [127] r2-argmax [0] w2-argmax [0] interp [ 0.11221969  0.88778031]\n",
      " Step 11 r-argmax [1] w-argmax [127] r2-argmax [0] w2-argmax [0] interp [ 0.2045632  0.7954368]\n",
      " Step 12 r-argmax [1] w-argmax [127] r2-argmax [0] w2-argmax [0] interp [ 0.12214237  0.87785763]\n",
      " Step 13 r-argmax [1] w-argmax [127] r2-argmax [0] w2-argmax [0] interp [ 0.12165487  0.87834513]\n",
      " Step 14 r-argmax [1] w-argmax [127] r2-argmax [0] w2-argmax [0] interp [ 0.12512231  0.87487769]\n",
      " Step 15 r-argmax [2] w-argmax [0] r2-argmax [0] w2-argmax [0] interp [ 0.14411348  0.85588652]\n",
      " Step 16 r-argmax [2] w-argmax [127] r2-argmax [0] w2-argmax [0] interp [ 0.08875471  0.91124529]\n",
      " Step 17 r-argmax [2] w-argmax [127] r2-argmax [0] w2-argmax [0] interp [ 0.05813569  0.94186431]\n",
      " Step 18 r-argmax [2] w-argmax [127] r2-argmax [127] w2-argmax [0] interp [ 0.11745173  0.88254827]\n",
      " Step 19 r-argmax [2] w-argmax [127] r2-argmax [127] w2-argmax [0] interp [ 0.13714272  0.86285728]\n",
      " Step 20 r-argmax [2] w-argmax [127] r2-argmax [127] w2-argmax [0] interp [ 0.15771735  0.84228265]\n",
      " Step 21 r-argmax [2] w-argmax [127] r2-argmax [127] w2-argmax [0] interp [ 0.15796119  0.84203881]\n",
      " Step 22 r-argmax [2] w-argmax [127] r2-argmax [127] w2-argmax [0] interp [ 0.12775046  0.87224954]\n",
      " Step 23 r-argmax [2] w-argmax [127] r2-argmax [0] w2-argmax [0] interp [ 0.1549992  0.8450008]\n",
      " Step 24 r-argmax [2] w-argmax [127] r2-argmax [0] w2-argmax [0] interp [ 0.12355745  0.87644255]\n",
      " Step 25 r-argmax [2] w-argmax [126] r2-argmax [0] w2-argmax [0] interp [ 0.11562389  0.88437611]\n",
      " Step 26 r-argmax [2] w-argmax [127] r2-argmax [0] w2-argmax [0] interp [ 0.14728755  0.85271245]\n",
      " Step 27 r-argmax [3] w-argmax [127] r2-argmax [0] w2-argmax [0] interp [ 0.12356704  0.87643296]\n",
      " Step 28 r-argmax [3] w-argmax [127] r2-argmax [0] w2-argmax [0] interp [ 0.13805193  0.86194807]\n",
      " Step 29 r-argmax [3] w-argmax [126] r2-argmax [1] w2-argmax [0] interp [ 0.14201635  0.85798365]\n",
      " Step 30 r-argmax [3] w-argmax [127] r2-argmax [1] w2-argmax [0] interp [ 0.15229422  0.84770578]\n",
      " Step 31 r-argmax [3] w-argmax [126] r2-argmax [1] w2-argmax [0] interp [ 0.16194212  0.83805788]\n",
      " Step 32 r-argmax [3] w-argmax [127] r2-argmax [1] w2-argmax [1] interp [ 0.17115915  0.82884085]\n",
      " Step 33 r-argmax [3] w-argmax [127] r2-argmax [1] w2-argmax [1] interp [ 0.15519637  0.84480363]\n",
      " Step 34 r-argmax [3] w-argmax [127] r2-argmax [1] w2-argmax [1] interp [ 0.15955514  0.84044486]\n",
      " Step 35 r-argmax [3] w-argmax [127] r2-argmax [2] w2-argmax [1] interp [ 0.15165788  0.84834212]\n",
      " Step 36 r-argmax [3] w-argmax [127] r2-argmax [2] w2-argmax [1] interp [ 0.15018147  0.84981853]\n",
      " Step 37 r-argmax [3] w-argmax [127] r2-argmax [2] w2-argmax [1] interp [ 0.15043312  0.84956688]\n",
      " Step 38 r-argmax [3] w-argmax [127] r2-argmax [2] w2-argmax [1] interp [ 0.15369952  0.84630048]\n",
      " Step 39 r-argmax [4] w-argmax [127] r2-argmax [3] w2-argmax [1] interp [ 0.16525608  0.83474392]\n",
      " Step 40 r-argmax [4] w-argmax [127] r2-argmax [3] w2-argmax [1] interp [ 0.16987574  0.83012426]\n",
      " Step 41 r-argmax [4] w-argmax [127] r2-argmax [3] w2-argmax [1] interp [ 0.16926652  0.83073348]\n",
      " Step 42 r-argmax [4] w-argmax [127] r2-argmax [3] w2-argmax [1] interp [ 0.16994774  0.83005226]\n",
      " Step 43 r-argmax [4] w-argmax [127] r2-argmax [3] w2-argmax [2] interp [ 0.17016041  0.82983959]\n",
      " Step 44 r-argmax [4] w-argmax [127] r2-argmax [4] w2-argmax [2] interp [ 0.16567159  0.83432841]\n",
      " Step 45 r-argmax [4] w-argmax [127] r2-argmax [4] w2-argmax [2] interp [ 0.16437131  0.83562869]\n",
      " Step 46 r-argmax [4] w-argmax [127] r2-argmax [4] w2-argmax [2] interp [ 0.16492158  0.83507842]\n",
      " Step 47 r-argmax [4] w-argmax [127] r2-argmax [4] w2-argmax [2] interp [ 0.16779798  0.83220202]\n",
      " Step 48 r-argmax [4] w-argmax [127] r2-argmax [5] w2-argmax [2] interp [ 0.16882575  0.83117425]\n",
      " Step 49 r-argmax [4] w-argmax [127] r2-argmax [5] w2-argmax [2] interp [ 0.16784108  0.83215892]\n",
      " Step 50 r-argmax [4] w-argmax [127] r2-argmax [5] w2-argmax [2] interp [ 0.1684922  0.8315078]\n",
      " Step 51 r-argmax [4] w-argmax [127] r2-argmax [5] w2-argmax [2] interp [ 0.16946858  0.83053142]\n",
      " Step 52 r-argmax [4] w-argmax [127] r2-argmax [5] w2-argmax [3] interp [ 0.16841847  0.83158153]\n",
      " Step 53 r-argmax [4] w-argmax [127] r2-argmax [6] w2-argmax [3] interp [ 0.16735888  0.83264112]\n",
      " Step 54 r-argmax [4] w-argmax [127] r2-argmax [6] w2-argmax [3] interp [ 0.16797048  0.83202952]\n",
      " Step 55 r-argmax [4] w-argmax [127] r2-argmax [6] w2-argmax [3] interp [ 0.1692071  0.8307929]\n",
      " Step 56 r-argmax [5] w-argmax [127] r2-argmax [6] w2-argmax [3] interp [ 0.16942585  0.83057415]\n",
      " Step 57 r-argmax [5] w-argmax [127] r2-argmax [6] w2-argmax [3] interp [ 0.16896421  0.83103579]\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "###################\n",
    "# Note on sequences\n",
    "#\n",
    "# Our sequences are of varying length, in the alphabet {0,...,num_classes - 3}.\n",
    "# Each input sequence begins with an initial symbol and ends with a terminal symbol\n",
    "# (the value of which are num_classes - 2 and num_classes - 1 by default). Output\n",
    "# sequences do not have either an initial nor a terminal symbol.\n",
    "#\n",
    "# Both input and output sequences are written on a \"tape\" of length N + N_out.\n",
    "# Input sequences are aligned at the BEGINNING of the tape, and all remaining space\n",
    "# is filled with terminal symbols. Output sequences are aligned at the END OF THE \n",
    "# MATCHING INPUT, with all remaining space filled with zero vectors.\n",
    "#\n",
    "# Example: suppose N = N_out = 10, and num_classes = 10 so that init_symbol = 8\n",
    "# and term_symbol = 9. Then a sequence of length 8 (seq_length = 10 below) is\n",
    "#\n",
    "# a = [4, 4, 5, 6, 3, 3, 6, 7]\n",
    "#\n",
    "# which written on the tape is\n",
    "#\n",
    "# [8, 4, 4, 5, 6, 3, 3, 6, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
    "#\n",
    "# If we are performing the copy task, so that the output sequence is also a, then\n",
    "# the output written on the tape will be (notice the alignment)\n",
    "#\n",
    "# [-, -, -, -, -, -, -, -, -, 4, 4, 5, 6, 3, 3, 6, 7, -, -, -]\n",
    "#\n",
    "# where - is a symbol whose encoding is the zero vector.\n",
    "\n",
    "error_means = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences. Each\n",
    "        # batch has a fixed length of the sequences. Recall that all input seqs\n",
    "        # have an initial and terminal symbol, so if seq_length = 10 then there\n",
    "        # are eight positions for the \"content\" symbols\n",
    "        seq_length = random.randint(seq_length_min,N)\n",
    "        \n",
    "        for z in range(batch_size):\n",
    "            a = [random.randint(0,num_classes-3) for k in range(seq_length-2)]\n",
    "            fa = func_to_learn(a)\n",
    "            a = [init_symbol] + a + [term_symbol] + [term_symbol for k in range(N+N_out-seq_length)]\n",
    "            a_onehot = [one_hots[e] for e in a]\n",
    "            fa_onehot = [[0.0]*num_classes for k in range(seq_length-1)] + \\\n",
    "                        [one_hots[e] for e in fa] + \\\n",
    "                        [[0.0]*num_classes for k in range(N+N_out-(seq_length-1)-len(fa))]\n",
    "            inp.append(np.array(a_onehot))\n",
    "            out.append(np.array(fa_onehot))        \n",
    "        \n",
    "        # An annoying thing here is that we cannot use a list as a key in a \n",
    "        # dictionary. The workaround we found on StackOverflow here:\n",
    "        # http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "        feed_dict = {}\n",
    "        for d in range(N + N_out):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(N + N_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "        \n",
    "        # for the first batch in an epoch, we have some logging\n",
    "        if( j == 0 and i % 25 == 0 ):\n",
    "            r1_val, w1_val = sess.run([read_addresses,write_addresses],feed_dict)\n",
    "            \n",
    "            if( use_model == 'pattern_ntm' ):\n",
    "                r2_val, w2_val = sess.run([read_addresses2,write_addresses2],feed_dict)\n",
    "                interps_val = sess.run(interps,feed_dict)\n",
    "                \n",
    "            s = 0\n",
    "            for r in range(len(w1_val)):\n",
    "                print_str = \" Step \" + str(s) + \" r-argmax [\" + str(r1_val[r].argmax()) + \"]\" + \\\n",
    "                            \" w-argmax [\" + str(w1_val[r].argmax()) + \"]\"\n",
    "                    \n",
    "                if( use_model == 'pattern_ntm' ):\n",
    "                    print_str = print_str + \" r2-argmax [\" + str(r2_val[r].argmax()) + \"]\" + \\\n",
    "                          \" w2-argmax [\" + str(w2_val[r].argmax()) + \"]\" + \\\n",
    "                          \" interp \" + str(interps_val[r])     \n",
    "                    \n",
    "                print(print_str)\n",
    "                \n",
    "                #if( r == len(write_addresses_val) - 1 ):\n",
    "                #    print(\"Write address -\")\n",
    "                #    print(write_addresses_val[r])               \n",
    "                                            \n",
    "                s = s + 1\n",
    "        \n",
    "        ##### Do gradient descent #####\n",
    "        #summary,mean_error_val,_ = sess.run([merged_summaries,mean_error,minimize], feed_dict)\n",
    "        mean_error_val,_ = sess.run([mean_error,minimize], feed_dict)\n",
    "        # DEBUG: does computing the mean error for every batch slow down training too much?\n",
    "        ########\n",
    "        \n",
    "        error_means.append(mean_error_val)\n",
    "        \n",
    "        # Write out TensorBoard logs\n",
    "        #file_writer.add_summary(summary)\n",
    "    \n",
    "    # Print the mean error of the final batch in the epoch\n",
    "    print(\"Epoch - \" + str(i+1) + \", length - \" + str(seq_length - 2) + \", error - \" + str(mean_error_val))\n",
    "    \n",
    "# Write out variables to disk\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took\", time.time() - pre_train_time, \"seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# VISUALISATIONS #\n",
    "##################\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 11.5, 12\n",
    "\n",
    "num_subplots = 2\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    num_subplots = 5\n",
    "\n",
    "plt.figure(1)\n",
    "ax1 = plt.subplot(num_subplots,1,1)\n",
    "ax2 = plt.subplot(num_subplots,1,2)\n",
    "\n",
    "ax1.imshow(np.stack(w1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax1.set_title('Write address')\n",
    "ax1.set_xlabel('position')\n",
    "ax1.set_ylabel('time')\n",
    "\n",
    "ax2.imshow(np.stack(r1_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "ax2.set_title('Read address')\n",
    "ax2.set_xlabel('position')\n",
    "ax2.set_ylabel('time')\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    ax3 = plt.subplot(num_subplots,1,3)\n",
    "    ax4 = plt.subplot(num_subplots,1,4)\n",
    "    ax5 = plt.subplot(num_subplots,1,5)\n",
    "\n",
    "    ax3.imshow(np.stack(w2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax3.set_title('Write address (ring 2)')\n",
    "    ax3.set_xlabel('position')\n",
    "    ax3.set_ylabel('time')\n",
    "\n",
    "    ax4.imshow(np.stack(r2_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax4.set_title('Read address (ring 2)')\n",
    "    ax4.set_xlabel('position')\n",
    "    ax4.set_ylabel('time')\n",
    "    \n",
    "    ax4.imshow(np.stack(interps_val), cmap='bone', interpolation='nearest', aspect='equal')\n",
    "    ax4.set_title('Interpolation')\n",
    "    ax4.set_xlabel('1 vs 2')\n",
    "    ax4.set_ylabel('time')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(2)\n",
    "ax3 = plt.subplot(1,1,1)\n",
    "sc = pandas.Series(error_means)\n",
    "ma = sc.rolling(window=500).mean()\n",
    "ax3.plot(sc.index, sc, color='lightgray')\n",
    "ax3.plot(ma.index, ma, color='red')\n",
    "ax3.set_yscale('log')\n",
    "ax3.set_xlim(sc.index.min(), sc.index.max())\n",
    "ax3.set_title('Error')\n",
    "ax3.set_xlabel('batch')\n",
    "ax3.set_ylabel('error')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "# Restore the weights from training\n",
    "sess = tf.Session()\n",
    "saver.restore(sess,save_path)\n",
    "\n",
    "inputs_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "targets_test = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest + Ntest_out)]\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size, ntm_memory_address_size, ntm_memory_content_size)\n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,ntm_memory_address_size,ntm_memory_content_size, ntm_powers)\n",
    "\n",
    "\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size, state = init_state_pattern_ntm(batch_size, controller_state_size, pattern_ntm_memory_address_sizes, \n",
    "                                               pattern_ntm_memory_content_sizes)\n",
    "    cell = ntm.PatternNTM(state_size, input_size, controller_state_size, pattern_ntm_memory_address_sizes,\n",
    "                          pattern_ntm_memory_content_sizes, pattern_ntm_powers, pattern_ntm_powers_2_on_1)\n",
    "\n",
    "# Set up test graph\n",
    "rnn_outputs_test = []\n",
    "reuse = True\n",
    "for i in range(Ntest + Ntest_out):\n",
    "    output, state = cell(inputs_test[i],state,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "\n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size])\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.log_softmax(logit) for logit in logits_test] \n",
    "mask = [tf.sign(tf.reduce_max(tf.abs(targets_test[i]))) for i in range(Ntest + Ntest_out)]\n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets_test[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest + Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "errors_test_mask = [errors_test[i] * mask[i] for i in range(Ntest + Ntest_out)]\n",
    "mean_error_test = tf.add_n(errors_test_mask)\n",
    "mean_error_test /= tf.add_n(mask)\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "\n",
    "seq_length = Ntest\n",
    "print(\"Testing on sequences of length \" + str(seq_length-2))\n",
    "print(\"\")\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    for z in range(batch_size):\n",
    "        a = [random.randint(0,num_classes-3) for k in range(seq_length-2)]        \n",
    "        fa = func_to_learn(a)\n",
    "        a = [init_symbol] + a + [term_symbol] + [term_symbol for k in range(Ntest+Ntest_out-seq_length)]            \n",
    "        a_onehot = [one_hots[e] for e in a]\n",
    "                            \n",
    "        fa_onehot = [[0.0]*num_classes for k in range(seq_length-1)] + \\\n",
    "                    [one_hots[e] for e in fa] + \\\n",
    "                    [[0.0]*num_classes for k in range(Ntest+Ntest_out-(seq_length-1)-len(fa))]\n",
    "        \n",
    "        inp.append(np.array(a_onehot))\n",
    "        out.append(np.array(fa_onehot))        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        in_node = inputs_test[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest + Ntest_out):\n",
    "        out_node = targets_test[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = sess.run(mean_error_test, feed_dict)\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"epochs        - \" + str(epoch))\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "#print(\"N_out         - \" + str(N_out))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "#print(\"Ntest_out     - \" + str(Ntest_out))\n",
    "#print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "#print(\"(css,mas,mcs) - (\" + str(controller_state_size) + \",\" + str(memory_address_size) + \",\" + str(memory_content_size) + \")\")\n",
    "#print(\"train percent - \" + str(training_percent))\n",
    "print(\"num_training  - \" + str(num_training)) #+ \"/\" + str((num_classes-2)**(N-2)))\n",
    "print(\"num_test      - \" + str(num_test)) #+ \"/\" + str((num_classes-2)**(N-2)))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error         - \" + str(final_error))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
