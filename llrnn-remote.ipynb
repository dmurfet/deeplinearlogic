{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Initial implementation of linear logic recurrent neural network\n",
    "#\n",
    "# The architecture is a modified RNN, see the paper \"Linear logic and recurrent neural networks\".\n",
    "# Our inputs are sequences of symbols taken from an alphabet of size num_classes. The length\n",
    "# of the sequences is N. Our outputs are also sequences of length N from the same alphabet.\n",
    "#\n",
    "# An input symbol is encoded as a one_hot vector, so our inputs are sequences of one-hot vectors\n",
    "# of size num_classes. Put another way, this is the dimension of the input space.\n",
    "#\n",
    "# The function to be learned is func_to_learn\n",
    "#\n",
    "# At present at each time step the RNN simply emits its hidden state. These outputs are\n",
    "# then passed through a fully-connected layer (one by one) to generate the output symbol\n",
    "# at each time step.\n",
    "\n",
    "# TODO:\n",
    "#\n",
    "# Dropout\n",
    "# Layer normalisation (see \"Recurrent neural networks in TensorFlow II\")\n",
    "# Hessian-free optimisation (see Sutskever paper)\n",
    "#\n",
    "# We currently only implement the Church numerals (i.e. Pi from the paper is empty)\n",
    "\n",
    "# QUESTION:\n",
    "#\n",
    "# How to deal with output sequences of a different length to the input? Probably should feed\n",
    "# the outputs at each time step into a second RNN\n",
    "\n",
    "# NOTES\n",
    "#\n",
    "# We focus on experiments involving fairly local transformations of the sequence. Our\n",
    "# aim is not to improve the ability of RNNs to capture long-range correlations (for this\n",
    "# one should use memory). Instead, our aim is to enable higher complexity local transformations.\n",
    "\n",
    "# GLOBAL FLAGS\n",
    "num_classes = 2\n",
    "batch_size = 1000\n",
    "state_size = 35 # dimension of the state space H\n",
    "operator_size = 5 # dimension of the auxiliary space K\n",
    "input_size = num_classes # dimension of the input space I\n",
    "N = 15 # length of sequences\n",
    "training_percent = 0.05 # precentage used for training\n",
    "epoch = 2\n",
    "use_linearlogic = 0 # if 0, we compute an ordinary Elman RNN\n",
    "\n",
    "# PROGRAM LIBRARY\n",
    "Lambda = [1,2] # which Church numerals to couple into the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weights: 1330\n"
     ]
    }
   ],
   "source": [
    "total_arg_size = input_size + state_size\n",
    "num_weights = total_arg_size*state_size + state_size # H,U,B\n",
    "\n",
    "if use_linearlogic:\n",
    "    num_weights += 2*operator_size*state_size # P,Q\n",
    "    num_weights += 2*operator_size + input_size*operator_size # C,D,V\n",
    "    num_weights += 2*operator_size*state_size # I,J\n",
    "            \n",
    "# This weight count does not include the weights in the output y, since they\n",
    "# are in common between all our RNN models\n",
    "print(\"Number of weights: \" + str(num_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "\n",
    "from random import shuffle\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops.rnn_cell import RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def church(n,T):\n",
    "    if n == 1:\n",
    "        return(T)\n",
    "    if n == 2:\n",
    "        return(tf.mul(T,T))\n",
    "    if n == 3:\n",
    "        return(tf.mul(T,tf.mul(T,T)))\n",
    "    \n",
    "    # TODO higher powers\n",
    "    return(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In tf.nn.rnn_cell there is a private function _linear that \n",
    "# we have modified here to be used in our LinearLogicRNNCell\n",
    "\n",
    "# A 2D tensor of shape [X,Y] means a matrix with X rows and Y cols\n",
    "# The row index here is interpreted as indexing into a batch.\n",
    "    \n",
    "class LinearLogicRNNCell(RNNCell):\n",
    "    \n",
    "    def __init__(self, num_units, input_size=None, activation=tanh):\n",
    "        if input_size is not None:\n",
    "            logging.warn(\"%s: The input_size parameter is deprecated.\" % self)\n",
    "        self._num_units = num_units\n",
    "        self._activation = activation\n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        # the scope business gives a namespace to our weight variable matrix names\n",
    "        with vs.variable_scope(scope or \"Linear\"): \n",
    "            # inputs has shape [batch_size, input_size]\n",
    "            # state has shape [batch_size, state_size]\n",
    "            args = [inputs,state]\n",
    "            total_arg_size = input_size + state_size\n",
    "    \n",
    "            # NOTE: the H and U that are referred to in the RNN update equation of\n",
    "            # the paper are the block parts of the following matrix HU.\n",
    "            # Also, everything is transposed here compared to the paper, since we\n",
    "            # need rows to correspond to entries in the batch in TensorFlow. So for\n",
    "            # example we actually compute hH + xU + B.\n",
    "            \n",
    "            HU = vs.get_variable(\"HU\", [total_arg_size, state_size])\n",
    "            B = vs.get_variable(\"B\", [state_size], initializer=init_ops.constant_initializer(0.0))\n",
    "            \n",
    "            # array_ops.concat(1,args) has shape [batch_size, input_size + state_size]\n",
    "            # multiplying this with the block matrix HU = (H|U) computes hH + xU \n",
    "            res = tf.matmul(array_ops.concat(1, args), HU)\n",
    "            \n",
    "            if use_linearlogic and len(Lambda) > 0:\n",
    "                # Lambda is defined globally, and is the list of Church numerals\n",
    "                # The notation here matches Definition 2.9 of the paper\n",
    "                I = vs.get_variable(\"I\", [operator_size,state_size])\n",
    "                J = vs.get_variable(\"J\", [state_size,operator_size])\n",
    "                V = vs.get_variable(\"V\", [input_size,operator_size])\n",
    "                \n",
    "                Ps = [vs.get_variable(\"P\" + str(n), [state_size,operator_size]) for n in Lambda]\n",
    "                Cs = [vs.get_variable(\"C\" + str(n), [operator_size]) for n in Lambda]\n",
    "                ps = [self._activation(tf.matmul(state,P) + C) for P,C in zip(Ps,Cs)] # entry shape [batch_size,operator_size]                    \n",
    "                make_diag = lambda A: tf.diag(A)\n",
    "                ps_diag = [tf.map_fn(make_diag,p) for p in ps] # entry shape [batch_size,operator_size,operator_size]\n",
    "                \n",
    "                # Compute the Church numerals\n",
    "                Vx = tf.matmul(inputs,V) # shape [batch_size,operator_size]\n",
    "                church_outputs = [church(n,Vx) for n in Lambda]                \n",
    "                Vx_diag = [tf.map_fn(make_diag,c) for c in church_outputs] # entry shape [batch_size,operator_size,operator_size]\n",
    "                Jh = tf.matmul(state,J) # shape [batch_size,operator_size]\n",
    "                Jh = tf.expand_dims(Jh,1) # shape [batch_size,1,operator_size]\n",
    "                VxJh = [tf.batch_matmul(Jh,c) for c in Vx_diag] # entry shape [batch_size,1,operator_size]\n",
    "                pVxJh = [tf.batch_matmul(c,p) for c,p in zip(VxJh,ps_diag)] # entry shape [batch_size,1,operator_size]\n",
    "                pVxJh = [tf.reshape(c,[-1,operator_size]) for c in pVxJh] # entry shape [batch_size,operator_size]\n",
    "                IpVxJh = [tf.matmul(c,I) for c in pVxJh] # entry shape [batch_size,state_size]\n",
    "                church_sum = tf.add_n(IpVxJh) # shape [batch_size,state_size]\n",
    "                \n",
    "                output = self._activation(res + church_sum + B)\n",
    "            else:\n",
    "                output = self._activation(res + B)\n",
    "                \n",
    "        return output, output\n",
    "        # note that as currently written the RNN emits its internal state at each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The function from sequences to sequences that we will try to learn\n",
    "\n",
    "def f_ident(seq):\n",
    "    return seq\n",
    "\n",
    "def f_reverse(seq):\n",
    "    t = [0]*len(seq)\n",
    "    for j in range(len(seq)):\n",
    "        t[len(seq)-j-1] = seq[j]\n",
    "    return t\n",
    "\n",
    "def f_swap01(seq):\n",
    "    t = []\n",
    "    for j in range(len(seq)):\n",
    "        if seq[j] == 0:\n",
    "            t.append(0)\n",
    "        else:\n",
    "            t.append(1)\n",
    "    return t\n",
    "\n",
    "def f1(seq):\n",
    "    t = []\n",
    "    for j in range(len(seq)):\n",
    "        if seq[j] == 1 and j > 0 and seq[j-1] == 1:\n",
    "            if j > 3:\n",
    "                t.append(seq[j-2])\n",
    "            else:\n",
    "                t.append(0)\n",
    "        else:\n",
    "            t.append(1)\n",
    "    return t\n",
    "\n",
    "def f_skiprepeat(seq):\n",
    "    t = []\n",
    "    for j in range(len(seq)):\n",
    "        if j % 2 == 0:\n",
    "            t.append(seq[j])\n",
    "        else:\n",
    "            t.append(seq[j-1])\n",
    "    return t\n",
    "\n",
    "def f_zeromeansrepeat(seq):\n",
    "    t = []\n",
    "    for j in range(len(seq)):\n",
    "        if j > 0 and seq[j-1] == 0:\n",
    "            t.append(1)\n",
    "        else:\n",
    "            t.append(seq[j])\n",
    "    return t\n",
    "\n",
    "func_to_learn = f_zeromeansrepeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 32768\n",
      "[1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0]\n",
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Create a shuffled list of all binary sequences of length N\n",
    "s = '{0:0' + str(N) + 'b}'\n",
    "seq_input = [s.format(i) for i in range(2**N)]\n",
    "shuffle(seq_input)\n",
    "seq_input = [map(int,i) for i in seq_input]\n",
    "ti = []\n",
    "for i in seq_input:\n",
    "    temp_list = []\n",
    "    for j in i:\n",
    "        temp_list.append(j)\n",
    "    ti.append(temp_list)\n",
    "seq_input = ti\n",
    "\n",
    "print(\"Number of sequences: \" + str(len(seq_input)))\n",
    "print(seq_input[0])\n",
    "# A typical element of seq_input at this point will be an array like\n",
    "# array([[1],[0],[1],[1],[0]])\n",
    "\n",
    "one_hots = []\n",
    "for i in range(num_classes):\n",
    "    a = [0.0]*num_classes\n",
    "    a[i] = 1.0\n",
    "    one_hots.append(np.array(a))\n",
    "\n",
    "seq_input_onehot = []\n",
    "for i in seq_input:\n",
    "    temp_list = []\n",
    "    for j in i:\n",
    "        temp_list.append(one_hots[j])\n",
    "    seq_input_onehot.append(np.array(temp_list))\n",
    "\n",
    "print(seq_input_onehot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0]\n",
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Training output\n",
    "seq_output = []\n",
    "\n",
    "# Swaps 0s for 1s in each sequence\n",
    "for i in seq_input:\n",
    "    seq_output.append(func_to_learn(i))\n",
    "                    \n",
    "print(seq_output[0])\n",
    "\n",
    "seq_output_onehot = []\n",
    "for i in seq_output:\n",
    "    temp_list = []\n",
    "    for j in i:\n",
    "        temp_list.append(one_hots[j])\n",
    "    seq_output_onehot.append(np.array(temp_list))\n",
    "\n",
    "print(seq_output_onehot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1638/32768\n",
      "\n",
      "The first one-hot encoded digit of the first three output sequences\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = int(training_percent * len(seq_input))\n",
    "print(\"Number of training examples: \" + str(NUM_EXAMPLES) + \"/\" + str(len(seq_input)))\n",
    "\n",
    "test_input = seq_input_onehot[NUM_EXAMPLES:3*NUM_EXAMPLES]\n",
    "test_output = seq_output_onehot[NUM_EXAMPLES:3*NUM_EXAMPLES]\n",
    "train_input = seq_input_onehot[:NUM_EXAMPLES]\n",
    "train_output = seq_output_onehot[:NUM_EXAMPLES]\n",
    "\n",
    "print(\"\")\n",
    "print(\"The first one-hot encoded digit of the first three output sequences\")\n",
    "print(test_output[0][0])\n",
    "print(test_output[1][0])\n",
    "print(test_output[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definition of the model\n",
    "\n",
    "# inputs, we create N of them, each of shape [None,input_size], one for\n",
    "# each position in the sequence\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(N)]\n",
    "\n",
    "# We use tf.nn.rnn rather than dynamic_rnn because there appears to\n",
    "# be a problem with tf.map_fn and the latter, at least in 0.10\n",
    "# state_size is the number of hidden neurons in each layer\n",
    "cell = LinearLogicRNNCell(state_size)\n",
    "\n",
    "# tf.nn.rnn returns a pair, the first is a list of the\n",
    "# outputs from each step, the second is the final internal state.\n",
    "rnn_outputs, last_state = tf.nn.rnn(cell,inputs,dtype=tf.float32)\n",
    "\n",
    "# Final fully connected layer\n",
    "E = tf.Variable(tf.truncated_normal([state_size,input_size]))\n",
    "F = tf.Variable(tf.constant(0.1, shape=[input_size]))\n",
    "\n",
    "# prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "prediction = [tf.nn.softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * tf.log(prediction[i])) for i in range(N)]\n",
    "\n",
    "cross_entropy = -tf.add_n(ce)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise the model\n",
    "init_op = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ..., 0 1 1]\n",
      "[0 0 0 ..., 1 0 0]\n",
      "[ True  True  True ...,  True  True  True]\n",
      "\n",
      "The mean of the errors in each digit for the test set:\n",
      "[1.0, 0.25518927, 0.24725275, 0.50091577, 0.45665446, 0.38583639, 0.3907204, 0.52014655, 0.46214896, 0.47466421, 0.4661172, 0.47893772, 0.48412699, 0.47771671, 0.47466421]\n",
      "Mean: 0.471673\n"
     ]
    }
   ],
   "source": [
    "# Display the errors before training\n",
    "feed_dict = {}\n",
    "for d in range(N):\n",
    "    in_node = inputs[d]\n",
    "    out_node = targets[d]\n",
    "    \n",
    "    ti = []\n",
    "    to = []\n",
    "    for k in range(len(test_input)):\n",
    "        ti.append(test_input[k][d]) # A vector giving the one-hot encoding of the dth symbol in the kth sequence\n",
    "        to.append(test_output[k][d])\n",
    "    feed_dict[in_node] = np.array(ti)\n",
    "    feed_dict[out_node] = np.array(to)\n",
    "\n",
    "# The first three digits of this should match the printout for the\n",
    "# first three test output sequences given earlier\n",
    "print(sess.run(tf.argmax(targets[0],1),feed_dict))\n",
    "print(sess.run(tf.argmax(prediction[0],1),feed_dict))\n",
    "print(sess.run(tf.not_equal(tf.argmax(targets[0], 1), tf.argmax(prediction[0], 1)),feed_dict))\n",
    "\n",
    "print(\"\")\n",
    "print(\"The mean of the errors in each digit for the test set:\")\n",
    "incorrects = sess.run(errors, feed_dict)\n",
    "print(incorrects)\n",
    "print(\"Mean: \" + str(np.mean(incorrects)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 1\n",
      "Epoch - 1\n",
      "Epoch - 2\n",
      "It took 0.243247032166 seconds to train.\n"
     ]
    }
   ],
   "source": [
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(len(train_input)/batch_size)\n",
    "print(\"Number of batches: \" + str(no_of_batches))\n",
    "\n",
    "# An annoying thing here is that we cannot use a list as a key in a \n",
    "# dictionary. The workaround we found on StackOverflow here:\n",
    "# http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "\n",
    "# epoch is a global var\n",
    "for i in range(epoch):\n",
    "    ptr = 0\n",
    "    for j in range(no_of_batches):\n",
    "        inp = train_input[ptr:ptr+batch_size]\n",
    "        out = train_output[ptr:ptr+batch_size]\n",
    "        ptr += batch_size\n",
    "        \n",
    "        feed_dict = {}\n",
    "        for d in range(N):\n",
    "            in_node = inputs[d]\n",
    "            out_node = targets[d]\n",
    "            \n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "                to.append(out[k][d])\n",
    "\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "        sess.run(minimize, feed_dict)\n",
    "    print(\"Epoch - \" + str(i+1))\n",
    "    \n",
    "print(\"It took\", time.time() - pre_train_time, \"seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First digits of test outputs (actual)\n",
      "[1 1 1 ..., 0 1 1]\n",
      "First digits of test outputs (predicted)\n",
      "[0 0 0 ..., 1 0 0]\n",
      "Cross-entropy: 42690.9\n",
      "[1.0, 0.25518927, 0.24725275, 0.30525032, 0.34157509, 0.23962149, 0.32631257, 0.41758242, 0.36416361, 0.36691087, 0.37210011, 0.38766789, 0.38705739, 0.38644689, 0.40018314]\n",
      "Mean: 0.386488\n"
     ]
    }
   ],
   "source": [
    "# Display the errors after training\n",
    "feed_dict = {}\n",
    "for d in range(N):\n",
    "    in_node = inputs[d]\n",
    "    out_node = targets[d]\n",
    "    \n",
    "    ti = []\n",
    "    to = []\n",
    "    for k in range(len(test_input)):\n",
    "        ti.append(test_input[k][d]) # A vector giving the one-hot encoding of the dth symbol in the kth sequence\n",
    "        to.append(test_output[k][d])\n",
    "    feed_dict[in_node] = np.array(ti)\n",
    "    feed_dict[out_node] = np.array(to)\n",
    "\n",
    "# The first three digits of this should match the printout for the\n",
    "# first three test output sequences given earlier\n",
    "data = sess.run([tf.argmax(targets[0],1),\n",
    "                 tf.argmax(prediction[0],1),\n",
    "                 cross_entropy],feed_dict)\n",
    "\n",
    "print(\"First digits of test outputs (actual)\")\n",
    "print(data[0])\n",
    "print(\"First digits of test outputs (predicted)\")\n",
    "print(data[1])\n",
    "print(\"Cross-entropy: \" + str(data[2]))\n",
    "\n",
    "# print the mean of the errors in each digit for the test set.\n",
    "incorrects = sess.run(errors, feed_dict)\n",
    "print(incorrects)\n",
    "print(\"Mean: \" + str(np.mean(incorrects)))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# N = 15, church = 1, 198 seconds to train\n",
    "# N = 15, church = 2, 205 seconds\n",
    "\n",
    "# for skiprepeat N = 15 without linearlogic, \n",
    "# [0.0, 0.2530525, 0.3768315, 0.18315019, 0.24832112, 0.25457877, 0.19948107, 0.25457877, 0.25198412, 0.25839439, 0.2269536, 0.26862028, 0.23076923, 0.26678878, 0.22710623]\n",
    "# with linear logic (church = 2), all zeros, training time 205\n",
    "\n",
    "# for skiprepeat N = 20 without linearlogic,\n",
    "\n",
    "# N = 15, training_percent = 0.1, epoch = 10, use_linearlogic = 1, Lambda = [1,2]\n",
    "# 1187 seconds to train\n",
    "# [0.0, 0.0, 0.498779, 0.061660562, 0.37057388, 0.12713675, 0.33653846, 0.17231379, 0.30525032, 0.17185593, 0.31913918, 0.17918193, 0.32341269, 0.1802503, 0.32066545]\n",
    "# mean 0.2244\n",
    "\n",
    "# N = 15, training_percent = 0.1, epoch = 10, use_linearlogic = 0, Lambda = [1,2]\n",
    "# 1 second to train\n",
    "# mean 0.233384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1220 weights hidden = 25 (epoch = 2, mean = 0.315608)\n",
    "# 1330 weights hidden = 35 (epoch = 2, (no LL) mean = 0.386488)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
