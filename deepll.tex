\documentclass[english,letter paper,12pt,leqno]{article}
\usepackage{array}
\usepackage{stmaryrd}
\usepackage{bussproofs}
\usepackage{amsmath, amscd, amssymb, mathrsfs, accents, amsfonts,amsthm}
\usepackage[all]{xy}
\usepackage{dsfont}
\usepackage{tikz}
\def\nicedashedcolourscheme{\shadedraw[top color=blue!22, bottom color=blue!22, draw=gray, dashed]}
\def\nicecolourscheme{\shadedraw[top color=blue!22, bottom color=blue!22, draw=white]}
\def\nicepalecolourscheme{\shadedraw[top color=blue!12, bottom color=blue!12, draw=white]}
\def\nicenocolourscheme{\shadedraw[top color=gray!2, bottom color=gray!25, draw=white]}
\def\nicereallynocolourscheme{\shadedraw[top color=white!2, bottom color=white!25, draw=white]}
\definecolor{Myblue}{rgb}{0,0,0.6}
\usepackage[a4paper,colorlinks,citecolor=Myblue,linkcolor=Myblue,urlcolor=Myblue,pdfpagemode=None]{hyperref}

\SelectTips{cm}{}

\setlength{\evensidemargin}{0.1in}
\setlength{\oddsidemargin}{0.1in}
\setlength{\textwidth}{6.3in}
\setlength{\topmargin}{0.0in}
\setlength{\textheight}{8.5in}
\setlength{\headheight}{0in}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{setup}[theorem]{Setup}

% Labels in tabular
\newcommand{\tagarray}{\mbox{}\refstepcounter{equation}$(\theequation)$}

\newtheoremstyle{example}{\topsep}{\topsep}
	{}
	{}
	{\bfseries}
	{.}
	{2pt}
	{\thmname{#1}\thmnumber{ #2}\thmnote{ #3}}
	
	\theoremstyle{example}
	\newtheorem{definition}[theorem]{Definition}
	\newtheorem{example}[theorem]{Example}
	\newtheorem{remark}[theorem]{Remark}
	\newtheorem{strat}[theorem]{Strategy}

\numberwithin{equation}{section}

% Operators
\def\eval{\operatorname{ev}}
\def\res{\operatorname{Res}}
\def\Coker{\operatorname{Coker}}
\def\Ker{\operatorname{Ker}}
\def\im{\operatorname{Im}}
\def\can{\operatorname{can}}
\def\K{\mathbf{K}}
\def\D{\mathbf{D}}
\def\N{\mathbf{N}}
\def\LG{\mathcal{LG}}
\def\Ab{\operatorname{Ab}}
\def\stab{\operatorname{stab}}
\def\Hom{\operatorname{Hom}}
\def\Func{\operatorname{Func}}
\def\modd{\operatorname{mod}}
\def\Modd{\operatorname{Mod}}
\def\be{\begin{equation}}
\def\ee{\end{equation}}
\def\nN{\mathds{N}}
\def\nZ{\mathds{Z}}
\def\nQ{\mathds{Q}}
\def\nR{\mathds{R}}
\def\nC{\mathds{C}}
\DeclareMathOperator{\Ext}{Ext}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\tot}{Tot}
\DeclareMathOperator{\ch}{ch}
\DeclareMathOperator{\str}{str}
\DeclareMathOperator{\hmf}{hmf}
\DeclareMathOperator{\HMF}{HMF}
\DeclareMathOperator{\hf}{HF}
\DeclareMathOperator{\At}{At}
\DeclareMathOperator{\Cat}{Cat}
\DeclareMathOperator{\Spec}{Spec}
\DeclareMathOperator{\id}{id}

\begin{document}

% Commands
\def\Res{\res\!}
\newcommand{\ud}{\mathrm{d}}
\newcommand{\Ress}[1]{\res_{#1}\!}
\newcommand{\cat}[1]{\mathcal{#1}}
\newcommand{\lto}{\longrightarrow}
\newcommand{\xlto}[1]{\stackrel{#1}\lto}
\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\md}[1]{\mathscr{#1}}
\def\sus{\l}
\def\l{\,|\,}
\def\sgn{\textup{sgn}}
\newcommand{\den}[1]{\llbracket #1 \rrbracket}

\title{Linear logic and recurrent neural networks}
\author{Huiyi Hu, Daniel Murfet}

\maketitle

\section{Introduction}

An old problem in artificial intelligence is \emph{program induction}: given a set of input-output pairs, the problem is to produce a program which generalises the given pairs. There is a large literature on this problem \cite{??,??} but it has received renewed attention in the past few years due to the advances in deep learning \cite{??,??,??}. The general theme of much of this recent work has been to use an RNN or LSTM controllers coupled to additional computational elements, such as memory or a library of special functions.

Our approach is to augment an RNN controller with a space of functional programs based on the polynomial semantics of intuitionistic linear logic \cite{??}. It is clear in principle that linear logic is well-suited for coupling to neural networks, and the aim of this paper is to demonstrate that in the simplest nontrivial case this coupling is actually of some practical value. Conceptually, one of the fundamental contributions of linear logic is that it demonstrates how to represent a general class of programs in terms of matrices, and multiplication of matrices.

% Arguably this problem is hard because learning methods are generally continuous, whereas programs (realised as say Turing machines, or $\lambda$-terms, or C programs, etc) are basically discrete. In this paper we initiate a novel two-step approach to the problem. In this first step, we couple a standard recurrent neural network to ``differentiable programmatic components'' which are vectors in a vector space which is the denotation of a type in linear logic. Once the optimisation algorithm has found a good vector representative of programs in this space, we apply the second step, which uses proof search to find a proof in linear logic whose denotation is observationally equivalent to the vector program.

\section{Architecture}

The architecture is a modified RNN. As usual we use ``weight'' as a synonym for variable, or more precisely, the variables which we will vary during gradient descent. At time $t \ge 1$ we denote the hidden state by $h^{(t)}$ and the input by $x^{(t)}$. We denote by $\sigma$ the function
\begin{gather*}
\sigma: \mathbb{R}^k \lto \mathbb{R}^k\,\\
\sigma( X )_i = \frac{1}{2}( X_i + |X_i| )\,.
\end{gather*}
An RNN is defined by its \emph{evolution equation} which expresses $h^{(t+1)}$ as a function of $h^{(t)}, x^{(t)}$ and its \emph{output equation} which gives the output $y^{(t)}$ as a function of $h^{(t)}$.

\begin{definition} A standard Elman-style RNN \cite{elman} is defined by weight matrices $H, U, B$ and the evolution equation
\be\label{eq:update_eqn}
h^{(t+1)} = \sigma\big( H h^{(t)} + U x^{(t+1)} + B\big)
\ee
where $h^{(0)}$ is some specified initial state. The outputs are defined by
\be\label{eq:output_rnn}
y^{(t)} = \sigma\big( L h^{(t)} + M \big)\,.
\ee
\end{definition}
% To motivate the modification, we recall that an RNN is analogous to a \emph{controller} or ``neural CPU'' with internal state $h^{(t)}$ (see Appendix \ref{section:appendix_cpu}). The internal state of the RNN is analogous to the content of the registers of a CPU.
% The basic operation in linear logic is composition (this is essentially the only way to nontrivially combine separate inputs) which becomes matrix multiplication in the semantics (it is not a coincidence we always end up with ${!}( \alpha \multimap \alpha )$'s as our inputs rather than ${!} \alpha$'s, since the latter can only have trivial proofs).

Our augmented RNN adds a new term to the evolution equation, in the style of recent multiplicative RNNs \cite{yuhai,sutskever,irsoy}. The key intuition is that the evolution equation maps inputs to transformations of the internal state. To explain, suppose the input vector space is $\mathscr{I}$ and the hidden state space is $\mathscr{H}$, so that $x^{(t)} \in \mathscr{I}$ and $h^{(t)} \in \mathscr{H}$ for all $t$. If our sequences are of a fixed length $T$ then the RNN computes a function of input sequences
\[
\bold{x} = \big( x^{(1)}, \ldots, x^{(T)} \big) \in \mathscr{I}^{\oplus T}\,.
\]
The value of the RNN on the input $\bold{x}$ is computed by applying \eqref{eq:update_eqn} for $0 \le t \le T - 1$, accumulating the values $\bold{y} = (y^{(1)}, \ldots, y^{(T)})$ from each time step, and finally applying a fully-connected layer and the softmax function $\tau$ obtain the output
\[
o^{(t)} = \tau( E y^{(t)} + F )\,.
\]
This is a sequence of vectors $\bold{o} = (o^{(1)},\ldots,o^{(T)}) \in \mathscr{I}^{\oplus T}$ where each $o^{(t)}$ has the property that its components in our chosen basis add to $1$. We view such a vector as a probability distribution over the basis, sampling from which gives the output of the RNN on $\bold{x}$.
\\

Note that the evolution equation implicitly determines a piecewise-linear function
\begin{gather*}
\mathscr{I} \lto \operatorname{Func}(\mathscr{H}, \mathscr{H} )\,,\\
x \mapsto \sigma\big( H (-) + U x + B\big)\,,
\end{gather*}
which specifies how we may view inputs as transforming the internal state of the RNN. One class of modifications of the RNN architecture adds more complicated (for example, piecewise-polynomial of higher degree) transformations of the internal state.

For example, compositional matrix-space models in NLP \cite{irsoy} add a new term to \eqref{eq:update_eqn} built from an auxiliary learned linear map from the input space $\mathscr{I}$ into the space of linear operators on the state space $\End_{\mathbb{R}}(\mathscr{H})$. Our approach is similar, but we use a particular class of \emph{non-linear} maps obtained as the denotations of proofs in linear logic. In principle proofs of any type could be coupled with an RNN controller, but here we consider only proofs of the following two types:
\begin{gather*}
\textbf{int}_\alpha = {!}(\alpha \multimap \alpha) \multimap (\alpha \multimap \alpha)\,,\\
\textbf{bint}_\alpha = {!}(\alpha \multimap \alpha) \multimap \big({!}(\alpha \multimap \alpha) \multimap (\alpha \multimap \alpha)\big)\,.
\end{gather*}
Under the Curry-Howard correspondence a proof $\pi$ of type $\textbf{bint}_\alpha$ can be thought of as a functional program taking two inputs of type $\alpha \multimap \alpha$ and giving an output of the same type. Conceptually proofs of type $\textbf{int}_\alpha$ and $\textbf{bint}_\alpha$ encode certain kinds of iteration. Rather than attempt to give a review of linear logic here, we give some examples.

\begin{example}\label{example_1} The \emph{denotation} of a proof $\pi$ of type $\textbf{int}_\alpha$ is a function
\[
\den{\pi}: \End_{\mathbb{R}}(\mathscr{H}) \lto \End_{\mathbb{R}}(\mathscr{H})
\]
which is a polynomial function of the entries of the input matrix. That is, writing $\mathscr{H} = \mathbb{R}^{n_H}$ and identifying $\End_{\mathbb{R}}(\mathscr{H})$ with $M_{n_H}(\mathbb{R})$ we have that $\den{\pi}(X)_{ij}: M_{n_H}(\mathbb{R}) \lto \mathbb{R}$ is a polynomial in the entries $X_{kl}$ for $1 \le i, j \le n_H$. For every integer $n \ge 0$ there is a proof $\underline{n}$ of type $\textbf{int}_\alpha$ called the \emph{Church numeral}, whose denotation is the function $\den{\underline{n}}(X) = X^n$.
\end{example}

\begin{example}\label{example_2} The denotation of a proof $\pi$ of type $\textbf{bint}_\alpha$ is a polynomial function
\[
\den{\pi}: \End_{\mathbb{R}}(\mathscr{H}) \times \End_{\mathbb{R}}(\mathscr{H}) \lto \End_{\mathbb{R}}(\mathscr{H})\,.
\]
For every binary sequence $S \in \{0,1\}^*$ there is a corresponding proof $\underline{S}$ of type $\textbf{bint}_\alpha$, where $\den{\underline{S}}$ sends a pair of matrices $X,Y$ to the product described by $S$, reading $X$ for $0$ and $Y$ for $1$. For example,
\[
\den{\underline{01101}}(X,Y) = X Y Y X Y\,.
\]
\end{example}

An important insight, made especially clear in linear logic, is that the complexity of an algorithm computing a function on binary sequences $\{0,1\}^T \lto \{0,1\}^T$ has to do with the ``level of iteration'' that is used in the algorithm. The most elementary forms of iteration are simply repetitions, as in the proofs of type $\textbf{int}_\alpha, \textbf{bint}_\alpha$ above, but there are much more powerful forms of iteration which lead to more complex algorithms.

The intuition guiding this paper is that an RNN controller will have an easier time approximating functions of a certain complexity, if it has access to iterators (from linear logic) of a similar level of complexity. At the lowest level these iterators take the form of polynomial operations on matrices, since we are taking our cue from compositional matrix-space models by mapping input symbols to matrices acting on the hidden state. The abstract theory suggests how to couple more sophisticated iterators to an RNN, but our focus in this paper is on proving the simplest version of the concept.
\\

Let $\Lambda = (\lambda_1,\ldots,\lambda_l)$ be a sequence of proofs of $\textbf{int}_\alpha$ and let $\Pi = (\pi_1,\ldots,\pi_k)$ be proofs of $\textbf{bint}_\alpha$. When we say that we allow the RNN controller to call this library of functional programs, what we actually mean is that it can predict a linear combination $p^{(t)}$ of the polynomial programs $\den{\lambda_i}$ to be applied to its internal state, and similarly for the $\pi_i$.

\begin{definition} The \emph{linear logic} RNN coupled to $\Lambda, \Pi$ has weight matrices $H,U,B$ as in the standard RNN, plus new weights $V, P, Q, C, D$ and an evolution equation:
\begin{gather*}
p^{(t+1)} = \sigma\big( P h^{(t)} + C \big)\,,\\
q^{(t+1)} = \sigma\big( Q h^{(t)} + D \big)\,,\\
h^{(t+1)} = \sigma\Big( Z + H h^{(t)} + U x^{(t+1)} + B\Big)
\end{gather*}
where
\begin{align*}
Z &= \sum_{i=1}^l p^{(t+1)}_i \den{\lambda_i}( Vx^{(t+1)} )\big( h^{(t)} \big) + \sum_{i=1}^k q^{(t+1)}_i \den{\pi_i}( Vx^{(t)}, Vx^{(t+1)})\big( h^{(t)} \big)\,.
\end{align*}
The weight matrix $V$ is interpreted as a linear map $\mathscr{I} \lto \End_{\mathbb{R}}(\mathscr{H})$ so that $Vx^{(t)}$ denotes a matrix of size $n_H \times n_H$ if $\mathscr{H} = \mathbb{R}^{n_h}$. The outputs $y^{(t)}$ of the linear logic RNN are defined by the same equation \eqref{eq:output_rnn} as before.
\end{definition}

Note that the RNN evolution equation for $h^{(t+1)}$ involves both the current input $x^{(t+1)}$ and the previous input $x^{(t)}$. We do this in order to allow the RNN to learn local transformations of an input sequence, for instance, the transformation of a sequence $a_0 a_1 a_2 a_3 \cdots$ which swaps every pair of symbols, to get $a_1 a_0 a_3 a_2 \cdots$ might use $\den{t_{10}}$. More complicated transformations could be learned by adapting the equation to a function of $x^{(i)}, \ldots, x^{(t+1)}$ with a more complicated type than $\textbf{bint}_\alpha$.

\begin{example} If $\Lambda = \{ \underline{1}, \underline{2} \}$ and $\Pi = \{ \underline{010} \}$ then $p^{(t)} \in \mathbb{R}^2, q^{(t)} \in \mathbb{R}$ and
\[
Z = p_1^{(t)}( V x^{(t)} )h^{(t)} + p_2^{(t)} ( V x^{(t)} )^2 h^{(t)} + q^{(t)} (Vx^{(t)})(Vx^{(t+1)})(Vx^{(t)})h^{(t)}\,.
\]
To be clear, here $(Vx^{(t)}) h^{(t)}$ is a matrix multiplication. This term is identical to those in multiplicative RNNs \cite{??}.
\end{example}

\begin{remark} If $\mathscr{I} = \mathbb{R}^{n_I}$ and $\mathscr{H} = \mathbb{R}^{n_H}$ then $V \in M_{n_H^2 \times n_I}(\mathbb{R})$. To avoid an explosion of weights due to the large integer $n_H^2$, we can map $\mathscr{I}$ to operators on a subspace of $\mathscr{H}$. This is done by choosing a decomposition $\mathscr{H} = \mathscr{H}_0 \oplus \mathscr{H}_1$ and letting $\rho: \mathscr{H} \lto \mathscr{H}_1, \iota: \mathscr{H}_1 \lto \mathscr{H}$ be respectively be the projection to and inclusion of the subspace $\mathscr{H}_1$. In the evolution equation we use $\iota \den{\lambda_i}( Vx^{(t+1)} )\big( \rho h^{(t)} \big)$ with $\den{\lambda_i}$ now a function $\End_{\mathbb{R}}(\mathscr{H}_1) \lto \End_{\mathbb{R}}(\mathscr{H}_1)$.
\end{remark}

\newpage

\appendix

\section{Background on CPUs}\label{section:appendix_cpu}

Recall that an assembly program for an ordinary CPU looks like
\begin{verbatim}
LOAD R1, A
ADD R3, R1, R2
STORE C, R3
\end{verbatim}
Where \verb+R1,R2,R3+ stand for the first three registers of the CPU and \verb+A,B,C+ are numbers representing addresses in memory. Thus series of instructions will result in the CPU fetching a number from memory location \verb+A+ and storing it in \verb+R1+, adding this number to a previously stored number in \verb+R2+ with the result being stored in \verb+R3+, and finally writing that register out to the memory address \verb+C+. In the analogy between a CPU and a vanilla RNN we think of the number read from \verb+A+ as the current input $x^{(t)}$ and the previously stored value in \verb+R2+ as (part of) the internal state $h^{(t-1)}$.

Recent work \cite{??,??} on coupling memory to neural networks takes as its starting point the first of the above instructions \verb+LOAD R1, A+ and makes it ``differentiable'' by having the RNN controller predict at time $t$ both the memory address \verb+A+ and the register \verb+R3+ to write to (in this case for example, as a mask on the vector giving the internal state $h^{(t+1)}$). The same differentiable interpretation can be given of the \verb+STORE+ command. This is done by adding suitable terms to the update equation \eqref{eq:update_eqn}.

In contrast our focus is on the third command, the \verb+ADD+. We increase the expressive power of the update equation by allowing it to predict at time $t$ an operation $p^{(t)}$ (a vector specifying a point in a space of ``programs'') which is to be performed on the input and internal state. This could be used with a differentiable memory, but for the sake of simplicity we do not try this. In order to preserve our analogy with CPU instructions even without \verb+LOAD+ and \verb+STORE+, we could imagine a CPU with a command
\begin{verbatim}
ADD R3, A, R2
\end{verbatim}
which at once reads the number from address \verb+A+, adds it to the stored value in \verb+R2+ and writes the result to \verb+R3+. Note that without a \verb+LOAD+ instruction, the only way a value could have gotten into \verb+R2+ in a previous time step is as the result of another \verb+ADD+.

\begin{remark} Although the architecture takes some inspiration from normal CPUs, there is an important distinction: on a normal CPU the program is given as a series of instructions prior to the beginning of execution. In contrast, in the model we have described each command is \emph{predicted} at runtime from the current internal state. Perhaps we can understand the process intuitively as follows: we are co-learning a part of $H$, call it $H_0$, which generates some part of the internal state $h^{(1)}_{0}, h^{(2)}_{0}, \ldots$ giving a path through the state space on which the weight matrix $P$ picks out the right program to run at each time step. The overall algorithm is distributed amongst the weights of $H_0$ and $P$.

This also suggests an alternative algorithm: we do not predict $p^{(t)}$ at each time step, rather we have some fixed number of time steps $T$ and matrices of weights $p^{(1)}, \ldots, p^{(T)}$ which are learned by gradient descent.
\end{remark}

\section{Old stuff}

The operation to be performed is given by a sequence of vectors
\be\label{eq:weight_P}
p^{(t)}_i = \sigma( P_i h^{(t-1)} ) \in \mathbb{R}^{n_P}\,, 1 \le i \le m
\ee
in a way that we will now explain. In outline, we think of the entries of $p^{(t)}_i$ as telling us the coefficients of monomials in the entries of $x^{(t)}$ and $h^{(t-1)}$ to use in the modified update equation. For simplicity let us only consider monomials of degree $2$ in what follows, since the general case is similar. We set
\be\label{eq:weight_E}
y^{(t)} = E x^{(t)} \oplus h^{(t-1)} \in \mathbb{R}^{n_Y}
\ee
where $E$ is another matrix of weights. See Example \ref{??} for an explanation.

Let $F$ denote a linear map $F: M_{n_Y}(\mathbb{R}) \lto \mathbb{R}^{n_Y^2}$ which reads off the entries of a matrix into a column vector. In TensorFlow we can represent this using \emph{reshape}. Writing $Y = y^{(t)}$ observe that $Y Y^T$ is an $n_Y \times n_Y$ matrix with $(i,j)$-entry $Y_i Y_j$. We choose $n_P = n_Y^2$ and compute the entry-wise multiplication
\[
p^{(t)}_i \odot F(Y Y^T) = p^{(t)}_i \odot F\big( y^{(t)} ( y^{(t)} )^T \big) \in \mathbb{R}^{n_P}\,.
\]
Finally, $q^{(t)}$ is the column vector whose $i$th row is $p^{(t)}_i \odot F(Y Y^T)$, that is,
\[
q^{(t)} = \begin{pmatrix} p^{(t)}_1 \odot F\big( y^{(t)} ( y^{(t)} )^T \big) \\
\vdots\\
p^{(t)}_m \odot F\big( y^{(t)} ( y^{(t)} )^T \big) \end{pmatrix}\,.
\] 
In summary, we view $x^{(t)}, h^{(t-1)}$ as respectively the differentiable analogues of \verb+A,R2+ and the sequence $p^{(t)}_1, \ldots, p^{(t)}_m$ as the analogue of the command \verb+ADD+. The output of the command is the vector $q^{(t)}$. We incorporate this output into the update equation as follows:
\be\label{eq:final_update}
h^{(t+1)} = \sigma\big( V q^{(t)} + H h^{(t)} + U x^{(t+1)} \big)\,.
\ee
Thus $V$ is the differentiable analogue of the register \verb+R3+. The weights are $P_i$ from \eqref{eq:weight_P}, $E$ from \eqref{eq:weight_E} and $V, H, U$ from \eqref{eq:final_update}. This architecture is easily generalised to polynomials of higher degree, by adding additional terms.

\begin{example} Let us consider how the system might reproduce the program that repeats every digit of an input binary sequence, e.g.
\be\label{eq:approx_map}
0110 \longmapsto 00111100\,.
\ee
We take the inputs $x \in \mathbb{R}^2$ with $e_1 = (1,0)$ standing for the binary digit $1$ and $e_0 = (0,1)$ standing for $0$. We suppose that the system has learned the embedding matrix $E$ such that $A = E(e_1)$ and $B = E(e_0)$ are matrices in $M_n(\mathbb{R}_{>0})$ with the property that the subgroup they generate under multiplication is a free group on two letters. This condition just means that the map
\[
\Psi: \{0,1\}^* \lto M_n( \mathbb{R} )
\]
from binary sequences to matrices, defined inductively for $s \in \{0,1\}$ by
\[
\Psi( s S ) = \begin{cases} B \Psi(S) & s = 0 \\ A \Psi(S) & s = 1 \end{cases}
\]
is injective. The space of matrices $\mathscr{H} = M_n(\mathbb{R})$ is the internal state of our RNN. To extract output from the RNN we apply a series of fully-connected layers with the final internal state $h^{(T)}$ as input, and we think of this series of layers as approximating a function $\Psi': M_n(\mathbb{R}) \lto \{0,1\}^*$ with the property that $\Psi' \circ \Psi = 1$, that is, which can read off from a product of matrices $ABA$ the corresponding binary sequence $101$. So, in order to approximate the function \eqref{eq:approx_map} our RNN needs to take the inputs
\[
x^{(1)} = B, x^{(2)} = A, x^{(3)} = A, x^{(4)} = B
\]
and produce the final internal state
\[
h^{(T)} = BBAAAABB \in M_n(\mathbb{R})\,.
\]
This can be be done if we assume that in the update equation \eqref{eq:final_update} has weights $H, U = 0$ and $V$ is chosen so that
\[
V q^{(t)} = (x^{(t)})^2 h^{(t-1)}\,.
\]
Note that the right hand side is a cubic polynomial in the entries of $x^{(t)}, h^{(t-1)}$ so we actually need the generalised form of \eqref{eq:final_update}.
\end{example}

\bibliographystyle{amsalpha}
\providecommand{\bysame}{\leavevmode\hbox to3em{\hrulefill}\thinspace}
\providecommand{\href}[2]{#2}
\begin{thebibliography}{BHLS03}

\bibitem{elman}
J.~Elman, \textsl{Finding structure in time}, Cognitive science, 14(2):179â€“211, 1990.

\bibitem{ntm}
A.~Graves, G.~Wayne and I.~Danihelka, \textsl{Neural turing machines}, arXiv preprint arXiv:1410.5401 (2014).

\bibitem{armmik}
A.~Joulin and T.~Mikolov, \textsl{Inferring algorithmic patterns with stack-augmented recurrent nets}, Advances in Neural Information Processing Systems, 2015.

\bibitem{graves_etal}
A.~Graves, \textsl{Hybrid computing using a neural network with dynamic external memory}, Nature 538.7626 (2016): 471--476.

\bibitem{yuhai}
Y.~Wu, S.~Zhang, Y.~Zhang, Y.~Bengio and R.~R.~Salakhutdinov, \textsl{On multiplicative integration with recurrent neural networks}, In Advances In Neural Information Processing Systems, pp. 2856-2864. 2016.

\bibitem{irsoy}
O.~Irsoy and C.~Cardie, \textsl{Modeling compositionality with multiplicative recurrent neural networks}, arXiv preprint arXiv:1412.6577 (2014).

\bibitem{sutskever}
I.~Sutskever, J.~Martens and G.~E.~Hinton, \textsl{Generating text with recurrent neural networks} Proceedings of the 28th International Conference on Machine Learning (ICML-11). 2011.

\bibitem{sutskever2}
I.~Sutskever, O.~Vinyals and Q.~V.~Le, \textsl{Sequence to sequence learning with neural networks}, Advances in neural information processing systems, 2014.

\end{thebibliography}

\end{document}