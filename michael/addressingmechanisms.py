"""
This module implements the 'Addressing Mechanisms' described in section 3.3 in the NTM paper.
"""
import autograd.numpy as np #replace for customised backpropagation later on

# Similarity measure calculated as the cosine similarity in equation (6)
def cosine_similarity(u_t, v_t):
    """
    Definition:
        Computes the cosine similarity of vectors u_t and v_t.
        Specifically \frac{u \cdot v}{||u|| \cdot ||v||} defined in equation (6).
    
    Parameters:   
        u_t 
        v_t     
    """
    
    # numerator is the inner product of equation (6)
    numerator = np.dot(u_t, v_t)

    # denominator is the product of the norms of equation (6)
    unorm = np.sqrt(np.sum(u_t*u_t))
    vnorm = np.sqrt(np.sum(v_t*v_t))
    denominator = (unorm * vnorm) + 1e-20

    return numerator / denominator

# In section 3.3.1 "Focusing by Content" defines  a normalised weighting (w_t)^c based on the similarity 
# k_t (key vector) and b_t (key strength)
# This funtion returns the normalised weighting (w_t)^c  equation (5).
def content_based_focus(k_t, b_t, memoryObject):
    """
    Definition:
        The content-based-addressing method described in 3.3.1.
        Specifically, this inmplements equations (5) and (6). It uses the function cosine_similarity(u_t, v_t).
    
    Parameters:
        k_t is the similarity key vector.
        b_t is the similarity key strength.
        memoryObject is a reference to our NTM memory object.
    """
    
    #This is the length M key vector K_t which each head (writing/reading) produces 
    def K(M_t):
        """
        Definition:
            Given the key vector k_t, compute the similarity function between k_t and M_t and exponentiate.
        
        Parameters:
            M_t: memory matrix
        """
        return np.exp(b_t * cosine_similarity(M_t, k_t))

    # Apply above function K(M_t) to every row in the matrix
    # This is slower - THINK about it!
    l = [] #1 is a row-vector of all 1s as defined in sectin 3.2
    for row in memoryObject:
        l.append(K(row)[0])

    # Return the normalized similarity weights (W_t)^c based on the similarity (i.e., cosine_similarity) and
    # key strength b_t.
    # This is essentially a softmax over the similarities with an extra degree of freedom parametrized by b_t.
    sims = np.array(l)

    n = sims
    d = np.sum(sims)
    w_ct = n/d
    return w_ct


#In section 3.3.1 it is defined the rotation applied to (w_t)^g by s_t
#This funtion returns the circular convolution (w_t)^~ equation (8).
def shift(w_gt, s_t):
    """
    Definition:
        Perform the shifting operation as described in equation (8).
    
    Parameters:
            w_gt is the gate weigthing
            s_t is the shifting weighting
    
    """
    
    # THINK about this function!! slow
    N = w_gt.size

    backward = [1, 0, 0]
    same     = [0, 1, 0]
    forward  = [0, 0, 1]
    null     = [0, 0, 0]

    restrictionList = []
    for i in range(0, N):
        if i == 0:
            restrictionList.append(backward)
        elif i == 1:
            restrictionList.append(same)
        elif i == N-1:
            restrictionList.append(forward)
        else:
            restrictionList.append(null)

    rT = np.array(restrictionList)

    if N >= 3:
        s_t = np.dot(rT, s_t)

    sums = []
    for i in range(N):
        sums.append(0)
        for j in range(N):
            sums[i] += w_gt[j] * s_t[(i - j) % N] #index arithmetic is computed modulo N as pointed in equation (8).

    return np.array(sums)


#THINK about it!
def location_based_focus(g_t, s_t, gamma_t, w_old, w_content):
    """
    The location-addressing method described in 3.3.2.
    Specifically, this is equations (7), (8), and (9).
    g_t is the interpolation gate and it lies in (0,1).
    s_t is the shift weight vector:
        The shift weight vector is of length V, where V is the number
        of allowed integer shifts. e.g. if we allow shifts 0,1, and -1
        (which are computed modulo N and so can be though of as 0,1,2)
        then our shift vector lies in R^3, has non-negative entries,
        and sums to 1. If you wanted to encode a matrix down-shift of 1 row,
        you would pass [0,1,0] here. [1,0,0] corresponds to no shift.
    gamma_t \geq 1 is is sharpening scalar.
    w_old is the weight used by this head for the last time step.
    w_content is the weight generated by the content addressing mechanism
        at the current time step, t.
    """
    # Use the interpolation gate to smooth between old and new weights.
    w_gt = g_t * w_content + (1-g_t) * w_old # This is equation (7)

    # convolve w_gt with s_t to get shifted weights
    w_tp = shift(w_gt, s_t)

    # Take every element of the weight vector to the gamma_t-th power.
    # pows = w_tp ** gamma_t

    # Normalize that vector by its sum.
    # w_t = pows / np.sum(pows)

    # return w_t
    # w_gt is unchanged through all this - i checked (shouldn't have to though)
    return w_tp, w_gt

def create_weights(k_t, b_t, g_t, s_t, gamma_t, w_old, memoryObject):
    """
    Convenience function to be called from NTM fprop.
    """
    w_content = content_based_focus(k_t, b_t, memoryObject)
    w_tp, w_gt = location_based_focus(g_t, s_t, gamma_t, w_old, w_content)
    return w_tp, w_gt, w_content
