{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Implementation of the Linear Logic Recurrent Neural Network (LLRNN)\n",
    "#\n",
    "# The architecture is a modified RNN, see the paper \"Linear logic and recurrent neural networks\".\n",
    "# Our inputs are sequences of symbols taken from an alphabet of size num_classes. The length\n",
    "# of the sequences is N. Our outputs are also sequences of length N from the same alphabet.\n",
    "\n",
    "################\n",
    "# GLOBAL FLAGS #\n",
    "################\n",
    "\n",
    "use_model             = 'ntm' # ntm, pattern_ntm, pattern_ntm_alt\n",
    "task                  = 'copy' # copy, repeat copy, pattern\n",
    "epoch                 = 20 # number of training epochs, default to 200\n",
    "num_classes           = 10 # number of symbols, INCLUDING initial and terminal symbols\n",
    "N                     = 30 # length of input sequences for training, default to 20, INCLUDING initial and terminal symbols\n",
    "Ntest                 = 35 # length of sequences for testing, default to N, INCLUDING initial and terminal symbols\n",
    "batch_size            = 500 # default to 500 (too large does not fit on GPUs)\n",
    "controller_state_size = 100 # dimension of the internal state space of the controller, default 100\n",
    "memory_address_size   = 128 # number of memory locations, default 20\n",
    "memory_content_size   = 20 # size of vector stored at a memory location, default 5\n",
    "powers_ring1          = [0,-1,1] # powers of R used on ring 1, default [0,-1,1]\n",
    "powers_ring2          = [0,-1,1] # powers of R used on ring 2, default [0,-1,1]\n",
    "model_optimizer       = 'rmsprop' # adam, rmsprop, default to rmsprop\n",
    "LOG_DIR               = '/tmp/log' # default /tmp/log\n",
    "num_training          = 10000 # default to int(training_percent * (num_classes-2)**N)\n",
    "num_test              = num_training\n",
    "init_symbol           = num_classes - 2\n",
    "term_symbol           = num_classes - 1\n",
    "\n",
    "##########\n",
    "# NOTES\n",
    "#\n",
    "# 1. Always put the zero power first in powers_ring since the code assumes this is there\n",
    "# 2. The initial and terminal symbols are always from the end of the list of symbols, so they\n",
    "# are respectively num_classes - 2 and num_classes - 1. So the number of symbols which are\n",
    "# not initial or terminal is num_classes - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# Our libraries\n",
    "import ntm\n",
    "import seqhelper\n",
    "import learnfuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the chosen function, the sequence\n",
      "[4, 7, 4, 3, 5, 7, 7, 2, 5, 0, 6, 2, 6, 5, 2, 6, 0, 7, 5, 7, 4, 1, 4, 6, 3, 3, 5, 6, 2, 6]\n",
      "is mapped to\n",
      "[4, 7, 4, 3, 5, 7, 7, 2, 5, 0, 6, 2, 6, 5, 2, 6, 0, 7, 5, 7, 4, 1, 4, 6, 3, 3, 5, 6, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# SETUP TASKS\n",
    "#\n",
    "# Our sequences are of one-hot vectors, which we interpret as follows:\n",
    "#\n",
    "# [1.0, 0.0, 0.0] = 0\n",
    "# [0.0, 1.0, 0.0] = 1\n",
    "# [0.0, 0.0, 1.0] = 2 etc\n",
    "#\n",
    "# We write our sequences and functions referring to sequences of integers,\n",
    "# and then convert to one-hot vectors for integration with TF.\n",
    "\n",
    "# Below N_out and Ntest_out are the lengths of the outputs in both the training\n",
    "# and testing regimes respectively. Since outputs do not include the initial and terminal\n",
    "# symbols, these default to N - 2 and Ntest - 2 respectively.\n",
    "\n",
    "###########\n",
    "# COPY TASK\n",
    "if( task == 'copy' ):\n",
    "    func_to_learn = learnfuncs.f_identity\n",
    "    N_out = N - 2\n",
    "    Ntest_out = Ntest - 2\n",
    "\n",
    "##################\n",
    "# REPEAT COPY TASK\n",
    "# put n zeros before the 1, for a copy task with n + 1 copies\n",
    "if( task == 'repeat copy' ):\n",
    "    pattern = [0,1]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 2 * (N - 2)\n",
    "    Ntest_out = 2 * (Ntest - 2)\n",
    "\n",
    "##############\n",
    "# PATTERN TASK\n",
    "if( task == 'pattern' ):\n",
    "    pattern = [1,0,0,2,0]\n",
    "    func_to_learn = lambda s: learnfuncs.f_repetitionpattern(s,pattern)\n",
    "    N_out = 2 * (N - 2)\n",
    "    Ntest_out = 2 * (Ntest - 2)\n",
    "\n",
    "# Give an example input/output pair\n",
    "a = [random.randint(0,num_classes-3) for i in range(N)]\n",
    "fa = func_to_learn(a)\n",
    "\n",
    "print(\"Under the chosen function, the sequence\")\n",
    "print(a)\n",
    "print(\"is mapped to\")\n",
    "print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# INITIALISE STATE #\n",
    "####################\n",
    "\n",
    "one_hots = seqhelper.one_hot_vectors(num_classes)\n",
    "input_size = num_classes # dimension of the input space I\n",
    "state_size = 0\n",
    "\n",
    "#####\n",
    "# NTM\n",
    "\n",
    "def init_state_ntm(batch_size, css, mas, mcs):\n",
    "    state_size = controller_state_size + 2*memory_address_size + memory_address_size * memory_content_size\n",
    "    \n",
    "    ra = [0.0]*mas\n",
    "    ra[0] = 1.0\n",
    "    batch_address = np.zeros([batch_size,mas]) + ra\n",
    "    \n",
    "    # DEBUG at the moment the read and write addresses are not distributions, i.e. they do not\n",
    "    # sum to 1, but after one step the gamma sharpening will normalise them. We should probably start\n",
    "    # with things that sum to 1, though.\n",
    "    init_controller_state = tf.truncated_normal([batch_size, css], 0.0, 1e-6, dtype=tf.float32)\n",
    "    #init_controller_state = tf.get_variable(\"init_ccs\", shape=[batch_size, controller_state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    init_read_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ \\\n",
    "                       #tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    \n",
    "    init_write_address = tf.constant(batch_address,dtype=tf.float32,shape=[batch_size,mas]) #+ \\\n",
    "                       #tf.random_uniform([batch_size, mas], 0.0, 1e-6)\n",
    "    \n",
    "    init_memory = tf.truncated_normal([batch_size, mas*mcs], 0.0, 1e-6, dtype=tf.float32)\n",
    "    #init_memory = tf.get_variable(\"init_mem\", shape=[batch_size, memory_address_size*memory_content_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    state = tf.concat([init_controller_state,init_read_address,init_write_address,init_memory],1)\n",
    "    return state_size, state\n",
    "\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size,\n",
    "                                  memory_address_size, memory_content_size)\n",
    "    \n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,memory_address_size,memory_content_size, powers_ring1)\n",
    "        \n",
    "#############\n",
    "# PATTERN NTM\n",
    "if( use_model == 'pattern_ntm' ):\n",
    "    state_size = controller_state_size + 4*memory_address_size + \\\n",
    "                memory_address_size * memory_content_size + \\\n",
    "                memory_address_size * len(powers_ring1)\n",
    "\n",
    "    cell = ntm.PatternNTM(state_size,input_size,controller_state_size,\n",
    "                          memory_address_size,memory_content_size, powers_ring1, powers_ring2)\n",
    "    \n",
    "    state = tf.truncated_normal([batch_size, state_size], 0.0, 0.01, dtype=tf.float32)\n",
    "    \n",
    "#################\n",
    "# PATTERN NTM ALT\n",
    "if( use_model == 'pattern_ntm_alt' ):\n",
    "    state_size = controller_state_size + 4*memory_address_size + \\\n",
    "                memory_address_size * memory_content_size + \\\n",
    "                memory_address_size * len(powers_ring1)\n",
    "\n",
    "    cell = ntm.PatternNTM_alt(state_size,input_size,controller_state_size,\n",
    "                          memory_address_size,memory_content_size, powers_ring1, powers_ring2)\n",
    "    \n",
    "    state = tf.truncated_normal([batch_size, state_size], 0.0, 0.01, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'gradients/NTM_87/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_86/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_85/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_84/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_83/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_82/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_81/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_80/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_79/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_78/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_77/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_76/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_75/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_74/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_73/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_72/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_71/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_70/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_69/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_68/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_67/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_66/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_65/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_64/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_63/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_62/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_61/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_60/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_58/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_56/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_54/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_52/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_50/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_48/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_46/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_44/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_42/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_40/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_38/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_36/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_34/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_32/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_30/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_28/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_26/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_24/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_22/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_20/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_18/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_16/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_14/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_12/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_10/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_8/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_6/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_4/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n",
      "[<tf.Tensor 'gradients/NTM_2/split_grad/concat:0' shape=(500, 2916) dtype=float32>, None, None]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DEFINE MODEL #\n",
    "################\n",
    "\n",
    "# During training we feed in sequences of length N and capture outputs of length Nout\n",
    "# During testing we feed in sequences of length Ntest and capture outputs of length Ntest_out\n",
    "# Assuming Ntest >= N, creating Ntest input nodes and Ntest_out output nodes covers both\n",
    "inputs = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest)]\n",
    "targets = [tf.placeholder(tf.float32, [None,input_size]) for _ in range(Ntest_out)]\n",
    "\n",
    "# Used in order to flag that we share weights across iterations.\n",
    "# Note that the training and test phases use all the same weights.\n",
    "reuse = False\n",
    "\n",
    "# Set up training graph\n",
    "read_addresses = []\n",
    "write_addresses = []\n",
    "gamma_writes = []\n",
    "gamma_reads = []\n",
    "ss = []\n",
    "\n",
    "for i in range(N):\n",
    "    # Logging\n",
    "    h0, curr_read, curr_write, _ = tf.split(state, [controller_state_size,\n",
    "                                                    memory_address_size,\n",
    "                                                    memory_address_size,-1], 1)\n",
    "    \n",
    "    #### RUN MODEL ####\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "    ###################\n",
    "    \n",
    "    # More logging\n",
    "    read_addresses.append(curr_read[0,:])\n",
    "    write_addresses.append(curr_write[0,:])\n",
    "    \n",
    "    with tf.variable_scope(\"NTM\",reuse=True):\n",
    "        W_gamma_write = tf.get_variable(\"W_gamma_write\", [controller_state_size,1])\n",
    "        B_gamma_write = tf.get_variable(\"B_gamma_write\", [])\n",
    "        gamma_write = 1.0 + tf.nn.relu(tf.matmul(h0,W_gamma_write) + B_gamma_write) # shape [batch_size,1]\n",
    "        \n",
    "        W_gamma_read = tf.get_variable(\"W_gamma_read\", [controller_state_size,1])\n",
    "        B_gamma_read = tf.get_variable(\"B_gamma_read\", [])\n",
    "        gamma_read = 1.0 + tf.nn.relu(tf.matmul(h0,W_gamma_read) + B_gamma_read) # shape [batch_size,1]\n",
    "        \n",
    "        W_s = tf.get_variable(\"W_s\", [controller_state_size,len(powers_ring1)])\n",
    "        B_s = tf.get_variable(\"B_s\", [len(powers_ring1)])\n",
    "        s = tf.nn.softmax(tf.matmul(h0,W_s) + B_s) # shape [batch_size,len(powers)]\n",
    "\n",
    "    gamma_writes.append(gamma_write[0,:])\n",
    "    gamma_reads.append(gamma_read[0,:])\n",
    "    ss.append(s[0,:])\n",
    "    reuse = True\n",
    "\n",
    "# We only start recording the outputs of the controller once we have\n",
    "# finished feeding in the input. We feed terminal symbols as input in the second phase.\n",
    "\n",
    "term_symbol_tensor = tf.constant(np.zeros([batch_size,input_size]) + one_hots[term_symbol],\n",
    "                                 dtype=tf.float32,\n",
    "                                 shape=[batch_size,input_size])\n",
    "\n",
    "rnn_outputs = []\n",
    "for i in range(N_out):\n",
    "    output, state = cell(term_symbol_tensor,state,'NTM',reuse)\n",
    "    rnn_outputs.append(output)\n",
    "\n",
    "# Final fully connected layer\n",
    "with tf.variable_scope(\"final_layer\"):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size],initializer=init_ops.constant_initializer(0.0))\n",
    "\n",
    "# prediction is a length N list of tensors of shape [None,input_size], where\n",
    "# the jth row of prediction[d] is, for the jth input sequence in the batch,\n",
    "# the probability distribution over symbols for the output symbol in position d.\n",
    "logits = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs]\n",
    "\n",
    "# Note: we use log_softmax to avoid precision issues with floats causing log(0) to create NaNs\n",
    "prediction = [tf.nn.log_softmax(logit) for logit in logits] \n",
    "ce = [tf.reduce_sum(targets[i] * prediction[i]) for i in range(N_out)]\n",
    "cross_entropy = -tf.add_n(ce)\n",
    "                    \n",
    "if( model_optimizer == 'adam' ):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "elif( model_optimizer == 'rmsprop' ):\n",
    "    optimizer = tf.train.RMSPropOptimizer(1e-4,decay=0.9,momentum=0.9)\n",
    "\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "\n",
    "mistakes = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction[i], 1)) for i in range(N_out)]\n",
    "errors = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes]\n",
    "\n",
    "# Summaries\n",
    "mean_error = tf.scalar_mul(np.true_divide(1,N_out), tf.add_n(errors))\n",
    "tf.summary.scalar('error', mean_error)\n",
    "\n",
    "# Initialise the model\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 argmax [0] w-gamma [ 1.00000107] w-rotations [ 0.33333391  0.33333343  0.3333326 ]\n",
      "Step 1 argmax [0] w-gamma [ 1.] w-rotations [ 0.32220221  0.47402194  0.20377588]\n",
      "Step 2 argmax [0] w-gamma [ 1.03490019] w-rotations [ 0.39750341  0.42493597  0.17756061]\n",
      "Step 3 argmax [127] w-gamma [ 1.] w-rotations [ 0.24760535  0.35341287  0.39898178]\n",
      "Step 4 argmax [0] w-gamma [ 1.] w-rotations [ 0.35845426  0.32375655  0.31778926]\n",
      "Step 5 argmax [127] w-gamma [ 1.08230996] w-rotations [ 0.41751343  0.33953899  0.24294756]\n",
      "Step 6 argmax [127] w-gamma [ 1.42537665] w-rotations [ 0.32971138  0.26045883  0.40982977]\n",
      "Step 7 argmax [0] w-gamma [ 1.] w-rotations [ 0.46257994  0.19975612  0.33766401]\n",
      "Step 8 argmax [0] w-gamma [ 1.68045056] w-rotations [ 0.29304686  0.51999277  0.18696029]\n",
      "Step 9 argmax [127] w-gamma [ 1.] w-rotations [ 0.55332398  0.26942691  0.17724907]\n",
      "Step 10 argmax [127] w-gamma [ 1.12069321] w-rotations [ 0.27245045  0.3345297   0.39301986]\n",
      "Step 11 argmax [127] w-gamma [ 1.] w-rotations [ 0.40373075  0.34402442  0.25224477]\n",
      "Step 12 argmax [127] w-gamma [ 1.] w-rotations [ 0.3307769   0.43718004  0.23204304]\n",
      "Step 13 argmax [127] w-gamma [ 1.07973099] w-rotations [ 0.39632183  0.33049446  0.27318376]\n",
      "Step 14 argmax [127] w-gamma [ 1.] w-rotations [ 0.26796958  0.4128179   0.31921262]\n",
      "Step 15 argmax [127] w-gamma [ 1.14175272] w-rotations [ 0.40106821  0.48513016  0.11380167]\n",
      "Step 16 argmax [126] w-gamma [ 1.14927983] w-rotations [ 0.34161395  0.1864851   0.47190091]\n",
      "Step 17 argmax [127] w-gamma [ 1.10209239] w-rotations [ 0.2838105   0.26214585  0.45404369]\n",
      "Step 18 argmax [127] w-gamma [ 1.42115557] w-rotations [ 0.26140964  0.53890181  0.19968857]\n",
      "Step 19 argmax [127] w-gamma [ 1.62040401] w-rotations [ 0.52724963  0.24455942  0.22819097]\n",
      "Step 20 argmax [127] w-gamma [ 1.] w-rotations [ 0.28477237  0.39504719  0.32018048]\n",
      "Step 21 argmax [126] w-gamma [ 1.26975894] w-rotations [ 0.33960101  0.39773244  0.26266664]\n",
      "Step 22 argmax [126] w-gamma [ 1.] w-rotations [ 0.39362556  0.40773705  0.19863741]\n",
      "Step 23 argmax [126] w-gamma [ 1.] w-rotations [ 0.2546908   0.18711697  0.55819231]\n",
      "Step 24 argmax [127] w-gamma [ 1.] w-rotations [ 0.22590153  0.31097695  0.4631215 ]\n",
      "Step 25 argmax [127] w-gamma [ 1.] w-rotations [ 0.34932491  0.44678599  0.2038891 ]\n",
      "Step 26 argmax [126] w-gamma [ 1.7813983] w-rotations [ 0.4161742   0.24952735  0.33429843]\n",
      "Step 27 argmax [127] w-gamma [ 1.] w-rotations [ 0.39751559  0.42201275  0.18047167]\n",
      "Step 28 argmax [126] w-gamma [ 1.32084894] w-rotations [ 0.29634356  0.2710619   0.43259451]\n",
      "Step 29 argmax [126] w-gamma [ 1.] w-rotations [ 0.28645936  0.45729229  0.25624835]\n",
      "Write address -\n",
      "[  1.49892092e-01   8.74694660e-02   3.88106629e-02   1.29409023e-02\n",
      "   3.19224875e-03   5.71159297e-04   7.24892525e-05   6.47769912e-06\n",
      "   4.82402641e-07   8.53916973e-08   6.07538695e-08   5.93437051e-08\n",
      "   5.92706328e-08   5.92657869e-08   5.92654388e-08   5.92654104e-08\n",
      "   5.92654033e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654033e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654033e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654033e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654033e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654033e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654104e-08\n",
      "   5.92654104e-08   5.92654104e-08   5.92654104e-08   5.92654281e-08\n",
      "   5.92657941e-08   5.92713221e-08   5.93592198e-08   6.11434174e-08\n",
      "   9.32875537e-08   6.28300711e-07   8.64622325e-06   9.37828227e-05\n",
      "   7.09268847e-04   3.79998190e-03   1.47926621e-02   4.27550524e-02\n",
      "   9.32860225e-02   1.55452475e-01   1.99232191e-01   1.96906835e-01]\n",
      "Epoch - 1 error - 0.851571\n",
      "Epoch - 2 error - 0.826857\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6510023b29f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m##### Do gradient descent #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged_summaries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;31m########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "random.seed()\n",
    "\n",
    "pre_train_time = time.time()\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(num_training/batch_size)\n",
    "\n",
    "# An annoying thing here is that we cannot use a list as a key in a \n",
    "# dictionary. The workaround we found on StackOverflow here:\n",
    "# http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow)\n",
    "\n",
    "# epoch is a global var\n",
    "for i in range(epoch):\n",
    "    for j in range(no_of_batches):\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        # We sample each batch on the fly from the set of all sequences\n",
    "        for z in range(batch_size):\n",
    "            # construct a sequence from 0,...,num_classes - 3 then append initial and terminal symbols\n",
    "            a = [random.randint(0,num_classes-3) for k in range(N-2)]\n",
    "            junk = [0 for k in range(Ntest-N+2)]\n",
    "            f_junk = [0 for k in range(Ntest_out-N_out)]\n",
    "            fa = func_to_learn(a) + f_junk\n",
    "            a = [init_symbol] + a + [term_symbol] + junk\n",
    "            a_onehot = [one_hots[e] for e in a]\n",
    "            fa_onehot = [one_hots[e] for e in fa]\n",
    "            inp.append(np.array(a_onehot))\n",
    "            out.append(np.array(fa_onehot))        \n",
    "        \n",
    "        # Note we have to feed all the input nodes, including those with d > N, \n",
    "        # although these inputs do not play any role in the running of the RNN\n",
    "        # during training (the additional nodes are there so we can use the\n",
    "        # same network for testing on longer sequences)\n",
    "        feed_dict = {}\n",
    "        for d in range(Ntest):\n",
    "            in_node = inputs[d]\n",
    "            # inp has dimensions [batch_size, N, num_classes] and we want to extract\n",
    "            # the 2D Tensor of shape [batch_size, num_classes] obtained by setting the\n",
    "            # second coordinate to d\n",
    "            ti = []\n",
    "            for k in range(batch_size):\n",
    "                ti.append(inp[k][d])\n",
    "            feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "        for d in range(Ntest_out):\n",
    "            out_node = targets[d]\n",
    "            to = []\n",
    "            for k in range(batch_size):\n",
    "                to.append(out[k][d])\n",
    "            feed_dict[out_node] = np.array(to)\n",
    "        \n",
    "        # for the first batch in an epoch, we have some logging\n",
    "        if( j == 0 and i % 25 == 0 ):\n",
    "            ss_val, gamma_reads_val, gamma_writes_val, read_addresses_val, write_addresses_val = sess.run([ss, gamma_reads,gamma_writes,read_addresses,write_addresses],feed_dict)\n",
    "    \n",
    "            s = 0\n",
    "            for r in range(len(write_addresses_val)):\n",
    "                print(\"Step \" + str(s) + \" argmax [\" + str(write_addresses_val[r].argmax()) + \"]\" + \n",
    "                      \" w-gamma \" + str(gamma_writes_val[r]) + \n",
    "                      \" w-rotations \" + str(ss_val[r]))\n",
    "                if( r == len(write_addresses_val) - 1 ):\n",
    "                    print(\"Write address -\")\n",
    "                    print(write_addresses_val[r])               \n",
    "                s = s + 1\n",
    "        \n",
    "        ##### Do gradient descent #####\n",
    "        summary,_ = sess.run([merged_summaries,minimize], feed_dict)\n",
    "        ########\n",
    "        \n",
    "        # Write out TensorBoard logs\n",
    "        file_writer.add_summary(summary)\n",
    "    current_mean = np.mean(sess.run(errors, feed_dict))\n",
    "    \n",
    "    # Print the mean error of the final batch in the epoch\n",
    "    print(\"Epoch - \" + str(i+1) + \" error - \" + str(current_mean))\n",
    "    \n",
    "# Matplotlib\n",
    "plt.xlabel('Memory location')\n",
    "plt.ylabel('Time')\n",
    "plt.title('Write address')\n",
    "plt.imshow(np.stack(write_addresses_val), cmap='bone', interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "# Write out variables to disk\n",
    "#saver = tf.train.Saver()\n",
    "#save_path = saver.save(sess,\"/tmp/model.ckpt\")\n",
    "#sess.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"It took\", time.time() - pre_train_time, \"seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch - 1, Mean error - 0.857515\n",
      "Batch - 2, Mean error - 0.858909\n",
      "Batch - 3, Mean error - 0.858849\n",
      "Batch - 4, Mean error - 0.863394\n",
      "Batch - 5, Mean error - 0.861273\n",
      "Batch - 6, Mean error - 0.857879\n",
      "Batch - 7, Mean error - 0.86097\n",
      "Batch - 8, Mean error - 0.862061\n",
      "Batch - 9, Mean error - 0.862121\n",
      "Batch - 10, Mean error - 0.864545\n",
      "Batch - 11, Mean error - 0.861152\n",
      "Batch - 12, Mean error - 0.861515\n",
      "Batch - 13, Mean error - 0.867697\n",
      "Batch - 14, Mean error - 0.865515\n",
      "Batch - 15, Mean error - 0.866242\n",
      "Batch - 16, Mean error - 0.862061\n",
      "Batch - 17, Mean error - 0.858667\n",
      "Batch - 18, Mean error - 0.861939\n",
      "Batch - 19, Mean error - 0.864788\n",
      "Batch - 20, Mean error - 0.864364\n",
      "\n",
      "###########\n",
      "# Summary #\n",
      "###########\n",
      "\n",
      "model         - ntm\n",
      "task name     - copy\n",
      "num_classes   - 10\n",
      "N             - 30\n",
      "N_out         - 28\n",
      "Ntest         - 35\n",
      "Ntest_out     - 33\n",
      "ring 1 powers - [0, -1, 1]\n",
      "ring 2 powers - [0, -1, 1]\n",
      "# epochs      - 20\n",
      "optimizer     - rmsprop\n",
      "# weights     - 18958\n",
      "(css,mas,mcs) - (100,128,20)\n",
      "num_training  - 10000/1000000000000000000000000000000\n",
      "num_test      - 10000/1000000000000000000000000000000\n",
      "\n",
      "\n",
      "error         - 0.862073\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# TESTING #\n",
    "###########\n",
    "\n",
    "if( use_model == 'ntm' ):\n",
    "    state_size, state = init_state_ntm(batch_size, controller_state_size,\n",
    "                                  memory_address_size, memory_content_size)\n",
    "    \n",
    "    cell = ntm.NTM(state_size,input_size,controller_state_size,memory_address_size,memory_content_size, powers_ring1)\n",
    " \n",
    "# Restore the weights from training\n",
    "#sess = tf.Session()\n",
    "#saver.restore(sess,save_path)\n",
    "\n",
    "# Set up test graph\n",
    "reuse = True\n",
    "for i in range(Ntest):\n",
    "    output, state = cell(inputs[i],state,'NTM',reuse)\n",
    "\n",
    "rnn_outputs_test = []\n",
    "for i in range(Ntest_out):\n",
    "    output, state = cell(term_symbol_tensor,state,'NTM',reuse)\n",
    "    rnn_outputs_test.append(output)\n",
    "    \n",
    "with tf.variable_scope(\"final_layer\",reuse=True):\n",
    "    E = tf.get_variable(\"E\",[controller_state_size,input_size])\n",
    "    F = tf.get_variable(\"F\",[input_size])\n",
    "\n",
    "logits_test = [tf.matmul(rnn_output, E) + F for rnn_output in rnn_outputs_test]\n",
    "prediction_test = [tf.nn.softmax(logit) for logit in logits_test] \n",
    "mistakes_test = [tf.not_equal(tf.argmax(targets[i], 1), tf.argmax(prediction_test[i], 1)) for i in range(Ntest_out)]\n",
    "errors_test = [tf.reduce_mean(tf.cast(m, tf.float32)) for m in mistakes_test]\n",
    "\n",
    "#### RUN TEST ####\n",
    "\n",
    "no_of_batches = int(num_test/batch_size)\n",
    "\n",
    "error_means = []\n",
    "for j in range(no_of_batches):\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    # We sample each batch on the fly from the set of all sequences\n",
    "    for z in range(batch_size):\n",
    "        a = [random.randint(0,num_classes-3) for k in range(Ntest-2)]\n",
    "        fa = func_to_learn(a)\n",
    "        a = [init_symbol] + a + [term_symbol]\n",
    "        a_onehot = [one_hots[e] for e in a]\n",
    "        fa_onehot = [one_hots[e] for e in fa]\n",
    "        inp.append(np.array(a_onehot))\n",
    "        out.append(np.array(fa_onehot))        \n",
    "        \n",
    "    feed_dict = {}\n",
    "    for d in range(Ntest):\n",
    "        in_node = inputs[d]\n",
    "        ti = []\n",
    "        for k in range(batch_size):\n",
    "            ti.append(inp[k][d])\n",
    "        feed_dict[in_node] = np.array(ti)\n",
    "\n",
    "    for d in range(Ntest_out):\n",
    "        out_node = targets[d]\n",
    "        to = []\n",
    "        for k in range(batch_size):\n",
    "            to.append(out[k][d])\n",
    "        feed_dict[out_node] = np.array(to)\n",
    "            \n",
    "    current_mean = np.mean(sess.run(errors_test, feed_dict))\n",
    "    error_means.append(current_mean)\n",
    "    print(\"Batch - \" + str(j+1) + \", Mean error - \" + str(current_mean))\n",
    "\n",
    "final_error = np.mean(error_means)\n",
    "\n",
    "print(\"\")        \n",
    "print(\"###########\")\n",
    "print(\"# Summary #\")\n",
    "print(\"###########\")\n",
    "print(\"\")\n",
    "print(\"model         - \" + use_model)\n",
    "print(\"task name     - \" + task)\n",
    "print(\"num_classes   - \" + str(num_classes))\n",
    "print(\"N             - \" + str(N))\n",
    "print(\"N_out         - \" + str(N_out))\n",
    "print(\"Ntest         - \" + str(Ntest))\n",
    "print(\"Ntest_out     - \" + str(Ntest_out))\n",
    "print(\"ring 1 powers - \" + str(powers_ring1))\n",
    "print(\"ring 2 powers - \" + str(powers_ring2))\n",
    "print(\"# epochs      - \" + str(epoch))\n",
    "print(\"optimizer     - \" + str(model_optimizer))\n",
    "print(\"# weights     - \" + str(ntm.count_number_trainable_params()))\n",
    "print(\"(css,mas,mcs) - (\" + str(controller_state_size) + \",\" + str(memory_address_size) + \",\" + str(memory_content_size) + \")\")\n",
    "#print(\"train percent - \" + str(training_percent))\n",
    "print(\"num_training  - \" + str(num_training) + \"/\" + str(num_classes**N))\n",
    "print(\"num_test      - \" + str(num_test) + \"/\" + str(num_classes**N))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"error         - \" + str(final_error))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
