{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "#  Initial implementation of linear logic recurrent neural network\n",
    "#\n",
    "# Many elements borrowed from \"A noobâ€™s guide to implementing RNN-LSTM using Tensorflow\"\n",
    "# see https://gist.github.com/monikkinom/e97d518fe02a79177b081c028a83ec1c\n",
    "\n",
    "# Global flags\n",
    "\n",
    "flag_useHigherMonomials = 0\n",
    "batch_size = 10\n",
    "input_size = 1\n",
    "state_size = 24\n",
    "N = 12 # length of sequences\n",
    "NUM_EXAMPLES = 1000 # number of training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The next three lines are recommend by TF\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import six\n",
    "import math\n",
    "\n",
    "from random import shuffle\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops.rnn_cell import _linear\n",
    "from tensorflow.python.ops.rnn_cell import RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "from tensorflow.python.ops.math_ops import matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We include here the function is_sequence from tensorflow/python/util/nest.py\n",
    "# because we need it in _linear_local\n",
    "\n",
    "def is_sequence(seq):\n",
    "  \"\"\"Returns a true if its input is a collections.Sequence (except strings).\n",
    "\n",
    "  Args:\n",
    "    seq: an input sequence.\n",
    "\n",
    "  Returns:\n",
    "    True if the sequence is a not a string and is a collections.Sequence.\n",
    "  \"\"\"\n",
    "  return (isinstance(seq, collections.Sequence)\n",
    "          and not isinstance(seq, six.string_types))\n",
    "\n",
    "# In tf.nn.rnn_cell there is a private function _linear that \n",
    "# we have modified here to be used in our LinearLogicRNNCell\n",
    "# but first we give the original function with some annotations\n",
    "# that helped us to understand what is going on\n",
    "\n",
    "def _linear_local(args, output_size, bias, bias_start=0.0, scope=None):\n",
    "    \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n",
    "\n",
    "    Args:\n",
    "    args: a 2D Tensor or a list of 2D Tensors, each of shape [batch, n]\n",
    "    output_size: int, second dimension of W[i].\n",
    "    bias: boolean, whether to add a bias term or not.\n",
    "    bias_start: starting value to initialize the bias; 0 by default.\n",
    "    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "\n",
    "    Returns:\n",
    "    A 2D Tensor with shape [batch, output_size] equal to\n",
    "    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "    \"\"\"\n",
    "    if args is None or (is_sequence(args) and not args):\n",
    "        raise ValueError(\"`args` must be specified\")\n",
    "    if not is_sequence(args):\n",
    "        args = [args]\n",
    "\n",
    "    if args[0].get_shape()[0] != batch_size:\n",
    "        raise ValueError(\"`args` must be tensors of size batch_size\")\n",
    "        \n",
    "    # Our input is a pair of 2D tensors of shapes [B, U] and [B, V]\n",
    "    # respectively, where B is the batch size, U is the dimension of the\n",
    "    # input space and V is the dimension of the state space (e.g. U = 1)\n",
    "    \n",
    "    # A 2D tensor of shape [X,Y] means a matrix with X rows and Y cols\n",
    "    # The row index here is interpreted as indexing into a batch.\n",
    "    \n",
    "    # array_ops.concat(1,args) is a Tensor of shape [B,U+V]\n",
    "    # which is obtained by stacking the BxU and BxV matrices\n",
    "    # together horizontally to get a Bx(U+V) matrix. We first modify\n",
    "    # args by adding on the end a new matrix which contains higher\n",
    "    # degree monomials in the entries of our input tensors. Let\n",
    "    # us write (I) for the input matrix (BxU) and (S) for the state\n",
    "    # matrix (BxV)\n",
    "    \n",
    "    # flag_useHigherMonomials is a global Boolean, see the top of the file\n",
    "    \n",
    "    f = lambda x: tf.reshape(tf.matmul(tf.transpose(x),x),[-1])\n",
    "    \n",
    "    if flag_useHigherMonomials == 1:\n",
    "        h = array_ops.concat(1, args) # (I|S)\n",
    "        h2 = tf.expand_dims(h,1)\n",
    "        h5 = tf.map_fn(f, h2)       \n",
    "        args = args + [h5]\n",
    "    \n",
    "    # Calculate the total size of arguments on dimension 1.\n",
    "    total_arg_size = 0\n",
    "    shapes = [a.get_shape().as_list() for a in args]\n",
    "\n",
    "    # each element shape in shapes will be of the form [None, Q]\n",
    "    # where Q is the dimension of either the state space or the\n",
    "    # input space. If we received two inputs [None, 5],[None, 2]\n",
    "    # then total_arg_size = 7\n",
    "    for shape in shapes:\n",
    "        if len(shape) != 2:\n",
    "            raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n",
    "        if not shape[1]:\n",
    "            raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n",
    "        else:\n",
    "            total_arg_size += shape[1]\n",
    "\n",
    "    # Now the computation. We create a matrix P of weights with\n",
    "    # output_size columns and total_arg_size rows. \n",
    "    \n",
    "    # NOTE: the W and U that are referred to in LinearLogicRNNCell\n",
    "    # are the block parts of the matrix P.\n",
    "    \n",
    "    # NOTE: The equation in LinearLogicRNNCell writes the matrix\n",
    "    # multiplication as W * input because there they (the authors)\n",
    "    # are thinking of column matrices, where in fact everything is\n",
    "    # done with row vectors, so we actually compute input * W.\n",
    "    \n",
    "    with vs.variable_scope(scope or \"Linear\"):\n",
    "        # Define our weight block matrix P\n",
    "        matrix = vs.get_variable(\"Matrix\", [total_arg_size, output_size])\n",
    "        if len(args) == 1:\n",
    "            res = matmul(args[0], matrix)\n",
    "        else:\n",
    "            res = matmul(array_ops.concat(1, args), matrix)\n",
    "        if not bias:\n",
    "            return res\n",
    "        bias_term = vs.get_variable(\"Bias\", [output_size], initializer=init_ops.constant_initializer(bias_start))\n",
    "        return res + bias_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearLogicRNNCell(RNNCell):\n",
    "    \n",
    "    def __init__(self, num_units, input_size=None, activation=tanh):\n",
    "        if input_size is not None:\n",
    "            logging.warn(\"%s: The input_size parameter is deprecated.\" % self)\n",
    "        self._num_units = num_units\n",
    "        self._activation = activation\n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Most basic RNN: output = new_state = activation(W * input + U * state + B).\"\"\"\n",
    "        with vs.variable_scope(scope or type(self).__name__):  # \"BasicRNNCell\"\n",
    "            output = self._activation(_linear_local([inputs, state], self._num_units, True))\n",
    "        return output, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 4096\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# Create a shuffled list of all binary sequnces of length N\n",
    "s = '{0:0' + str(N) + 'b}'\n",
    "train_input = [s.format(i) for i in range(2**N)]\n",
    "shuffle(train_input)\n",
    "train_input = [map(int,i) for i in train_input]\n",
    "ti = []\n",
    "for i in train_input:\n",
    "    temp_list = []\n",
    "    for j in i:\n",
    "        temp_list.append([j])\n",
    "    ti.append(np.array(temp_list))\n",
    "train_input = ti\n",
    "\n",
    "print(\"Number of sequences: \" + str(len(train_input)))\n",
    "print(train_input[0])\n",
    "# A typical element of train_input at this point will be an array like\n",
    "# array([[1],[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Training output\n",
    "train_output = []\n",
    "\n",
    "for i in train_input:\n",
    "    count = 0\n",
    "    for j in i:\n",
    "        if j[0] == 1:\n",
    "            count+=1\n",
    "    temp_list = ([0]*(N+1))\n",
    "    temp_list[count]=1\n",
    "    train_output.append(temp_list)\n",
    "        \n",
    "# This matches every sequence in train_input with the one-hot encoded representation\n",
    "# of its classification, that is, e_i where 0 \\le i \\le 20 and i is the number of 1s\n",
    "# that appear in the sequence\n",
    "\n",
    "print(train_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_input = train_input[NUM_EXAMPLES:]\n",
    "test_output = train_output[NUM_EXAMPLES:]\n",
    "train_input = train_input[:NUM_EXAMPLES]\n",
    "train_output = train_output[:NUM_EXAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "inputs must be a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-56edb42f81ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#cell = tf.nn.rnn_cell.BasicRNNCell(num_hidden)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearLogicRNNCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(cell, inputs, initial_state, dtype, sequence_length, scope)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cell must be an instance of RNNCell\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inputs must be a sequence\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inputs must not be empty\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: inputs must be a sequence"
     ]
    }
   ],
   "source": [
    "# Definition of the model\n",
    "data = tf.placeholder(tf.float32, [None,N,1])\n",
    "target = tf.placeholder(tf.float32, [None,N+1])\n",
    "\n",
    "num_hidden = state_size\n",
    "#cell = tf.nn.rnn_cell.LSTMCell(num_hidden,state_is_tuple=True)\n",
    "#cell = tf.nn.rnn_cell.BasicRNNCell(num_hidden)\n",
    "cell = LinearLogicRNNCell(num_hidden)\n",
    "val, state = tf.nn.dynamic_rnn(cell,data,dtype=tf.float32)\n",
    "val = tf.transpose(val,[1,0,2])\n",
    "last = tf.gather(val, int(val.get_shape()[0]) - 1)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([num_hidden,N+1]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[N+1]))\n",
    "\n",
    "# note that adding \n",
    "prediction = tf.nn.softmax(tf.matmul(last, weight) + bias)\n",
    "cross_entropy = -tf.reduce_sum(target * tf.log(prediction))\n",
    "nn\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "\n",
    "mistakes = tf.not_equal(tf.argmax(target, 1), tf.argmax(prediction, 1))\n",
    "error = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialise the model\n",
    "init_op = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "# Training\n",
    "no_of_batches = int(len(train_input)/batch_size)\n",
    "print(\"Number of batches: \" + str(no_of_batches))\n",
    "epoch = 5\n",
    "\n",
    "for i in range(epoch):\n",
    "    ptr = 0\n",
    "    for j in range(no_of_batches):\n",
    "        inp, out = train_input[ptr:ptr+batch_size],train_output[ptr:ptr+batch_size]\n",
    "        ptr += batch_size\n",
    "        sess.run(minimize,{data: inp, target: out})\n",
    "    print(\"Epoch - \" + str(i))\n",
    "    \n",
    "incorrect = sess.run(error,{data: test_input, target: test_output})\n",
    "print('Epoch {:2d} error {:3.1f}%'.format(i+1,100 * incorrect))\n",
    "\n",
    "print(\"Example ----\")\n",
    "print(train_input[0])\n",
    "print(\"Correct classification is:\")\n",
    "print(train_output[0])\n",
    "print(\"We predicted:\")\n",
    "print(sess.run(prediction,{data: [train_input[0]]}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
